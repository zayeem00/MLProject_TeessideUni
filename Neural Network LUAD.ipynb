{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zayeem00/MLProject_TeessideUni/blob/master/Neural%20Network%20LUAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UgJo3g3hYkod",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88022e3d-e94a-4159-b991-be91ccf21c8b"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rjcx2KYzY3ug",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "87d99c3b-c341-47af-a525-d7ba7744703e"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-31527f12-e8b4-4319-a7b9-f4fc69968840\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-31527f12-e8b4-4319-a7b9-f4fc69968840\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving survival_LUAD.xlsx to survival_LUAD (1).xlsx\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'survival_LUAD.xlsx': b'PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00b\\xee\\x9dh^\\x01\\x00\\x00\\x90\\x04\\x00\\x00\\x13\\x00\\x08\\x02[Content_Types].xml \\xa2\\x04\\x02(\\xa0\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x94\\xcbN\\xc30\\x10E\\xf7H\\xfcC\\xe4-J\\xdc\\xb2@\\x085\\xed\\x82\\xc7\\x12*Q>\\xc0\\xc4\\x93\\xc6\\xaac[\\x9eii\\xff\\x9e\\x89\\xfb\\x10B\\xa1\\x15j7\\xb1\\x12\\xcf\\xdc{2\\xf1\\xcdh\\xb2nm\\xb6\\x82\\x88\\xc6\\xbbR\\x0c\\x8b\\x81\\xc8\\xc0U^\\x1b7/\\xc5\\xc7\\xec%\\xbf\\x17\\x19\\x92rZY\\xef\\xa0\\x14\\x1b@1\\x19__\\x8df\\x9b\\x00\\x98q\\xb7\\xc3R4D\\xe1AJ\\xac\\x1ah\\x15\\x16>\\x80\\xe3\\x9d\\xda\\xc7V\\x11\\xdf\\xc6\\xb9\\x0c\\xaaZ\\xa89\\xc8\\xdb\\xc1\\xe0NV\\xde\\x118\\xca\\xa9\\xd3\\x10\\xe3\\xd1\\x13\\xd4ji){^\\xf3\\xe3-I\\x04\\x8b\"{\\xdc\\x16v^\\xa5P!XS)bR\\xb9r\\xfa\\x97K\\xbes(\\xb83\\xd5`c\\x02\\xde0\\x86\\x90\\xbd\\x0e\\xdd\\xce\\xdf\\x06\\xbb\\xbe7\\x1eM4\\x1a\\xb2\\xa9\\x8a\\xf4\\xaaZ\\xc6\\x90k+\\xbf|\\\\|z\\xbf(\\x8e\\x8b\\xf4P\\xfa\\xba6\\x15h_-[\\x9e@\\x81!\\x82\\xd2\\xd8\\x00Pk\\x8b\\xb4\\x16\\xad2n\\xcf}\\xc4?\\x15\\xa3L\\xcb\\xf0\\xc2 \\xdd\\xfb%\\xe1\\x13\\x1c\\xc4\\xdf\\x1bd\\xba\\x9e\\x8f\\x90dN\\x18\"m,\\xe0\\xa5\\xc7\\x9eDO97*\\x82~\\xa7\\xc8\\xc9\\xb88\\xc0O\\xedc\\x1c|n\\xa6\\xd1\\x07\\xe4\\x04E\\xf8\\xff\\x14\\xf6\\x11\\xe9\\xba\\xf3\\xc0B\\x10\\xc9\\xc0!$}\\x87\\xed\\xe0\\xc8\\xe9;{\\xec\\xd0\\xe5[\\x83\\xee\\xf1\\x96\\xe9\\x7f2\\xfe\\x06\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xb5U0#\\xf4\\x00\\x00\\x00L\\x02\\x00\\x00\\x0b\\x00\\x08\\x02_rels/.rels \\xa2\\x04\\x02(\\xa0\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x92MO\\xc30\\x0c\\x86\\xefH\\xfc\\x87\\xc8\\xf7\\xd5\\xdd\\x90\\x10BKwAH\\xbb!T~\\x80I\\xdc\\x0f\\xb5\\x8d\\xa3$\\x1b\\xdd\\xbf\\'\\x1c\\x10T\\x1a\\x83\\x03G\\x7f\\xbd~\\xfc\\xca\\xdb\\xdd<\\x8d\\xea\\xc8!\\xf6\\xe24\\xac\\x8b\\x12\\x14;#\\xb6w\\xad\\x86\\x97\\xfaqu\\x07*&r\\x96Fq\\xac\\xe1\\xc4\\x11v\\xd5\\xf5\\xd5\\xf6\\x99GJy(v\\xbd\\x8f*\\xab\\xb8\\xa8\\xa1K\\xc9\\xdf#F\\xd3\\xf1D\\xb1\\x10\\xcf.W\\x1a\\t\\x13\\xa5\\x1c\\x86\\x16=\\x99\\x81Z\\xc6MY\\xdeb\\xf8\\xae\\x01\\xd5BS\\xed\\xad\\x86\\xb0\\xb77\\xa0\\xea\\x93\\xcf\\x9b\\x7f\\xd7\\x96\\xa6\\xe9\\r?\\x889L\\xec\\xd2\\x99\\x15\\xc8sbg\\xd9\\xae|\\xc8l!\\xf5\\xf9\\x1aUSh9i\\xb0b\\x9er:\"y_dl\\xc0\\xf3D\\x9b\\xbf\\x13\\xfd|-N\\x9c\\xc8R\"4\\x12\\xf82\\xcfG\\xc7%\\xa0\\xf5\\x7fZ\\xb44\\xf1\\xcb\\x9dy\\xc47\\t\\xc3\\xab\\xc8\\xf0\\xc9\\x82\\x8b\\x1f\\xa8\\xde\\x01\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\x81>\\x94\\x97\\xf3\\x00\\x00\\x00\\xba\\x02\\x00\\x00\\x1a\\x00\\x08\\x01xl/_rels/workbook.xml.rels \\xa2\\x04\\x01(\\xa0\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xacRMK\\xc40\\x10\\xbd\\x0b\\xfe\\x870w\\x9bv\\x15\\x11\\xd9t/\"\\xecU\\xeb\\x0f\\x08\\xc9\\xb4)\\xdb&!3~\\xf4\\xdf\\x1b*\\xba]X\\xd6K/\\x03o\\x86y\\xef\\xcd\\xc7v\\xf75\\x0e\\xe2\\x03\\x13\\xf5\\xc1+\\xa8\\x8a\\x12\\x04z\\x13l\\xef;\\x05o\\xcd\\xf3\\xcd\\x03\\x08b\\xed\\xad\\x1e\\x82G\\x05\\x13\\x12\\xec\\xea\\xeb\\xab\\xed\\x0b\\x0e\\x9as\\x13\\xb9>\\x92\\xc8,\\x9e\\x148\\xe6\\xf8(%\\x19\\x87\\xa3\\xa6\"D\\xf4\\xb9\\xd2\\x864j\\xce0u2js\\xd0\\x1d\\xcaMY\\xde\\xcb\\xb4\\xe4\\x80\\xfa\\x84S\\xec\\xad\\x82\\xb4\\xb7\\xb7 \\x9a)f\\xe5\\xff\\xb9C\\xdb\\xf6\\x06\\x9f\\x82y\\x1f\\xd1\\xf3\\x19\\tI<\\ry\\x00\\xd1\\xe8\\xd4!+\\xf8\\xc1E\\xf6\\x08\\xf2\\xbc\\xfcfMy\\xcek\\xc1\\xa3\\xfa\\x0c\\xe5\\x1c\\xabK\\x1e\\xaa5=|\\x86t \\x87\\xc8G\\x1f\\x7f)\\x92s\\xe5\\xa2\\x99\\xbbU\\xef\\xe1tB\\xfb\\xca)\\xbf\\xdb\\xf2,\\xcb\\xf4\\xeff\\xe4\\xc9\\xc7\\xd5\\xdf\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00s\\xdf\\x1f\\xde-\\x02\\x00\\x00\\x82\\x04\\x00\\x00\\x0f\\x00\\x00\\x00xl/workbook.xml\\xacTM\\x8f\\xd30\\x10\\xbd#\\xf1\\x1f,\\xdf\\xdb|4\\xed\\xb6Q\\xd3\\xd5\\xf6\\x03\\xb1\\x12B\\xd5Rv/{q\\x9dIc\\xea\\xd8\\xc1vh+\\xc4\\x7fg\\x9cP(\\xf4\\xb2\\x08.\\x1e{l\\xbf\\x99yo\\xec\\xe9\\xed\\xb1\\x92\\xe4\\x0b\\x18+\\xb4\\xcah\\xd4\\x0f)\\x01\\xc5u.\\xd4.\\xa3\\x1f7ozcJ\\xacc*gR+\\xc8\\xe8\\t,\\xbd\\x9d\\xbd~5=h\\xb3\\xdfj\\xbd\\'\\x08\\xa0lFK\\xe7\\xea4\\x08,/\\xa1b\\xb6\\xafkP\\xb8ShS1\\x87K\\xb3\\x0blm\\x80\\xe5\\xb6\\x04p\\x95\\x0c\\xe20\\x1c\\x05\\x15\\x13\\x8av\\x08\\xa9y\\t\\x86.\\n\\xc1a\\xa9yS\\x81r\\x1d\\x88\\x01\\xc9\\x1c\\xa6oKQ\\xdb3Z\\xc5_\\x02W1\\xb3o\\xea\\x1e\\xd7U\\x8d\\x10[!\\x85;\\xb5\\xa0\\x94T<\\xbd\\xdf)m\\xd8Vb\\xd9\\xc7hxF\\xc6\\xe9\\x15t%\\xb8\\xd1V\\x17\\xae\\x8fPA\\x97\\xe4U\\xbdQ\\x18DQW\\xf2lZ\\x08\\t\\x8f\\x1d\\xed\\x84\\xd5\\xf5{V\\xf9(\\x92\\x12\\xc9\\xac[\\xe5\\xc2A\\x9e\\xd1\\x11.\\xf5\\x01~s\\x98\\xa6\\x9e7B\\xe2n\\x94$qH\\x83\\xd9O)\\xd6\\x86\\xe4P\\xb0F\\xba\\r\\x8ap\\x86\\xc7\\x83\\xa3$\\x8c\"\\x7f\\x12\\x8b\\xba\\x93\\x0e\\x8cb\\x0e\\x16Z9\\xe4\\xf0\\x07\\xfb\\xff\\xcaW\\x8b\\xbd(5\\xaaC\\x1e\\xe0s#\\x0c`Sx\\xdafS\\x1c\\x19O\\xd9\\xd6\\xae\\x99+Ic$vV\\xfa\\xbc6\\xfa\\x13p\\xb7d\\x8eYp\\xcf\\x17\\xf4\\xb2k\\xed\\xfe\\x82`\\xc6}\\xa5\\x01\\x96\\xda\\xa5\\xd3\\xcd\\xff,{6\\xf5\\xcd\\xfb(\\xe0`\\x7f\\x11\\xe8\\x97\\xe4\\xf8$T\\xae\\x0f\\x19\\xc5\\xa7p\\xba\\x98\\x1fZ\\xf7\\x93\\xc8]\\x99\\xd1x<\\x0eq\\xbf\\xf3\\xbd\\x05\\xb1+\\x1d\\xf2\\x1c\\x0f\\xd0\\xe9\\x83_`\\xb7\\xfd\\x8e1ZKT\\xab\\xf3\\x07\\xff\\x06\"|X\\xde\\xde{))1\\xa9\\xc0\\x89\\xb9\\xcf[\\xa1\\x82\\xf35\\xce$G]\\xbdi\\x0f\\x8e\\xe2I4\\xf01\\xe0\\xe8\\xdeY\\xd7Z\\xa4Td\\xf4k\\x94\\x84w7\\xe1$\\xe9\\x85\\xab\\xc1\\xb0\\x97\\x8c\\'qo\\x9c\\x0c\\xe2\\xde\"Y\\xc6\\xab\\xe1\\xcdj\\xb9\\x9a\\x0f\\xbf\\xfd\\xdf.Fe\\xd3\\xf3G\\xe0\\xb3,\\x99q\\x1b\\xc3\\xf8\\x1e\\xbf\\x8f\\x07(\\xe6\\xa8\\xac/\\xceS\\x82\\xf9vc\\x9bup\\xbe5\\xfb\\x0e\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x007\\xa1\\x81\\xca\\xac\\x00\\x00\\x00\\xce\\x00\\x00\\x00\\x14\\x00\\x00\\x00xl/sharedStrings.xml4\\x8eM\\n\\xc20\\x10\\x85\\xf7\\x82w\\x08\\xb3\\xb7\\xa9.DJ\\x9a.\\x04/\\xa0\\x1e \\xa4c\\x1bH&5\\x93\\x06\\xbd\\xbdQp\\xf9x?\\xdfS\\xc3+xQ0\\xb1\\x8b\\xd4\\xc3\\xbeiA \\xd98:\\x9az\\xb8\\xdf.\\xbb\\x13\\x08\\xce\\x86F\\xe3#a\\x0fod\\x18\\xf4v\\xa3\\x98\\xb3\\xa8]\\xe2\\x1e\\xe6\\x9c\\x97NJ\\xb63\\x06\\xc3M\\\\\\x90\\xaa\\xf3\\x88)\\x98\\\\e\\x9a$/\\t\\xcd\\xc83b\\x0e^\\x1e\\xda\\xf6(\\x83q\\x04\\xc2\\xc6\\x95r\\xe5\\x82X\\xc9=W<\\xff\\xb5V\\xec\\xb4\\xfa!:^\\x8c\\xad\\xe8\\xba\\xc1\\x98\\n\\x82\\xbe\\xae\\xa9\\xb8b\\xbcP2k%\\xbfIY\\xff\\xe8\\x0f\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00u>\\x99i\\x93\\x06\\x00\\x00\\x8c\\x1a\\x00\\x00\\x13\\x00\\x00\\x00xl/theme/theme1.xml\\xecY[\\x8b\\xdbF\\x14~/\\xf4?\\x08\\xbd;\\xbeI\\xb2\\xbd\\xc4\\x1bl\\xd9N\\xda\\xec&!\\xeb\\xa4\\xe4ql\\x8f\\xad\\xc9\\x8e4F3\\xde\\x8d\\t\\x81\\x92<\\xf5\\xa5PHK_\\n}\\xebC)\\r4\\xd0\\xd0\\x97\\xfe\\x98\\x85\\x846\\xfd\\x11=3\\x92\\xad\\x99\\xf58\\x9b\\xcb\\xa6\\xb4%kX\\xa4\\xd1w\\xce|s\\xce\\xd17\\x17]\\xbct/\\xa6\\xce\\x11N9aI\\xdb\\xad^\\xa8\\xb8\\x0eN\\xc6lB\\x92Y\\xdb\\xbd5\\x1c\\x94\\x9a\\xae\\xc3\\x05J&\\x88\\xb2\\x04\\xb7\\xdd%\\xe6\\xee\\xa5\\xdd\\x8f?\\xba\\x88vD\\x84c\\xec\\x80}\\xc2wP\\xdb\\x8d\\x84\\x98\\xef\\x94\\xcb|\\x0c\\xcd\\x88_`s\\x9c\\xc0\\xb3)Kc$\\xe06\\x9d\\x95\\'):\\x06\\xbf1-\\xd7*\\x95\\xa0\\x1c#\\x92\\xb8N\\x82bp{}:%c\\xec\\x0c\\xa5Kww\\xe5\\xbcO\\xe16\\x11\\\\6\\x8ciz ]c\\xc3Ba\\'\\x87U\\x89\\xe0K\\x1e\\xd2\\xd49B\\xb4\\xedB?\\x13v<\\xc4\\xf7\\x84\\xebP\\xc4\\x05<h\\xbb\\x15\\xf5\\xe7\\x96w/\\x96\\xd1NnD\\xc5\\x16[\\xcdn\\xa0\\xfer\\xbb\\xdc`rXS}\\xa6\\xb3\\xd1\\xbaS\\xcf\\xf3\\xbd\\xa0\\xb3\\xf6\\xaf\\x00Tl\\xe2\\xfa\\x8d~\\xd0\\x0f\\xd6\\xfe\\x14\\x00\\x8d\\xc70\\xd2\\x8c\\x8b\\xee\\xd3\\xef\\xb6\\xba=?\\xc7j\\xa0\\xec\\xd2\\xe2\\xbb\\xd7\\xe8\\xd5\\xab\\x06^\\xf3_\\xdf\\xe0\\xdc\\xf1\\xe5\\xcf\\xc0+P\\xe6\\xdf\\xdb\\xc0\\x0f\\x06!D\\xd1\\xc0+P\\x86\\xf7-1i\\xd4B\\xcf\\xc0+P\\x86\\x0f6\\xf0\\x8dJ\\xa7\\xe75\\x0c\\xbc\\x02E\\x94$\\x87\\x1b\\xe8\\x8a\\x1f\\xd4\\xc3\\xd5h\\xd7\\x90)\\xa3W\\xac\\xf0\\x96\\xef\\r\\x1a\\xb5\\xdcy\\x81\\x82jXW\\x97\\xecb\\xca\\x12\\xb1\\xad\\xd6bt\\x97\\xa5\\x03\\x00H E\\x82$\\x8eX\\xce\\xf1\\x14\\x8d\\xa1\\x8aCD\\xc9(%\\xce\\x1e\\x99EPxs\\x940\\x0e\\xcd\\x95ZeP\\xa9\\xc3\\x7f\\xf9\\xf3\\xd4\\x95\\x8a\\x08\\xda\\xc1H\\xb3\\x96\\xbc\\x80\\t\\xdfh\\x92|\\x1c>N\\xc9\\\\\\xb4\\xddO\\xc1\\xab\\xabA\\x9e?{v\\xf2\\xf0\\xe9\\xc9\\xc3_O\\x1e=:y\\xf8s\\xde\\xb7re\\xd8]A\\xc9L\\xb7{\\xf9\\xc3W\\x7f}\\xf7\\xb9\\xf3\\xe7/\\xdf\\xbf|\\xfcu\\xd6\\xf5i<\\xd7\\xf1/~\\xfa\\xe2\\xc5o\\xbf\\xbf\\xca=\\x8c\\xb8\\x08\\xc5\\xf3o\\x9e\\xbcx\\xfa\\xe4\\xf9\\xb7_\\xfe\\xf1\\xe3c\\x8b\\xf7N\\x8aF:|Hb\\xcc\\x9dk\\xf8\\xd8\\xb9\\xc9b\\x18\\xa0\\x85?\\x1e\\xa5of1\\x8c\\x101,P\\x04\\xbe-\\xae\\xfb\"2\\x80\\xd7\\x96\\x88\\xdap]l\\x86\\xf0v\\n*c\\x03^^\\xdc5\\xb8\\x1eD\\xe9B\\x10K\\xcfW\\xa3\\xd8\\x00\\xee3F\\xbb,\\xb5\\x06\\xe0\\xaa\\xecK\\x8b\\xf0p\\x91\\xcc\\xec\\x9d\\xa7\\x0b\\x1dw\\x13\\xa1#[\\xdf!J\\x8c\\x04\\xf7\\x17s\\x90Wbs\\x19F\\xd8\\xa0y\\x83\\xa2D\\xa0\\x19N\\xb0p\\xe43v\\x88\\xb1etw\\x081\\xe2\\xbaO\\xc6)\\xe3l*\\x9c;\\xc4\\xe9\"b\\r\\xc9\\x90\\x8c\\x8cB*\\x8c\\xae\\x90\\x18\\xf2\\xb2\\xb4\\x11\\x84T\\x1b\\xb1\\xd9\\xbf\\xedt\\x19\\xb5\\x8d\\xba\\x87\\x8fL$\\xbc\\x16\\x88Z\\xc8\\x0f15\\xc2x\\x19-\\x04\\x8am.\\x87(\\xa6z\\xc0\\xf7\\x90\\x88l$\\x0f\\x96\\xe9X\\xc7\\xf5\\xb9\\x80L\\xcf0eN\\x7f\\x829\\xb7\\xd9\\\\Oa\\xbcZ\\xd2\\xaf\\x82\\xc2\\xd8\\xd3\\xbeO\\x97\\xb1\\x89L\\x059\\xb4\\xf9\\xdcC\\x8c\\xe9\\xc8\\x1e;\\x0c#\\x14\\xcf\\xad\\x9cI\\x12\\xe9\\xd8O\\xf8!\\x94(rn0a\\x83\\xef3\\xf3\\r\\x91\\xf7\\x90\\x07\\x94lM\\xf7m\\x82\\x8dt\\x9f-\\x04\\xb7@\\\\uJE\\x81\\xc8\\'\\x8b\\xd4\\x92\\xcb\\xcb\\x98\\x99\\xef\\xe3\\x92N\\x11V*\\x03\\xdaoHzL\\x923\\xf5\\xfd\\x94\\xb2\\xfb\\xff\\x8c\\xb2\\xdb5\\xfa\\x1c4\\xdd\\xee\\xf8]\\xd4\\xbc\\x93\\x12\\xeb;u\\xe5\\x94\\x86o\\xc3\\xfd\\x07\\x95\\xbb\\x87\\x16\\xc9\\r\\x0c/\\xcb\\xe6\\xcc\\xf5A\\xb8?\\x08\\xb7\\xfb\\xbf\\x17\\xeem\\xef\\xf2\\xf9\\xcbu\\xa1\\xd0 \\xde\\xc5Z]\\xad\\xdc\\xe3\\xad\\x0b\\xf7)\\xa1\\xf4@,)\\xde\\xe3j\\xed\\xcea^\\x9a\\x0c\\xa0Qm*\\xd4\\xcer\\xbd\\x91\\x9bGp\\x99o\\x13\\x0c\\xdc,E\\xca\\xc6I\\x99\\xf8\\x8c\\x88\\xe8 BsX\\xe0W\\xd56t\\xc6s\\xd73\\xee\\xcc\\x19\\x87u\\xbfjV\\x1bb|\\xca\\xb7\\xda=,\\xe2}6\\xc9\\xf6\\xab\\xd5\\xaa\\xdc\\x9bf\\xe2\\xc1\\x91(\\xda+\\xfe\\xba\\x1d\\xf6\\x1a\"C\\x07\\x8db\\x0f\\xb6v\\xafv\\xb53\\xb5W^\\x11\\x90\\xb6oBB\\xeb\\xcc$Q\\xb7\\x90h\\xac\\x1a!\\x0b\\xaf\"\\xa1Fv.,Z\\x16\\x16M\\xe9~\\x95\\xaaU\\x16\\xd7\\xa1\\x00j\\xeb\\xac\\xc0\\xc2\\xc9\\x81\\xe5V\\xdb\\xf5\\xbd\\xec\\x1c\\x00\\xb6T\\x88\\xe2\\x89\\xccSv$\\xb0\\xca\\xaeL\\xce\\xb9fz[0\\xa9^\\x01\\xb0\\x8aXU@\\x91\\xe9\\x96\\xe4\\xbauxrtY\\xa9\\xbdF\\xa6\\r\\x12Z\\xb9\\x99$\\xb42\\x8c\\xd0\\x04\\xe7\\xd5\\xa9\\x1f\\x9c\\x9cg\\xae[EJ\\rz2\\x14\\xab\\xb7\\xa1\\xa0\\xd1h\\xbe\\x8f\\\\K\\x119\\xa5\\r4\\xd1\\x95\\x82&\\xceq\\xdb\\r\\xea>\\x9c\\x8d\\x8d\\xd1\\xbc\\xedNa\\xdf\\x0f\\x97\\xf1\\x1cj\\x87\\xcb\\x05/\\xa238<\\x1b\\x8b4{\\xe1\\xdfFY\\xe6)\\x17=\\xc4\\xa3,\\xe0Jt25\\x88\\x89\\xc0\\xa9CI\\xdcv\\xe5\\xf0\\xd7\\xd5@\\x13\\xa5!\\x8a[\\xb5\\x06\\x82\\xf0\\xaf%\\xd7\\x02Y\\xf9\\xb7\\x91\\x83\\xa4\\x9bI\\xc6\\xd3)\\x1e\\x0b=\\xedZ\\x8b\\x8ctv\\x0b\\n\\x9fi\\x85\\xf5\\xa92\\x7f{\\xb0\\xb4d\\x0bH\\xf7A49vFt\\x91\\xdeDPb~\\xa3*\\x038!\\x1c\\x8e\\x7f\\xaaY4\\'\\x04\\xce3\\xd7BV\\xd4\\xdf\\xa9\\x89)\\x97]\\xfd@Q\\xd5P\\xd6\\x8e\\xe8<B\\xf9\\x8c\\xa2\\x8by\\x06W\"\\xba\\xa6\\xa3\\xee\\xd61\\xd0\\xee\\xf21C@7C8\\x9a\\xc9\\t\\xf6\\x9dg\\xdd\\xb3\\xa7j\\x199M4\\x8b9\\xd3P\\x159k\\xda\\xc5\\xf4\\xfdM\\xf2\\x1a\\xabb\\x125Xe\\xd2\\xad\\xb6\\r\\xbc\\xd0\\xba\\xd6J\\xeb\\xa0P\\xad\\xb3\\xc4\\x19\\xb3\\xeekL\\x08\\x1a\\xb5\\xa23\\x83\\x9ad\\xbc)\\xc3R\\xb3\\xf3V\\x93\\xda9.\\x08\\xb4H\\x04[\\xe2\\xb6\\x9e#\\xac\\x91x\\xdb\\x99\\x1f\\xecNW\\xad\\x9c V\\xebJU\\xf8\\xea\\xc3\\x87\\xfem\\x82\\x8d\\xee\\x82x\\xf4\\xe0\\x14xA\\x05W\\xa9\\x84/\\x0f)\\x82E_v\\x8e\\x9c\\xc9\\x06\\xbc\"\\xf7D\\xbeF\\x84+g\\x91\\x92\\xb6{\\xbf\\xe2w\\xbc\\xb0\\xe6\\x87\\xa5J\\xd3\\xef\\x97\\xbc\\xbaW)5\\xfdN\\xbd\\xd4\\xf1\\xfdz\\xb5\\xefW+\\xbdn\\xed\\x01L,\"\\x8a\\xab~\\xf6\\xd1e\\x00\\x07Qt\\x99\\x7fzQ\\xed\\x1b\\x9f_\\xe2\\xd5Y\\xdb\\x851\\x8b\\xcbL}^)+\\xe2\\xea\\xf3K\\xb5\\xb6\\xfd\\xf3\\x8bC@t\\xee\\x07\\xb5A\\xab\\xde\\xea\\x06\\xa5V\\xbd3(y\\xbdn\\xb3\\xd4\\n\\x83n\\xa9\\x17\\x84\\x8d\\xde\\xa0\\x17\\xfa\\xcd\\xd6\\xe0\\x81\\xeb\\x1c)\\xb0\\xd7\\xa9\\x87^\\xd0o\\x96\\x82j\\x18\\x96\\xbc\\xa0\"\\xe97[\\xa5\\x86W\\xabu\\xbcF\\xa7\\xd9\\xf7:\\x0f\\xf2e\\x0c\\x8c<\\x93\\x8f<\\x16\\x10^\\xc5k\\xf7o\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00U3$\\xcfe\\x03\\x00\\x00\\xc3\\n\\x00\\x00\\r\\x00\\x00\\x00xl/styles.xml\\xc4Vm\\x8f\\x9b8\\x10\\xfe~R\\xff\\x83\\xe5\\xef,\\x98\\x00\\x85(\\xa4j6\\x8bT\\xa9w:i\\xf7\\xa4~u\\xc0$\\xd6\\xfa\\x05\\x81\\xb3Mz\\xba\\xff~c\\x03\\t\\xedf_\\xb4\\xed\\xe9\\xf2%x\\x18?\\xf3\\xcc<\\xe3\\xc1\\x8b\\x0f\\x07)\\xd0\\x03k;\\xaeU\\x8e\\xc9U\\x80\\x11S\\xa5\\xae\\xb8\\xda\\xe6\\xf8\\xaf\\xbb\\xc2K1\\xea\\x0cU\\x15\\x15Z\\xb1\\x1c\\x1fY\\x87?,\\xdf\\xfd\\xb6\\xe8\\xccQ\\xb0\\xdb\\x1dc\\x06\\x01\\x84\\xear\\xbc3\\xa6\\x99\\xfb~W\\xee\\x98\\xa4\\xdd\\x95n\\x98\\x827\\xb5n%5\\xb0l\\xb7~\\xd7\\xb4\\x8cV\\x9d\\xdd$\\x85\\x1f\\x06A\\xe2K\\xca\\x15\\xee\\x11\\xe6\\xb2|\\r\\x88\\xa4\\xed\\xfd\\xbe\\xf1J-\\x1bj\\xf8\\x86\\x0bn\\x8e\\x0e\\x0b#Y\\xce?m\\x95n\\xe9F\\x00\\xd5\\x03\\x89h\\x89\\x0e$i\\xc31\\x823=\\n\"y\\xd9\\xeaN\\xd7\\xe6\\n@}]\\xd7\\xbcd\\x8f\\xb9f~\\xe6\\xd3\\xf2\\x8c\\x04\\xb0oC\"\\xb1\\x1f\\x84}\\xe2\\xcbE\\xad\\x95\\xe9P\\xa9\\xf7\\xca\\xe4\\xd8\\xf2\\xb4\\xa4\\xe7\\xf7J\\x7fU\\x85}\\x05\\x9a\\xe0\\xdek\\xb9\\xe8\\xbe\\xa1\\x07*\\xc0B\\xb0\\xbf\\\\\\x94Z\\xe8\\x16\\x19(6\\xe4\\xea,\\x8aJ\\xd6{\\\\S\\xc17-\\xb7n5\\x95\\\\\\x1c{sh\\rN\\x9f\\xc1Or\\xa8\\x965\\xfa\\x96\\xc7\\x18gc\\xbd\\x9e\\x8e\\x15\\xd8\\r?\\x1f\\xcb\\x85\\xec &\\x17\\xe2T\\x81\\xd8&\\x0b\\x86\\xe5\\x02\\xc45\\xacU\\x05,\\xd0\\xf0|wl U\\x05}\\xd8Sv~/xo[z$a\\xfc\\xfa\\r\\x9d\\x16\\xbc\\xb2,\\xb6\\xd7\\xd3\\x02G\\x16as\\xc1\\xe6O\\x88\\xda:\\xbe\\x86\\xd4\\x931\\x90\\xe1\\xb6\\x0f\\x82\\xab8\\x83\\xdf,\\xcd\\x920KI\\x10\\xa5.\\x81G\\xe1\\x9fw\\xffo\\x98\\xbd\\x07b)I\\xd24\\xcd\\xa2\\x19\\x89\"\\xd7SO3\\xbb\\xe8~\\x91\\x99+\\x1d\\xf4\\xc3F\\xb7\\x15\\xcc\\xa3\\xf1L\\xcc@\\x8b\\xde\\xb4\\\\\\x08V\\x1b\\x90\\xa1\\xe5\\xdb\\x9d\\xfd7\\xba\\xb1\\xa2hc\\xe0\\xd8.\\x17\\x15\\xa7[\\xad\\xa8\\xb0\\xed<\\xee\\x98\\xee\\x849\\x06#+\\xc7f\\x07#\\xe7\\x87\\xf3\\xe3z\\xda\\xb7\\xf8\\x03\\xfc\\xcb\\xce\\x8e\\xc5\\xf7$&\\x9b\\xca\\xfb\\xcb!z\\xb6\\xff#\\xd9\\x97\\x13\\x83\\xb2\\x8eU}\\xd9\\xf9\\xb9|\\x06\\x15@\\xd3\\x92\\tqk\\xab\\xff\\xa5>\\tk\\xe7\\xda\\xa1Fj/\\x0bi>U\\xd0\\xf5\\x18\\xd9)4>\\xc2A\\x1a\\x1e{\\x11\\xfb\\x85\\x15w\\x8a\\xd6cO`\\xa37\\xc1\\xa2C}\\xc2\\x7f\\x8a\\x14\\x01~\\x03)\\x18\\xd4gR`\\x1fw#\\xda4\\xe2h\\x07\\xb7\\x1d\\xc9\\xc3\\n\\xf6\\x9cW+\\xd7\\xdd\\xc3\\xc0~E\\xfa\\xb3i$\\xfb}\\x18x\\xfe\\xfaH\\xd1\\xaf\\x89\\xe4\\xd4\\x01=&\\xa2\\x7f\\'\\xf9I<d?!9\\xfe\\xc3\\xde\\x0f\\xc4$\\xb1\\xcd\\x9e\\x0b\\x18\\x83\\x17\\xe4\\x06\\xcc\\xeapn wf\\x8d\\xfd\\xd6\\xbb\\xd6:E\\x81>\\xaaXM\\xf7\\xc2\\xdc\\x9d^\\xe6\\xf8\\xfc\\xfc;\\xab\\xf8^B-\\x07\\xaf?\\xf9\\x836\\x0e\"\\xc7\\xe7\\xe7\\xcfv\\xc8\\x90\\xc4\\x8e}v0\\x9f;\\x18\\x0c\\xf0\\x8f\\xf6-\\xcf\\xf1\\xdf7\\xab\\xf7\\xd9\\xfa\\xa6\\x08\\xbd4X\\xa5^4c\\xb1\\x97\\xc5\\xab\\xb5\\x17G\\xd7\\xab\\xf5\\xba\\xc8\\x820\\xb8\\xfegr\\xe9\\xf8\\x89+\\x87\\xbb \\xc1I!\\xd1\\xbc\\x13p1i\\x87d\\x07\\xf2\\xb7g[\\x8e\\'\\x8b\\x9e\\xbe\\xfb\\xec\\x01\\xed)\\xf7,L\\x82\\x8f1\\t\\xbcb\\x16\\x10/Jh\\xea\\xa5\\xc9,\\xf6\\x8a\\x98\\x84\\xeb$Z\\xdd\\xc4E<\\xe1\\x1e\\xbf\\xf1\\x92\\x13\\xf8\\x84\\x8c\\x97\\x9c\\x03\\x89\\xe7\\x86K&\\xb8\\x1a\\xb5\\x1a\\x15\\x9aZA$X>\\x93\\x84?*\\xe1\\x9fo\\x9f\\xcb\\x7f\\x01\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\x0b\\xf4\\xf4\\xac\\xdb\\x17\\x00\\x00z\\xb6\\x00\\x00\\x18\\x00\\x00\\x00xl/worksheets/sheet1.xml\\x9c\\x9d_o\\x1b;\\x92\\xc5\\xdf\\x17\\xd8\\xef`\\xe8}\\xda\\xe2\\x9fV\\xb7\\x82\\xd8\\x83\\xbb{1\\xd8yX`13\\xbb\\xef\\x8a\\xa3\\xc4Fl\\xcbk\\xe9&s\\xbf\\xfd\\x9ef\\xa1)\\x96\\xaf\\x8f\\xd2g\\x03\\x0c\\x10g\\xce\\xa5h\\x8aU,\\x16\\x7f,~\\xfc\\xf3?\\x9f\\x1e\\xaf\\xbe\\xef_\\x8f\\x0f\\x87\\xe7\\x9bU\\xe8\\xd6\\xab\\xab\\xfd\\xf3\\xdd\\xe1\\xf3\\xc3\\xf3\\xd7\\x9b\\xd5\\x7f\\xff\\xe3/\\x7f\\x1aWW\\xc7\\xd3\\xee\\xf9\\xf3\\xee\\xf1\\xf0\\xbc\\xbfY\\xfd\\xbe?\\xae\\xfe|\\xfb\\xaf\\xff\\xf2\\xf1\\xc7\\xe1\\xf5\\xdb\\xf1~\\xbf?]\\xa1\\x85\\xe7\\xe3\\xcd\\xea\\xfetz\\xf9p}}\\xbc\\xbb\\xdf?\\xed\\x8e\\xdd\\xe1e\\xff\\x8c\\xff\\xe7\\xcb\\xe1\\xf5iw\\xc2\\x8f\\xaf_\\xaf\\x8f/\\xaf\\xfb\\xdd\\xe7\\xf2\\x1f==^\\xc7\\xf5zs\\xfd\\xb4{x^Y\\x0b\\x1f^\\x97\\xb4q\\xf8\\xf2\\xe5\\xe1n\\xff\\xeb\\xe1\\xee\\xb7\\xa7\\xfd\\xf3\\xc9\\x1ay\\xdd?\\xeeN\\xe8\\xff\\xf1\\xfe\\xe1\\xe58\\xb7\\xf6t\\xb7\\xa4\\xb9\\xa7\\xdd\\xeb\\xb7\\xdf^\\xfetwxzA\\x13\\x9f\\x1e\\x1e\\x1fN\\xbf\\x97FWWOw\\x1f\\xfe\\xfa\\xf5\\xf9\\xf0\\xba\\xfb\\xf4\\x88\\xdf\\xfb\\x9f!\\xef\\xee\\xe6\\xb6\\xcb\\x0f\\x7fh\\xfe\\xe9\\xe1\\xee\\xf5p<|9uh\\xee\\xda:\\xfa\\xc7\\xdfy{\\xbd\\xbdFK\\xb7\\x1f??\\xe07\\x98\\x86\\xfd\\xeau\\xff\\xe5f\\xf5K\\xf8\\xf0K\\x1f\\xc6\\xd5\\xf5\\xed\\xc72B\\xff\\xf3\\xb0\\xffql\\xfe~u\\xda}\\xfa\\xfb\\xfeq\\x7fw\\xda\\x7f\\xc6\\x17\\xb5\\xba\\x9a\\xbe\\x80O\\x87\\xc3\\xb7I\\xf8W\\xfc\\xd3\\x1am\\x1e\\x8b`jswwz\\xf8\\xbe\\xff\\xf7\\xfd\\xe3\\xe3\\xcd\\xeaoa\\x83/\\xf1\\x7f\\xcb\\xc7L\\x7f\\xc7G\\\\\\xd7\\xcfh\\xff>\\x7f\\xde_\\xca\\x97\\xf6_\\xafW\\x9f\\xf7_v\\xbf=\\x9e\\xfev\\xf8\\xf1\\x1f\\xfb\\x87\\xaf\\xf7\\'|p\\x8fA\\x98\\xc6\\xe2\\xc3\\xe7\\xdf\\x7f\\xdd\\x1f\\xef\\xf0%\\xe0\\xa3\\xbb\\xd8\\xd7\\x8e\\xff\\xba;\\xedn?\\xbe\\x1e~\\\\\\xe1\\x0bE?\\x8f/\\xbbiz\\x84\\x0f\\xf8\\xbb\\xb5\\xd0\\rh\\xe4t\\xffp\\xf7\\xed\\xdf\\x0eS\\x93\\xef\\xb6\\x98\\xf0\\xeb\\xdcMM\\xfc2\\xb5QT\\xd0\\x1e\\xf1\\xaf\\xdfo\\xd7\\x1f\\xaf\\xbf\\xa3\\xdfw\\xf8\\x1f>\\xa7~X\\xbc\\xfca\\xff8\\xbc\\xb0\\x0fC\\xf7\\xe7O\\x9b\\x1a\\xb9Y\\xc5K\\x9f\\x93\\xfc\\xe7\\xbc?\\x1cs{\\x93\\xf8f5\\xfd6\\xdfo\\xfbu\\xb7N\\xef\\xf7=+mN\\xe2\\xb9\\x8f\\xa9\\xdb\\x8e\\xef7\\x89An\\xc7\\xfer7\\'\\xf1\\xdc\\xcd\\xb0\\xed\\xb6y\\xdd\\xfc\\t\\xef\\x7f\\xc04\\xab\\x9a/\\xf7\\xf2\\x07L\\xe2\\xb9\\xcf\\x01\\x9d\\xde\\xbe\\xdf\\xe6\\xa0\\xb49\\x89\\xe7N\\xe7\\xdcm\\x87\\xf7\\xdb\\x9c\\\\\\xd8\\xe2~N\\xe2:\\xb6\\x1b\\xda\\xcf\\xad\\xd2\\xe6$\\x9e\\xfbI\\xe6n\\x80\\xe3]\\xde\\xc9\\xa2\\x9e{9}[\\xdb\\xf6\\x0f\\x99b\\xe1\\x8d1^\\xfe\\xbe\\x8az\\xeet\\xcc\\xdd\\x9aL\\x82\\xf0\\xc6\\xea~\\xd2jk^y\\xe0\\xadJ6\\x16Z#\\x1b\\xbbu|\\x7f\\x1e\\x04\\xc9\\xc8\\x8a\\xba\\xce\\xd8\\x11f\\xd6\\x8e1\\xb1\\xb9\\xc9=\\n_cku\\x11\\xce!\\x93\\x8eK\\x96V\\xbc\\xfdO\\\\X\\x90\\xec\\xac\\xa8\\xeb\\\\\\xa06\\x81\\xd5K\\xf9\\xe5[K\\x0b\\xb0\\xde\\x9e\\xfc\\xf2\\x92\\xa9\\x85\\xd6\\xd6\\x88\\x8f\\x89\\x92\\xad\\x15\\xf5<\\x9c\\x11\\xc6\\xb6!\\xab\\x8fd^\\xd1V5[\\x17B\\xa0c\\x1a%\\xf3*\\xea\\xb6\\xafl\\x04$\\xf3\\x8a\\xadyE\\xf4\\x95L\\xfe(\\xd9WQ\\xd7\\xbe\\xf6|\\x04$\\x93\\xc2J~\\xf6\\xb5\\xf1B\\xab\\x92I\\xc5v\\xf5\\x8ad\\xa2F\\xc9\\xa6\\x8a\\xba\\xda\\x14kR\\xb2\\xa8\\xd8ZT\\xe4\\x81A\\x94,\\xaa\\xa8kh\\xd0S\\x97\\x9d$\\xab*\\xea\\xea]/\\xb4*YUj\\xad\\n.\\x85\\x98j\\x92\\x8c\\xaa\\xa8\\xebD\\x9d\\x16\\x82\\xf7\\x1d@\\xd2\\xe2BgT\\x91\\x0f\\xabdT\\xa9\\r\\r\\xb7X\\t]`@\\x96\\x97$YXQ\\x9f\\x17\\x03\\xb6f%\\xc9\\xc0\\x8a\\xba\\xaeY]f_\\x9cdb\\xa9\\x8d\\x0f\\x87.\\x11g\\x98$#+\\xea\\xb9\\xa7\\xf0/l2H6\\x96\\xdaUk\\xec6$\\x90\\xcd\\x92\\x89\\x15u\\r\\x12\\x07\\xde\\xaadb\\xd9-\\\\\\x08ZX_%\\x1b\\xcbm\\\\\\x98\\xd0W2\\xacY\\xb2\\xb1\\xa2\\xae\\x1b\\x04\\x98\\x03\\x89a\\xb3\\xb6\\xfbjm,\\x8e]b}\\x95\\xcc*\\xb7\\x0bW\\xea\\xb1\\xa3&\\x1bE\\xc9\\xae\\xb2\\xdbv\\x85n\\xc3\\xb6\\x9f\\x92a\\xe57\\x1b\\xaf\\x9e\\x8d\\x80dY\\xf9M@8\\x900>K\\xa6U\\xd4\\xd5]m\\xba\\xc8v\\xcb\\x92m\\xf5\\x93z\\xb6\\xad\\xdc%b\\x04\\xbddZE]\\xa7k\\xea\\xc8\\x0c\\xe8%\\xcb*\\xea\\xdaSlh\\xc9\"\\xd0K\\x96U\\xd4\\xcd\\xd6\\x9b\\xd9@/YVQ\\xd7\\xbeb\\xfd&\\xf6\\xdak\\xa9\\rgY\\xd8\\x1e\\xb2/K\\xb2\\xac\\xbe\\xb5\\xac\\x8c\\xf5\\x9b}[\\x92e\\xf5\\xce\\xb2b7\\xb0oK\\xb2\\xac\\xde%5\\xba@\\x16\\xc2^2\\xac\\xa2\\xfeY\\x00\\xbb\\x91\\xac\\xaa\\xa8k\\xf2e\\xe8Bh\\xc3\\x17\\xf2\\xb5m$\\x1b+\\xea\\xb9\\xd7i\\xec\"\\xcbmIF\\xb6q\\xcb\\x17\\x92%\\xc4\\xcdn$#+\\xea\\xda\\xd7\\xd4\\r\\xac\\xaf\\x92\\x91m\\xdc\\xf2\\xb5\\xe9z2\\xc56\\x92\\x91\\x15u\\xd3W\\xe6f7Z\\xd2\\xd0\\xed\\xbb\\xb0\\xd4\\x12#\\xdbHFV\\xd4u\\xe6\\x0et\\xe7\\xb9\\x91\\x8c\\xac\\xa8\\xeb\\x8ef\\xdb\\xf5\\xac\\xaf\\x92\\x95m\\xda\\xc8p\\xda\\xd3\\x91Eq\\x90\\x0c\\xad\\xa8\\xeb\\x96f\\xe8\\x06\\x12o\\x0f\\x92m\\x15u\\x1d\\xd7\\xd4\\x05b\\x05\\x83d[E]\\xfb\\x8a\\x10\\x86\\x8d\\x80d[\\x83\\xdb~\\xa1\\xafl\\x04$\\xdb\\x1aZ\\xdb\\x1a\\xb1; \\xb65\\x1d\\x80,O\\x13\\x16u\\xb5\\xad\\xd8e6\\x02\\x92m\\r\\xadmmB\\x17Y_\\xb5\\x94|\\xbb\\x80\\x85M\\x97\\xd9\\x1c\\x90lkp\\x99\\r,\\x8b\\xe4\\xf0@2\\xad\\xa15-\\x9cydb\\xb0\\xa3dZE]\\xdd\\x00\\\\\\x169\\x93\\x90,kl7]X\\x17\\xc9d\\x1d%\\xc3*\\xea\\xba\\xdanh\\xf6y\\x94\\x0c\\xab\\xa8k\\xbah\\xecF\\xd6W\\xc9\\xb0\\xc67{.\\xb6\\xed\\x1e%\\xc3*\\xea\\xea\\xb0\\xb6\\x1d\\xb1\\xabQ\\xb2\\xab\\xa2\\xae\\xfe*Q\\x7f5JvU\\xd4uXc\\xb7a\\x93U;\\xecj\\xedj\\xdb%\\xf6]Iv5\\xfadF\\x9f\\\\\\x0e\\x8a\\xc40[\\xc9\\xc8\\x8a\\xba\\xc9\\x1e\\xb2\\x93\\xbf\\xaddeE\\xddd:\\x13\\xeb\\xabdf\\xdb66\\x84\\x97!\\x0ea+YYQ\\xd7\\xae\\x86\\xaeg\\xadJV\\xb6m\\xad\\x0c\\x87\\xb4,[\\xb0\\x95\\xac\\xac\\xa8k_S\\x97\\xc9B\\xb3\\x95\\xcc\\xac\\xa8\\xe7)0l\\xba\\xd0\\xb7\\'\\xd6d\"o%\\x9b+\\xea\\xea\\x1e\\xb0q$\\xfb\\xa6\\xaddsE\\xdd\\xa4\\x93Y\\x9c\\xb8\\xd5\\x8e\\x98\\xddb\\xc6\\xfdCX\\x8b\\x07\\xcdm\\xa2\\x03i\\xa9\\x9e\\x1d\\x03\\xaf\\xb5\\xd3\\xe5\"\\xaf\\t\\x84\\rM\\xf8\\x85\\xb5v\\xbe\\\\\\xe4\\xd53`\\'FfBXk\\'\\xccE\\xde\\xc4`#;c^k\\x87\\xccE>\\xf7\\x17\\xabe;\\x89\\xd7,S\\x11\\xd6\\xda1s\\x917\\xeb\\x1c\\xf3Aa\\xad\\x1d4\\x17y\\x9d\\xca\\xdbn`\\xe4\\xc1Z;n.\\xf2\\xb9\\xbf}\\xdf\\x8d\\xechx\\xad\\x9d8\\x17y\\x9d\\x1c\\x88xh\\x7f\\xb53\\xe7uk|}\\xa2\\xb9\\xdb R\\x1e\\x0e\\xf3\\xc08D\\xe2\\x80\\x82\\x8av\\xb8\\x1c>\\xe2I\\xcf\\x8f\\x90sx\\x91\\xf4(\\xf2:\\xd8<\\xf5\\x1c\\n\\xbd\\xb1\\x98\\xcf1y\\x9d\\xcc\\x91\\xee[\\x82\\x88{8\\xde#^jW3\\xbe\\x82\\x84\\x9c\\xa3\\n\\x9ep\\t\\x05\\xdc\\x10\\x06\\xa2\\xdd\\xc0\\r\\x89\\xbb:\\x11\\xf6p\\xb4\\xc7&\\xd2ta\\x10y\\x8f\"o\\x16\\xbe\\x9114\\x85\\xe1\\x10\\xc6\\xe1\\xff\\x13o\\x06\\r\\x021\\xf9\\xdc\\xf9\\xbcF\\xa8\\xbc 5\\x19\\n\\xe5\\xb1\\xfc7qP\\x08N\\xc1\\xa8\\xbdkTHpXH@\\x16)3GRP\\x0f\\xa1\\xc3\\xfe\\x10;\\xd0\\x9di\\xd0\\xd8\\x10\\x93\\x9f=5\\xe7\\x05C\\x01>\\x84\\x1e\\xb7\\x87\\x01[\\xba;\\r\\x85\\xf8\\x10\\x9au\\x89\\xca5;\\xba\\t\\x1a#b\\xf2\\xd9w\\x0c\\x81f~BA?\\x84\\xee\\xba\\xa3\\xb65\\x9c3#\\xcf4V$\\xbc\\x85EH\\xa2&h\\xb0\\x88\\xc9\\xe7\\xe9\\xb0Yw\\x81E_\\x05\\x00Y>\\x0e\\x9e\\x17\\x01\"@\\xfb\\xabE\\xa1\\x0e\\x19\\x01\\xe6\\xc8\\xdb\\x159\\xc7\\xd6\\xde2\\x12\\x16\\xeck+\\x1c\\x880\\x0e.o\\x89C\\x11\\x17\\x862g\\xa1q#\\xc1\\x81#\\xe8;\\x8b\\xcc5r$8t\\x04\\x1c!oW\\x0bB\\x1d=\\x02\\x8f\\xdf\\xd3\\xb9\\xa1\\x05\\xa1\\x0e \\x19\\x00{\\xb0\\x1dU\\x81B\\x84\\xef\\xd0\\x9d\\x14\\xc0W\\xb0\\xa0Y\\xa3H\\x82\\xc3HF\\xba{\\x0f\\x05\\x0cY\\xde]\\xcf\\x91\\xf48\\xebc\\xd0\\xaefz\\x8e$\\xc9\\xe8/\\x85\\x815\\xd3s,\\t\\\\\\x10#T\\x82\\x06\\x93\\x98|vm\\xfd\\xc0\\x97\\xe6\\x02\\x88\\x08\\xe3\\xdb.t\\xd8\\x93P\\x97Y\\x10\\x11\\xa1\\xddv\\xa5\\xbb\\xd8\\xaefn\\x9e)\\x89\\x1dK\\xbe\\x85\\x82\\x89\\x08\\xfd\\xf5g\\xdf\\x0c)\\xc0\\x16H\\x82\\x97=V\\x82\\xbc\\x08\\xdbJ\\x16Rdyw\\x1dX\\x02.\\x98\"\\xe1\\x1aZ\\x12\\x1c[\\x12\\x10\\xd43\\xef\\xab\\xd1%\\xc1\\xe1%\\xa0k\\x98\\x15kxI\\xf0|\\t\\xe2\\x13\\xda]-\\xdd\\xe2\\x08\\x13\\xb0k\\xcc9h\\x84I(\\xf2\\xba\\xc7C|\\xe2\\xc0\\x05\\xb6~\\x16\\x84D\\x98\\x1a\\xad\\xe5%\\x90<tL4\\xcbs\\xcc\\t\\xdae\\x87\\xc1\\xa1`$B\\x7f\\xdd\\xa1\\x1dv3\\xd4D4\\xd3s\\xe0I\\xba\\xb0\\xe0k\\xf0I\\xf0\\xf4\\t\\x16|694\\xe2$8\\xe4d\\xba\\xfa\\xc4\\xc6\\xa1P$\\xcb\\xc7\\xd7C\\'XA\\xd9~Z\\xa3N\\x82\\xc3N\\xb0Y\\xa4Y\\xb2B\\x92\\x08\\xfdm\\x83L\\xa07\\x83\\x0b2\\xe9\\xa0h\\xd9\\x16\\xc7\\xa1`\\xf9\\xef\\x99\\xf1i J(\\xf2\\xba\\x03\\x19\\xba\\x91^\\xd5\\xd1\\x8c\\xcf\\xb1(\\t\\x93\\x8eEm\\x1a\\x8c\\x12<\\x8d\\x02#\\xa1\\xe3\\xa0\\x19\\x9f\\xe3Q\\xc65\\x8f25 %8\"\\xa5\\xa7\\x87\\xa6A#RL^\\x8f\\x19\\x80N0\\x1b\\xd1\\x98\\x94\\xe0\\xa0\\x14l\\x0e\\x96\\xcd\\xe5\\x02\\x9d,7\\x18\\xc7\\xa8\\x04\\xa4\\x7f\\xd9\\x9c+\\xd8\\x89\\xd0nk\\x88\\x99\\xdf\\x91\\t\\x1a\\xa6b\\xf2\\x9a\\xb6\\xc7\\xf1!\\x0b\\x06\\nz\"\\xf4\\xd7-|\\x1d\\xcd\\x1b\\x0e\\x9a\\xe9\\x15y\\xedn7\\xd0\\xa9\\xa1\\xed\\xef\\x1c\\xab\\x02\\xb4\\x8a\\x19^\\xc1O\\x84Ah\\xb7w\\x1bl\\x9fYw5\\\\%8^\\x05\\x93\\x81\\xf9\\xe1\\x82\\xa0,\\xef\\xae#V\\x12\\xce#\\xd90h\\xccJp\\xd0\\xca\\x96\\x03\\xb2A\\xa3VL^O\\x15\\xf8]\\xe2PH\\x14a\\x1c\\x9c\\xad\\xc1\\xbf3\\x9b\\xd0\\xc8\\x95\\xe0\\xd0\\x95\\xb0\\xc6\\x19\\x0e\\xfd\\xe2\\xb43=\\x87\\xaf\\x0c\\xb8\\xd9O\\xb6\\xe5\\x1a\\xbe\\x12\\x1c\\xbf\\x92@\\x9b\\xb1l\\xca\\xa8Y[\\x917\\x87\\n\\x8c\\x8b\\t\\x85J\\x11\\xbe7\\x97M\\xc1\\xd5\\\\\\x97\\x12c\\x83\\xa2Q,\\xc1a,\\xe0P\\x99Ik\\x18Kp\\x1cK\\xc4%k6\\xd6\\x05MY>&\\x8ed\\xa1\\x1bH\\rd\\t\\x8ed\\xb9\\xe4(\\n\\x9c\"t\\xd6\\x11c\\xfc~]\\xd0`\\x16\\x93WG\\xc1\\xc1\\xd1\\xa0\\xe1,&o\\xc0\\x05\\xba\\xfb\\xd0\\x18\\x96\\xe0!\\x96\\x0bG\\x08\\x1a\\xc5\\x12\\x1c\\xc6\\x82M\\r\\xf3k\\x1a\\xc6\\x82}s{\\x7f\\x973\\xc4\\xa8\\x1e\\xa3\\xe4kL^\\xfd\\x04\\x969z7\\\\\\xbb\\xc6\\xed8\\x16\\xc4h,\\xd3\\x1f5\\x8e\\xc5\\xe4u: KL\\xdc{\\xd48\\x16\\x937\\xe0\\x18\\xf3\\xc3Q\\xe3XL>\\xf7\\x17\\x9b\\xf2@|N\\xd4\\xd0\\x15\\x93\\xd7\\xfe\\xe2b \\x89\\x81\\xa3\\x86\\xae\\x98\\xbc\\x19_\\x86\\xc4D\\r]1\\xf9\\xdc_Pi\\xb4\\xbb\\xd22\\x17=\\xb9\\x82s\\xcb\\xb5\\x83*IE\\x82\\xc2\\xa5,v\\x99\\xd1a,\\xa8t@V9\\x9c\\x0eI\\xa6\\xe7(\\x164\\xcbf\\x86\\x06\\xb1DW\\xa0\\x04\\x96\\xc7\\xd2\\x1fQ\\xe3VL^=\\x05\\xc7*\\xa2\\xc6\\xad\\x98\\xbc\\xce\\xe4-f\\xf2\\x92\\xafP\\x83X\\xa2/Z\\x82\\xb1&n9j\\x85JL~\\xde+\\xad{\\xd7wZ\\x11C\\n=\\xa3+\\\\\\x12\\xb0u\\xa23E\\xda\\xe8E_\\xbe\\x84\\x13\\xd9Q\\x03ZL^}\\x08\\x92\\x16\\xcc\\xd85\\xa0%\\xba\\x1a&\\x18\\x07F7G\\xb1\\x90\\x89\\xabd\\x82\\xbb\\x19[\\xac\\x8a\\xe7?\\xcc\\xda5\\x86%\\xfa\\xc2&\\x98\\xe6\\xacX\\x88Z\\xd9\\xa4\\xc5\\xa8\\xe1E\\xe8\\x02\\xa3!,\\xd1\\x157A\\xfc\\xc9\\x0cF,n\\xe2\\xab\\x9b\\xa0Y674~%\\xba\\xfa&\\xf0z\\x99\\xf6W\\xb3=W\\xe1\\x04s\\x8e\\xdd\\n\\x8cb\\x95\\x13W\\xe6\\x04\\xfde&\\xad\\x01,\\xd1\\x95:\\xc1\\xa1?;\\xec\\x8eb\\xad\\x13\\xc7\\xafL\\x94\\x17YY5~%\\xfaj\\'\\xb8\\xc9\\xc9\\xa29\\x8d_\\x89\\x8e_A-\\x06\\x1a\\xcdi\\x15O\\xa2\\xe3W&O\\xccf\\x99V\\xf3$\\x16y]\\x03G\\x9a\\xb4\\x88\\x1a\\xbfb\\xf2\\xea\\x89\\x17\\x96\\xaf\\x8b\\x1a\\xbfb\\xf2\\xba\\xf3\\xbbPZH\\x03X\\xa2\\x07X\\xa6\\xed\\xfa\\x82Bc\\xb1\\xe0)\\xcbc;G\\xb3\\xe0\\x1be\\xb9\\xd9\\xa8UC1y\\xb3me\\xf9\\x97\\xa8\\xd1,&\\xaf3\\xa5c\\x15&\\xa2\\x06\\xb3\\x98\\xbcv\\x17\\xa1\\x00\\xf3G\\x1a\\xcd\\x12\\xdfTE\\xe9\\x12[M\\x0b\\x9e\\xb2\\xfcks4\\xcb\\xe2%[\\xab\\x92\\x12\\x1d\\xda\\x02\\xf2\\x99\\xee55\\xb4%\\x16y\\x8d\\xa4i\\xa5\\x98\\xa8\\x91-&\\xafv\\x18\\xf8V^#[\\xa2\\xab\\x95\\xc2!\\xfeXH\\x15\\xe1+lo\\xc4\\xc2G3\\x8e,j`\\x8b\\xc9\\x1b\\x9f\\xd7/r\\x1b\\x1a\\xe6\\x12\\x1d\\xe6\\xb2\\xa6\\xb7\\xcf\\xa3F\\xb9\\x98\\xbc\\x89\\x9cY\\x8c\\xa8A.\\xd1A.\\x18j\\x9c*.\\x08p5\\xe2%:\\xe2e*\\x95\\xca\\\\\\x88\\x86\\xbcD\\x87\\xbcLQ\\xff\\x92\\x12\\xacQ+\\xb0b\\xf2y\\xe03\\x9f\\x8b\\x1a\\xff\\x12\\x1d\\xff\\x920&\\xee2\\x1e\\x0bv4\\xfe%\\xbe\\xad\\xb8\\xc2\\x8aA\\xc4\\x02\\xb4,7P\\xc7\\xbf \\x96d\\xc8@\\xd4\\xf8\\x17\\x93\\xcfc=\\x02E\\xa0\\x13E:\\x82\\x8f\\x8e\\x7f\\x99 `\\x16\\x9ci\\xfcKt\\xfc\\x0b\\xa0e\\xba\\xfd\\xd6\\xf8\\x97\\xe8\\xf9\\x17\\x14\\xc6Xr\\x94\\x125\\x18\\xc6\\xe4\\xf3`\\xf7\\xb8O\\xc8\\x06[\\x83a\\xa2\\x83azTz\"\\x1b\\x02\\x8d\\x85\\x89\\xae\\x08\\xcb\\x06\\xc6\\xeeX\\x18\\xb6\\xeb\\xd0J\\xb2D\\xc7\\xc2\\x0c\\xb8\\xd1\\xe4nb\\xb3\\x92\\x93\\x1a\\x18\\x13\\x1d\\x183\\xdd\\x19f\\x03\\xa4\\xe5l<\\x17\\x83I\\xce\\x02*\\x8d\\x8b\\x89\\x8e\\x8b\\x01\\xb8\\xc1`\\xacX@\\x97\\xe5N\\xc4\\xd7iA\\x82\\x96\\x9d/h\\\\L\\xf4\\\\\\x0c\\xe0<6\\xaf50&\\xfaZ-\\xc0\\x99\\xd9\\xc4\\xd6\\xc0\\x98\\xe8\\xc0\\x18\\xac\\x04\\xec\\xfem\\xd4X\\x18\\x93\\xcfA\\xdf\\x18\\xf8\\x0eZca\\xa2\\xab\\xd82A\\x95\\xf4{\\xd3\\xca\\xd0\\xfa\\x9a-(\\x03\\xe2Vs:\\xd8Z\\xd2\\xc6\\x95p\\x19\\xe1E\\xd8J\\xa0\\x911\\xd1\\x9118:e\\xb9\\xab\\x82\\xba,\\xb7\\x11G\\xc6\\xa0d\\xdaH\\xe7\\xb2\\xb6 \\xbaB.\\x17\\n2F\\r\\x8d1y]S\\xc0,0\\x1f\\xa4\\xb11\\xd1\\xb11\\xe8/M^ilLtl\\xcc\\x85r\\xe2QccL^7\\\\\\xbcLy\\xd4\\xd8\\x18\\x937gp\\xecN}\\xd4\\xd8\\x18\\x93\\xd7\\xdc\\x01\\xaf\\xc6\\x16\\xb5\\xc2.&\\xaf\\xfd\\xe55\\x16\\xa2\\xc6\\xc6\\x98\\xbc\\xe9/\\xa3\\xd2\\xa3\\xc6\\xc6\\x98\\xbc\\xce_\\n\\xbbG\\r\\x8d1\\xf9\\xdc\\xdd\\xccW\\x0e\\r\\x86\\x89\\xbe\\xa6K\\xa2W3\\xa3F\\xc3\\x98\\xfc|\"K\\x0fi4\\x18&:\\x18\\x06\\x07\\xbd\\xcc\\xf5j8Lt8\\xcc\\x94\\x8ba>R\\xc3a\\xe2\\x1fJ\\xbb\\x900M\\xa3a\\xa2\\xab\\xed\\x82\\xd3\\x08\\xe6!5\\x18&\\xfa\\xe2.]\\xcf\\x82J\\x8d\\x85\\x89\\x8e\\x85\\xe1\\x05\\xaf\\xa2\\x86\\xc2\\x98\\xfc\\xbc\\xa9v+\\xfd\\x9a.\\x9e\\x1a\\x18\\x83B\\xde\\r\\x18\\x838\\x9b\\xd5\\xf2\\xd6\\xb8\\x98T\\xe4MB\\x94\\xec\\n\\x92V\\xde\\xc5\\xe4\\xd5\\xa5\\xf1\\xeb\\xdbI\\xc3bL\\xdet\\x97\\xd5\\x12J\\x1a\\x16c\\xf2\\xb3Oc\\xa4u\\xd2\\xa8\\x18\\x93\\xd7\\x0c(\\xa2w\\xe2%\\x92F\\xc5\\x98\\xbca\\t\\xf2\\x92\\x93\\xdc\\xa4!2&\\xaf\\xf9\\x0bz\\xd2\\x984B\\xc6\\xe4g\\x7f\\xcc\\x8a\\xb2k\\xa5]\\x92\\x03d\\xfaH]F\\xd2\\x98\\x18\\x93\\xd7\\xd8\\'\\xd1\\x13\\xc1\\xa4A1&\\xaf\\x83\\x0b?O<g\\xd2\\xa8\\x18\\x93\\xd7\\xfe\\xf2r\\xd7I\\xa3bL\\xdedl\\x13\\xf3?\\x1a\\x15\\x93\\xdc\\xd3=4\\xbd\\x924\\x0c\\xc6\\xe4soq\\xab\\x86\\xdd~I\\x1a\\x06c\\xf2yt\\x91\\x94 \\xfb\\xb8\\xa4Ur1y\\x1d\\\\Z>(i\\x85\\\\L>\\xf7\\xf6\\xc2^<i\\xdc\\x8b\\xc9k\\\\\\x89Sr\\xf2\\x1e\\x86\\x86\\xbd$\\x8f\\xbd\\xa0\\x96\\x08\\t}\\x92\\x86\\xbd\\x98|\\xee.y~+i\\x94\\x8b\\xc9\\xab\\x13CM\\x1f\\xe6\\xc64\\xca%\\xb9J-\\xb4\\xaf\\xdaC#\\x8ep\\x19\\xa6\\x1b)\\x0b*\\xe5%\\x8dw1y\\x8d\\x81@8\\xd0\\x01\\xd1\\x1e\\x1bq\\xbc\\x0b\\x1d\\x10)m\\x92\\x1c\\xebB\\xdb\\x94\\xf2\\x95\\xc9q.\\xa8\\xfc\\xcbj\\x05&\\rt1\\xf9O\\'\\xaf\\x94/I\\x1era\\x8f\\xd9h\\xcf\\xf98\\xc2\\x05\\x87\\x12\\xf4\\x8d\\x1c\\xed=\\x1f\\x07\\xb8 =\\xb7A\\xc0z\\xfe\\xc3\\x1c\\x8f\\xf8\\xbe\\x8f\\xa7]\\xd6\\xfc\\xa4&\\xa9O\\xfc\\xb4\\xe5Zp\\xeaA\\x8b;$\\x8dw1\\xf9\\xcff\\x85\\xf8\\xac\\x8f+\\xcf\\xc2\\xacB|\\xd5\\xc7\\xa3-\\xa8%\\xb6u\\x87\\xd4l\\xeai\\xa6\\xe7\\xd0\\x16\\\\wa\\xa8]\\x12\\x1f\\xfay\\xf3\\xd2\\x0f\\x03\\x19\\x93F\\xb6\\x98\\xbc\\x89\\xdd\\xd9C/\\xc8\\x18+\\x98\\xb5\\xc9\\xeb^\\x83s\\x06IC[L^\\xfb\\x0b\\xf2\\x92\\xa1\\xf2Ic[L^;\\x1c\\xe8\\xbd\\xce\\xa4\\xe1,&o\\xc2\\x9e\\xadc\\xe5YP\\xa1\\xb1-\\xc9\\xb1-(\\xf7@S\\x1fI\\xa3[L\\xden\\xed\\x98\\'\\xd5\\xe8\\x96\\xe4\\xe9\\x16$\\x9d\\xe9@h\\xd6\\xe7\\xea\\xb6\\x80\\x9fe\\x07>I\\xc3[L^\\x83\\xcc\\x0b\\x93C[\\xfb\\x1c\\xd0\\xb2\\xc9\\xb8\\xc7\\xdf\\xc2vlS\\xa3\\xe1-\\xc9\\x15qA\\x12\\x8f\\xc5B\\x1a\\xde\\x92\\x1c\\xde\\x82st\\x92\\x15K\\x1a\\xd0b\\xf2\\xba\\xfbX\\xe3\\xee\\x0e\\xcbRhDKrD\\x0bH\\xc1\\x0b\\rKU\\\\\\x92\\xab\\xe2\\x82\\xa9\\xccLD\\xa3X\\x92\\xa3X\\x00l\\xb0{\\x93I\\x03WL\\xded\\x81\\xd8\\x1d\\xe0\\xa4\\x81+&\\xaf\\x8e\\x0e\\x1b&:\\x0e\\xd2e\\xa3\\xe4\\x9e\\x0b\\xe2\\xcf\\xe9&\\xed\\xbd \\x93\\xcf\\xdd\\x9d\\xaa\\xe42\\xb3\\xd0\\xb8\\x95\\xe4\\xb8\\x95\\x0bO~&\\x8d[1y\\xb3\\x8e0\\xa63i\\xa8\\x8a\\xc9\\xcf\\xf6FK~$\\rU1y\\x1d_zs;i\\xa8\\x8a\\xc9\\xeb\\xec\\xe5\\x83\\xa0\\xed\\xeb\\xfc{A\\xb8\\xa8\\xc4\\xf2_\\x1a\\x8e\\x92\\x1c\\x8e\\x82\\xcd.\\xab\\xcb\\x95\\xb4\\'\\x83L^\\x97e^\\xa7%i<\\x8a\\xc9\\xebd\\x08\\xd8\"0\\xe7\\xab\\x01)\\xa8O\\xfb\\xf3G\\xc7\\x93\\x06\\xa3\\x98|\\xee\\xec\\x16W\\xe3\\xd8z\\xa9\\xc1(\\xc9\\xc1(\\xd8<\\xd3\\x90X\\x83Q\\x92\\x7f:\\x88_\\x84O\\x1a\\x8cb\\xf2sHL\\x9f\\xa5J\\x1a\\x8cb\\xf2\\x051\\x8fV\\x97%y\\x16\\x05\\xeb\\x04\\x9bbZ]\\x96\\xe4\\xf0\\x13\\xdc\\xf9e\\xb7e\\x92\\x86\\x9f\\x98\\xbc:^,@\\xb4\\xbf\\xda\\xba\\xe6_\\x11\\x82\\r\\xb3\\xfc\\x81V\\x9a%9\\xfe\\x04!\\xf0\\xc0\\x0e\\xd14\\xfe$\\xf9\\xa7\\x84\\x90Sa\\xeb\\xbb\\xc6\\x9f$_\\x9b\\x05\\xe6\\xc6|\\xaf\\xc6\\x9f$\\xff\\xa0\\x10\\xbf5\\x944\\xfe\\xc4\\xe4\\xf3|\\x00\\xce\\xc1v.\\x1a~\\x92\\xdc\\x9bB\\x9c\\x85N\\x1a}b\\xf2z\\x02\\x03\\xab`\\xb3W\\xa3O\\x92+\\xcc\\x82\\xcb\\x14,z\\xd2\\xe0\\x93\\xe4\\n\\xb3`\\xafB\\xe7\\x98fk\\xae.\\xcbT\\xd6c\\xc9S\\x8eI#QL\\xde\\x04\\xc0\\xd4Qh(Jr(\\n\\x8a\\xd5\\xd0\\xc0ZCQ\\x92/\\xcc\\xc2+\\xeb%\\x8dE1\\xf9\\xcf\\xd2\\x81\\x1a\\x88\\x92<\\x88\\xc2\\xcb\\t\\'\\rD1y\\rx8m\\x9f4\\x12\\xc5\\xe4uMF\\xc5P\\x92\\\\\\xd4H\\x94\\xe4H\\x94\\xa9x#kV\\xcb\\x9a8\\x12\\x05\\x07\\x94#s\\x12\\x1a\\x8a\\x92\\xfc\\xe3B\\x08\\x7f\\xe90hi\\x13G\\x9f\\x80\\x16&+\\x06\\xd0\\x17%gi\\xf2\\xba\\xb7\\x00%K\\xdb\\xd5\\xde(wUY\"^!d\\x0fTk\\xf8Iv\\xaf\\x0b]H\\x85f\\r?1yudt6d\\r?1y\\x1d^\\x14\\x81$\\xb3,k\\xf8\\x89\\xc9\\xab\\xad\\xf1w\\xc2\\xb2F\\x9c\\x98\\xbcI\\x94006k\\xc8\\x89\\xc9\\x9b\\xcd\\x1b\\xbb\\xfc\\x9a5\\xe8\\xc4\\xe4s\\x7f3n\"\\x92\\x802k\\xd0\\x89\\xc9k\\x7f\\xe9\\r\\xb3\\xac1\\'&o\\x12\\x0flY\\xcb\\x1asb\\xf2\\x1a\\xf10\\xcf\\x9b5\\xe2\\xc4\\xe4\\xb5\\xb78Q~\\xdf\\xf3f\\r81y\\xb3wc\\xc1o\\xd6\\x90\\x13\\x93\\xd7\\xb9\\x8b\\x0b\\xd6\\xc4\\xf3f\\r91\\xf9\\xdc_\\xbc$A\\x9b\\x95\\xce\\xc0\\xb3\\xaf\\xb5\\xf2\\xf6\\x05(\\xb2\\xcae\\r@1y\\x1dk@\\xb2\\xb4\\xf3Rp\\x99\\xddKB\\xb8\\x15\\xcd\\xe8\\x8b\\xac\\x11(&\\xaf\\xfd\\xe5\\x95\\xa5\\x81\\xdbI\\xcb\\x9c/\\xbcB\\xc3\\xc9\\xacA(&?\\xbb\\t\\xea-5\\x06%\\xfb\\xd7\\x82\\x16\\xd6\\x9a\\xcaZ\\xd9\\x15\\x93W\\x9f\\x81\\xc2\\xbc$\\x95\\x945\\x0e\\xc5\\xe4\\xd5k\\xe09\\x85%\\xb7h\\xb3V\\x84\\xc5\\xe4s\\xe79\\xfb\\x9d\\xb5G\\x84L~\\xce\\xb4\\xf2i\\xad\\xd5`\\xc9\\xbe\\x06K\\xb7f\\xc1\\x90\\x86\\xa6d_\\x83eD\\xf9\\xca\\x05u+\\xb2V\\x90\\xc5\\xe4u\\x8e\\xa3\\x88<\\xeb\\xbcV\\x91%{^\\x05\\xf7\\xc9\\x98\\xcb\\xd3*\\xb2d\\x07\\xacL\\xae\\x89E\\xb4\\x1a\\xa3\\x92\\x1d\\xa3\\x82]:\\xf3\\xa4\\x1a\\xa1\\x92\\xdf\\x16d\\xa1\\xab\\x96\\x06\\xa8\\xe4\"\\xaff\\x88\\xd7<\\xe8\\xf0J\\xc7\\x06\\xd9A*(\\'\\xc4\\xaeGd\\rT1y\\xed//\\xe5\\x95\\xb5\\xb2+&\\xafn\\x0e\\xd3\\x97E1\\x1a\\x9b\\x92\\x1d\\x9b\\x82\\x8c#\\x9d\\x0e\\xd26/\\x17\\x94\\xa5v\\x97\\x97\\xd6\\x80W\\x95\\x16@\\xf7\\x86P\\xc0A\\xf0\\x92\\xc2\\x0cY\\x03UL^]\\x05O\\x97f\\x8dS1y\\x8d\\xf2q\\x8d\\x9a\\xed\\xa24N%\\xfb\\x17\\x85\\x90\\t!i\\xe3\\xac\\xa1)&\\x9f\\xfb\\xbb\\xc1M\\x116\\xe742%\\x17y]\\xf5\\x90\\x11b\\xbb\\x1d\\x8dL\\xc9\\x8eL\\xc1\\xf9mf\\x93Y+\\xbc\\x92\\x1d\\x99\\x82I\\xc7\\xd2\\xfdY#SL^\\xb3m|\\x95\\xd6J\\xaddG\\xa6\\xc0\\xb51\\x8f\\xa9\\xb1(\\xd9?(\\x84B\\xb2lt5\\x18%\\xfb\\x07\\x85\\xf0 \\x86\\xab\\x08I;/=\\xe6\\x95]\\xad\\x15^\\xb47k`\\x8a\\xc9\\xab\\xa7\\x80;r\\xd5\\x17\\x98yk\\xa5V\\xb2\\x83T.T\\x06\\xce\\x1a\\xa5b\\xf2fSE\\x97V\\x8dR\\xc9\\xae\\xbc\\xca\\xf4\\x8e\\x1e\\xfd\\x0e\\xa5Tgv\\xe5U\"\\x9e\\xaag\\x11\\x9cV^%;J\\xe5B]\\xa5\\xaca*&\\xafk\\xe0\\xc2\"BYcVL^C}^\\x95;k\\xcc\\x8a\\xc9\\x9b5pp%?\\xd8\\xc8k\\x00Kv\\x0f\\x0fM#O23\\x1a\\xbf\\x92]\\xa9\\x95\\xa9\\xe8\\xe0\\xb2\\xbeK\\xe8Xv\\x85W\\xd0\\xf7\\x91-bZ\\xad\\x95\\xecx\\x16,6\\xd4\\xcdj<Kv<\\x0b\\xfa\\xcb\\xdb\\xd5\\xac\\xd2\\xd5W\\x89\\x08>\\xd8\\xfe[\\xe3Y\\xb2\\xab\\xaf\\x02&\\x80\\xdd\\xe9\\xcf\\x1a\\xceb\\xf2ybO\\x8f\\x94\\xe1B\\xec\\xf9\\x0f\\x8bp4\\xbe%\\xbbb+\\xfc\\xc8+kx\\x8b\\xc9\\x9b\\xa8\\x9a\\x06z\\x1a\\xde\\x92=\\xde\\x82#/\\xb6\\xd7\\xd4\\xf0\\x96\\xec\\xde\\x1d\\xc2\\xdc\\xa0{n\\ro\\xc9\\xae\\xd6\\n\\x1e\\x16c/\\xeed\\x8do1y\\xb3\\x9c3@ k|\\x8b\\xc9g\\'\\x9d\\x10\\x98.\\xa9j\\x965\\xd8\\xc5\\xe4ub#\\x11\\xc6\\xe23\\xad\\xd8Jv\\xb0\\x0b\\x1c\\xd2\\xb2\\xec\\x8cF\\xbedG\\xbe\\\\\\x8a\\x194\\xf2%;\\xf2e\\x83\\xe29d\\x85\\xd1\\xc0\\x97\\xec\\xc0\\x17\\xb0v\\xec\\x92V\\xd6\\xc0\\x17\\x93\\xd7\\xec\\x01\\x9c)3D\\r|\\xc9E~N\\xf0\\xb2\\xf2>Y\\x03_L\\xde\\x04\\x1f\\xec\\xf09k\\xe4\\x8b\\xc9\\x1b;\\xec}\\x95x\\xf6\\x1dj\\x07\\x17\\x0e\\x83\\x992@tnh\\x0b\\xa2\\xe7`p\\x8b\\x88\\xad)Z\\x11\\x96\\xecA\\x18\\x04\\x06l\\x18\\xb4LM!e\\xea\\x9a\\xc2\\x13@\\x1a\\xf9\\x92\\x1d\\xf92\\xa2D\\x06\\x1b\\x05\\x8d|\\xc9\\x8e|\\xe9\\x81\\xec\\xb3M\\x96F\\xbedW\\x85\\x05\\xd7\\x0e\\x19l\\x955\\xfa\\xc5\\xe4\\xf3\\xf0\\x82HeE>\\xb3F\\xbf\\x98|6=\\x8c\\x03\\x8b\\xc85\\xf8%\\xbf-\\xc3\\xc2\\xb6n\\x1a\\xfc\\x92\\x1d\\xfc\\x02N\\x906\\xab\\x99\\x9a\\x83_pl\\xce \\x95\\xac\\xc1/&\\xaf\\xeb5\\xeau\\xd1I\\xa6\\xd9\\x9a\\x7f\\x93\\x08y/\\x97. \\xbe\\x1e\\xb1\\x82\\x92\"5ys8\\xcf\\xd6\\xa6^\\xab\\xc4b\\xf2\\xea(\\xf8\\xed\\xbd^CaL^\\xfb\\xcb\\x1f\\xaa\\xee5\\x14\\xc6\\xe4\\xb5\\xbf\\xc0\\'\\x89\\x7f\\xef5\\x16\\xc6\\xe4\\xcd\\x1a\\xcd\\xc7W:\\x91\\xe8\\x0b:S\\x17=\\xb0\\x82\\xb4\\xbf\\xd2:\\xd7\\x17t\\xa6\\xf6\\x17\\xef]\\xd0y&\\x19__\\xd0\\x99\\xb9\\xbf\\x03\\xcal-\\xb9\\x00\\xd7k`\\x8c\\xc9\\xe7\\xce\\xe3\\x11A\\x1f|\\x92|A\\xafQ2&\\xaf\\xbf\\t\\xdf\\xa6\\xa0\\xde\\xacd\\x89\\xee\\xbd\"\\xe4\\x7fH\\x86\\xa0\\xd7(\\x19\\x93\\x9f\\x839\\x16\\xb7\\xf4\\x1a&c\\xf2z:\\x81;\\x0el\\xfei\\x9cL\\xef\\n\\xb3\\xf0\\x9b\\xeb\\xbd\\x86\\xc9\\x98\\xfc\\x9c<gg\\x08\\xbdF\\xc9\\x98\\xbc\\xc9{-z\\x1f\\xab\\xd7\\xca\\xb4\\x98\\xbc\\xc6\\xcd\\xfc\\xb5\\x8d^\\xc3dL>w\\xbeG\\xcdT\\x12t\\xf5Z\\x9d\\x16\\x93W\\x17\\xb2\\xe9F\\xb7\\x7f}\\xbb8^\\x1f\\xef\\xf7\\xfb\\xd3\\xaf\\xbb\\xd3\\xee\\xf6\\xe3\\xcb\\xee\\xeb\\xfe?w\\xaf_\\x1f\\x9e\\x8fW\\x8f\\xfb/\\xa7\\x9b\\x15\\x80\\xf3\\xd5\\xd5\\xeb\\xc3\\xd7\\xfb\\xf9\\xef\\xa7\\xc3K\\xf9Wx\\xcbO\\x87\\xd3\\xe9\\xf04\\xfft\\xbf\\xdf}\\xde\\xbfN?\\xa1\\xfa\\xc1\\x97\\xc3\\xe14\\xffp}\\xfb\\xf1\\xfa\\xc7\\xe1\\xf5[\\xf9\\x9c\\xdb\\xff\\x13\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xb4}\\xd6\\xc7P\\x01\\x00\\x00o\\x02\\x00\\x00\\x11\\x00\\x08\\x01docProps/core.xml \\xa2\\x04\\x01(\\xa0\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x92_K\\xc30\\x14\\xc5\\xdf\\x05\\xbfC\\xc9\\xb3m\\x92\\xd5?3\\xb4\\x1dS\\x19\\x0c\\xdd\\x10VQ|\\x0b\\xc9\\xddVl\\xd3\\x92D\\xbb}{\\xd3v\\xab\\x95\\xf9\\xe0cr\\xce\\xfd\\xe5\\x9cK\\xa2\\xc9\\xae\\xc8\\xbd/\\xd0&+U\\x8ch@\\x90\\x07J\\x942S\\x9b\\x18\\xbd\\xa43\\x7f\\x8c<c\\xb9\\x92</\\x15\\xc4h\\x0f\\x06M\\x92\\xf3\\xb3HTL\\x94\\x1a\\x9euY\\x81\\xb6\\x19\\x18\\xcf\\x91\\x94a\\xa2\\x8a\\xd1\\xd6\\xda\\x8aal\\xc4\\x16\\nn\\x02\\xe7PN\\\\\\x97\\xba\\xe0\\xd6\\x1d\\xf5\\x06W\\\\|\\xf0\\r\\xe0\\x11!\\xd7\\xb8\\x00\\xcb%\\xb7\\x1c7@\\xbf\\xea\\x89\\xe8\\x80\\x94\\xa2GV\\x9f:o\\x01R`\\xc8\\xa1\\x00e\\r\\xa6\\x01\\xc5?^\\x0b\\xba0\\x7f\\x0e\\xb4\\xca\\xc0Ydv_\\xb9N\\x87\\xb8C\\xb6\\x14\\x9d\\xd8\\xbbw&\\xeb\\x8du]\\x07u\\xd8\\xc6p\\xf9)~[<\\xad\\xda\\xaa~\\xa6\\x9a]\\t@I$\\x05\\x13\\x1a\\xb8-u\\x12\\\\x\\x8f\\xd3\\xe5\\xfcq\\xea\\xad\\xa6\\xf3\\xe5<\\xc2\\x03\\xad\\xd9c\\xce\\x8d]\\xb8\\x95\\xaf3\\x90w\\xfbS\\xfb\\xa9\\xc5\\xd1\\xdb2\\xdd\\x13 =\\x17\\x8fue\\x8e\\xcakx\\xff\\x90\\xceP2\"\\xf4\\xd6\\'\\xa1OoR:f!a\\xe1\\xe5{\\x93\\xe0\\xd7|\\x13\\xb7\\xbb(\\x0e9\\xfeA\\x0ci:\"\\x8c^1B\\x06\\xc4# \\x89\\xf0\\xc9\\x17I\\xbe\\x01\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xea\\xf0\\xeb(\\x96\\x01\\x00\\x00$\\x03\\x00\\x00\\x10\\x00\\x08\\x01docProps/app.xml \\xa2\\x04\\x01(\\xa0\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x9c\\x92\\xc1n\\xdb0\\x0c\\x86\\xef\\x05\\xf6\\x0e\\x86\\xee\\x8d\\x9c\\xae(\\x8a@V1\\xa4\\x1bz\\xe8\\xd0\\x00q\\xda\\xb3&\\xd3\\xb1PY\\x12D\\xd6\\x88\\xf7\\xf4\\x95m$q\\xb6\\x9dv#\\xf9\\x13\\xbf>\\x91\\x14\\x0f\\x87\\xd6f\\x1dD4\\xde\\x15l\\xb9\\xc8Y\\x06N\\xfb\\xca\\xb8}\\xc1v\\xe5\\x8f\\xeb{\\x96!)W)\\xeb\\x1d\\x14\\xac\\x07d\\x0f\\xf2\\xcb\\x95\\xd8D\\x1f \\x92\\x01\\xcc\\x92\\x85\\xc3\\x825Da\\xc59\\xea\\x06Z\\x85\\x8b$\\xbb\\xa4\\xd4>\\xb6\\x8aR\\x1a\\xf7\\xdc\\xd7\\xb5\\xd1\\xf0\\xe8\\xf5G\\x0b\\x8e\\xf8M\\x9e\\xdfq8\\x10\\xb8\\n\\xaa\\xebp2d\\x93\\xe3\\xaa\\xa3\\xff5\\xad\\xbc\\x1e\\xf8\\xf0\\xb5\\xecC\\x02\\x96\\xe2[\\x08\\xd6hE\\xe9\\x97\\xf2\\xa7\\xd1\\xd1\\xa3\\xaf)\\xfb~\\xd0`\\x05\\x9f\\x8b\"\\xd1mA\\x7fDC\\xbd\\xcc\\x05\\x9f\\xa7b\\xab\\x95\\x85u2\\x96\\xb5\\xb2\\x08\\x82\\x9f\\x0b\\xe2\\t\\xd40\\xb4\\x8d2\\x11\\xa5\\xe8h\\xd5\\x81&\\x1f34\\xbf\\xd3\\xd8nX\\xf6K!\\x0c8\\x05\\xebT4\\xcaQ\\xc2\\x1a\\xda\\xa6d\\x8cm@\\x8a\\xf2\\xcd\\xc7wl\\x00\\x08\\x05O\\rSq\\x0c\\xe7\\xbd\\xf3\\xd8\\xdc\\xca\\xe5\\xd8\\x90\\x82\\xcb\\xc6\\xc1`\\x02I\\xc2%bi\\xc8\\x02\\xbe\\xd4\\x1b\\x15\\xe9\\x1f\\xc4\\xcb9\\xf1\\xc80\\xf1N8\\xdb\\x81ozs\\xce7~9\\xbd\\xf4\\x87\\xf7\\xda\\xb7A\\xb9^\\x96\\x00\\x88\\xa6\\x82l\\xe7\\xccxs\\xd4\\x0b~\\x14\\xc5\\xb3q\\xef\\xb8\\x0b\\xa5\\x7fT\\x04\\xc7\\t_\\x16\\xc5\\xb6Q\\x11\\xaa\\xb4\\x94\\xd3\\x06N\\x05\\xf1\\x94\\x86\\x1b\\xed`\\xb2n\\x94\\xdbCu\\xec\\xf9[\\x18\\xee\\xe1u:z\\xb9\\xbc[\\xe4_\\xf3\\xb4\\xeaYM\\xf0\\xf3y\\xcbO\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00b\\xee\\x9dh^\\x01\\x00\\x00\\x90\\x04\\x00\\x00\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00[Content_Types].xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xb5U0#\\xf4\\x00\\x00\\x00L\\x02\\x00\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x97\\x03\\x00\\x00_rels/.relsPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\x81>\\x94\\x97\\xf3\\x00\\x00\\x00\\xba\\x02\\x00\\x00\\x1a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xbc\\x06\\x00\\x00xl/_rels/workbook.xml.relsPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00s\\xdf\\x1f\\xde-\\x02\\x00\\x00\\x82\\x04\\x00\\x00\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xef\\x08\\x00\\x00xl/workbook.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x007\\xa1\\x81\\xca\\xac\\x00\\x00\\x00\\xce\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00I\\x0b\\x00\\x00xl/sharedStrings.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00u>\\x99i\\x93\\x06\\x00\\x00\\x8c\\x1a\\x00\\x00\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\'\\x0c\\x00\\x00xl/theme/theme1.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00U3$\\xcfe\\x03\\x00\\x00\\xc3\\n\\x00\\x00\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xeb\\x12\\x00\\x00xl/styles.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\x0b\\xf4\\xf4\\xac\\xdb\\x17\\x00\\x00z\\xb6\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00{\\x16\\x00\\x00xl/worksheets/sheet1.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xb4}\\xd6\\xc7P\\x01\\x00\\x00o\\x02\\x00\\x00\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c.\\x00\\x00docProps/core.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xea\\xf0\\xeb(\\x96\\x01\\x00\\x00$\\x03\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x131\\x00\\x00docProps/app.xmlPK\\x05\\x06\\x00\\x00\\x00\\x00\\n\\x00\\n\\x00\\x80\\x02\\x00\\x00\\xdf3\\x00\\x00\\x00\\x00'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "ZL50NhHNY3yn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tar=pd.read_excel('survival_LUAD.xlsx')\n",
        "dt=pd.read_excel('LUAD.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OgFI5RBwY333",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "6a85f5ec-daba-4695-e7bb-f3e94cb85b80"
      },
      "cell_type": "code",
      "source": [
        "dt.head(2)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UBE2Q2P2</th>\n",
              "      <th>SSX9</th>\n",
              "      <th>CXORF67</th>\n",
              "      <th>EFCAB8</th>\n",
              "      <th>SDR16C6P</th>\n",
              "      <th>EFCAB12</th>\n",
              "      <th>A1BG</th>\n",
              "      <th>A1CF</th>\n",
              "      <th>RBFOX1</th>\n",
              "      <th>GGACT</th>\n",
              "      <th>...</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Patient's Vital Status</th>\n",
              "      <th>Disease Free (Months)</th>\n",
              "      <th>Person Cigarette Smoking History Pack Year Value</th>\n",
              "      <th>Fraction Genome Altered</th>\n",
              "      <th>Overall Survival Status</th>\n",
              "      <th>Disease Free Status</th>\n",
              "      <th>Person Neoplasm Status</th>\n",
              "      <th>Primary Tumor Site</th>\n",
              "      <th>Neoplasm Disease Stage American Joint Committee on Cancer Code3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HUG0_SYMBOL</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TCGA-05-4244-01</th>\n",
              "      <td>-0.4238</td>\n",
              "      <td>-0.1379</td>\n",
              "      <td>-0.1805</td>\n",
              "      <td>0.4529</td>\n",
              "      <td>-0.3317</td>\n",
              "      <td>-0.1012</td>\n",
              "      <td>-0.6977</td>\n",
              "      <td>-0.1476</td>\n",
              "      <td>-0.224</td>\n",
              "      <td>0.3832</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.456523</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-05-4249-01</th>\n",
              "      <td>-0.3291</td>\n",
              "      <td>-0.1379</td>\n",
              "      <td>-0.1805</td>\n",
              "      <td>-0.0869</td>\n",
              "      <td>-0.3317</td>\n",
              "      <td>-0.1661</td>\n",
              "      <td>-0.1483</td>\n",
              "      <td>-0.1371</td>\n",
              "      <td>-0.226</td>\n",
              "      <td>-0.5346</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>50.03</td>\n",
              "      <td>52.0</td>\n",
              "      <td>0.222128</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 2864 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 UBE2Q2P2    SSX9  CXORF67  EFCAB8  SDR16C6P  EFCAB12    A1BG  \\\n",
              "HUG0_SYMBOL                                                                     \n",
              "TCGA-05-4244-01   -0.4238 -0.1379  -0.1805  0.4529   -0.3317  -0.1012 -0.6977   \n",
              "TCGA-05-4249-01   -0.3291 -0.1379  -0.1805 -0.0869   -0.3317  -0.1661 -0.1483   \n",
              "\n",
              "                   A1CF  RBFOX1   GGACT  \\\n",
              "HUG0_SYMBOL                               \n",
              "TCGA-05-4244-01 -0.1476  -0.224  0.3832   \n",
              "TCGA-05-4249-01 -0.1371  -0.226 -0.5346   \n",
              "\n",
              "                                              ...                                 \\\n",
              "HUG0_SYMBOL                                   ...                                  \n",
              "TCGA-05-4244-01                               ...                                  \n",
              "TCGA-05-4249-01                               ...                                  \n",
              "\n",
              "                 Sex  Patient's Vital Status  Disease Free (Months)  \\\n",
              "HUG0_SYMBOL                                                           \n",
              "TCGA-05-4244-01    0                       1                   0.00   \n",
              "TCGA-05-4249-01    0                       1                  50.03   \n",
              "\n",
              "                 Person Cigarette Smoking History Pack Year Value  \\\n",
              "HUG0_SYMBOL                                                         \n",
              "TCGA-05-4244-01                                              38.0   \n",
              "TCGA-05-4249-01                                              52.0   \n",
              "\n",
              "                 Fraction Genome Altered  Overall Survival Status  \\\n",
              "HUG0_SYMBOL                                                         \n",
              "TCGA-05-4244-01                 0.456523                        1   \n",
              "TCGA-05-4249-01                 0.222128                        1   \n",
              "\n",
              "                 Disease Free Status  Person Neoplasm Status  \\\n",
              "HUG0_SYMBOL                                                    \n",
              "TCGA-05-4244-01                    0                       0   \n",
              "TCGA-05-4249-01                    0                       0   \n",
              "\n",
              "                 Primary Tumor Site  \\\n",
              "HUG0_SYMBOL                           \n",
              "TCGA-05-4244-01                   5   \n",
              "TCGA-05-4249-01                   5   \n",
              "\n",
              "                 Neoplasm Disease Stage American Joint Committee on Cancer Code3  \n",
              "HUG0_SYMBOL                                                                       \n",
              "TCGA-05-4244-01                                                 10                \n",
              "TCGA-05-4249-01                                                  3                \n",
              "\n",
              "[2 rows x 2864 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "metadata": {
        "id": "himGdNPE77yh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fed72513-b811-4244-b619-5b598ccc3dd5"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(dt,tar,test_size=0.3)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(361, 2864) (361, 1)\n",
            "(156, 2864) (156, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pDAhhTJAY36_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "20e8aa1e-f50b-434f-8dc6-4a0248c5d424"
      },
      "cell_type": "code",
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_66 (Dense)             (None, 128)               366720    \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 531,585\n",
            "Trainable params: 531,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "foDpq4t2fWI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6DE6LKTKgUqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dt=dt.set_index(dt.columns[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ay6Y0EuggAsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6851
        },
        "outputId": "c59edb8a-5681-45d6-cdb4-69fae986e4aa"
      },
      "cell_type": "code",
      "source": [
        "NN_model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split = 0.2,callbacks=callbacks_list) \n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 288 samples, validate on 73 samples\n",
            "Epoch 1/100\n",
            "288/288 [==============================] - 0s 575us/step - loss: 1.0483 - mean_absolute_error: 1.0483 - val_loss: 15.8405 - val_mean_absolute_error: 15.8405\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 15.50315\n",
            "Epoch 2/100\n",
            "288/288 [==============================] - 0s 561us/step - loss: 1.1567 - mean_absolute_error: 1.1567 - val_loss: 16.2553 - val_mean_absolute_error: 16.2553\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 15.50315\n",
            "Epoch 3/100\n",
            "288/288 [==============================] - 0s 561us/step - loss: 1.2765 - mean_absolute_error: 1.2765 - val_loss: 15.9446 - val_mean_absolute_error: 15.9446\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 15.50315\n",
            "Epoch 4/100\n",
            "288/288 [==============================] - 0s 574us/step - loss: 1.2998 - mean_absolute_error: 1.2998 - val_loss: 16.0453 - val_mean_absolute_error: 16.0453\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 15.50315\n",
            "Epoch 5/100\n",
            "288/288 [==============================] - 0s 559us/step - loss: 1.5569 - mean_absolute_error: 1.5569 - val_loss: 15.8240 - val_mean_absolute_error: 15.8240\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 15.50315\n",
            "Epoch 6/100\n",
            "288/288 [==============================] - 0s 586us/step - loss: 1.2420 - mean_absolute_error: 1.2420 - val_loss: 15.8507 - val_mean_absolute_error: 15.8507\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 15.50315\n",
            "Epoch 7/100\n",
            "288/288 [==============================] - 0s 580us/step - loss: 1.3749 - mean_absolute_error: 1.3749 - val_loss: 16.0830 - val_mean_absolute_error: 16.0830\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 15.50315\n",
            "Epoch 8/100\n",
            "288/288 [==============================] - 0s 598us/step - loss: 1.4656 - mean_absolute_error: 1.4656 - val_loss: 15.6756 - val_mean_absolute_error: 15.6756\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 15.50315\n",
            "Epoch 9/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 1.1859 - mean_absolute_error: 1.1859 - val_loss: 15.8542 - val_mean_absolute_error: 15.8542\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 15.50315\n",
            "Epoch 10/100\n",
            "288/288 [==============================] - 0s 572us/step - loss: 1.7069 - mean_absolute_error: 1.7069 - val_loss: 15.8025 - val_mean_absolute_error: 15.8025\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 15.50315\n",
            "Epoch 11/100\n",
            "288/288 [==============================] - 0s 581us/step - loss: 1.5933 - mean_absolute_error: 1.5933 - val_loss: 15.8309 - val_mean_absolute_error: 15.8309\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 15.50315\n",
            "Epoch 12/100\n",
            "288/288 [==============================] - 0s 587us/step - loss: 1.4317 - mean_absolute_error: 1.4317 - val_loss: 15.6123 - val_mean_absolute_error: 15.6123\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 15.50315\n",
            "Epoch 13/100\n",
            "288/288 [==============================] - 0s 588us/step - loss: 1.0772 - mean_absolute_error: 1.0772 - val_loss: 15.8543 - val_mean_absolute_error: 15.8543\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 15.50315\n",
            "Epoch 14/100\n",
            "288/288 [==============================] - 0s 570us/step - loss: 1.1004 - mean_absolute_error: 1.1004 - val_loss: 15.9186 - val_mean_absolute_error: 15.9186\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 15.50315\n",
            "Epoch 15/100\n",
            "288/288 [==============================] - 0s 550us/step - loss: 1.0593 - mean_absolute_error: 1.0593 - val_loss: 15.6247 - val_mean_absolute_error: 15.6247\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 15.50315\n",
            "Epoch 16/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 0.9945 - mean_absolute_error: 0.9945 - val_loss: 16.0933 - val_mean_absolute_error: 16.0933\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 15.50315\n",
            "Epoch 17/100\n",
            "288/288 [==============================] - 0s 573us/step - loss: 1.0449 - mean_absolute_error: 1.0449 - val_loss: 15.3083 - val_mean_absolute_error: 15.3083\n",
            "\n",
            "Epoch 00017: val_loss improved from 15.50315 to 15.30834, saving model to Weights-017--15.30834.hdf5\n",
            "Epoch 18/100\n",
            "288/288 [==============================] - 0s 554us/step - loss: 0.9718 - mean_absolute_error: 0.9718 - val_loss: 15.7754 - val_mean_absolute_error: 15.7754\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 15.30834\n",
            "Epoch 19/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 0.9423 - mean_absolute_error: 0.9423 - val_loss: 16.0175 - val_mean_absolute_error: 16.0175\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 15.30834\n",
            "Epoch 20/100\n",
            "288/288 [==============================] - 0s 558us/step - loss: 1.0006 - mean_absolute_error: 1.0006 - val_loss: 15.5160 - val_mean_absolute_error: 15.5160\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 15.30834\n",
            "Epoch 21/100\n",
            "288/288 [==============================] - 0s 575us/step - loss: 0.9929 - mean_absolute_error: 0.9929 - val_loss: 15.2276 - val_mean_absolute_error: 15.2276\n",
            "\n",
            "Epoch 00021: val_loss improved from 15.30834 to 15.22762, saving model to Weights-021--15.22762.hdf5\n",
            "Epoch 22/100\n",
            "288/288 [==============================] - 0s 566us/step - loss: 1.0876 - mean_absolute_error: 1.0876 - val_loss: 15.3363 - val_mean_absolute_error: 15.3363\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 15.22762\n",
            "Epoch 23/100\n",
            "288/288 [==============================] - 0s 544us/step - loss: 1.0510 - mean_absolute_error: 1.0510 - val_loss: 15.3623 - val_mean_absolute_error: 15.3623\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 15.22762\n",
            "Epoch 24/100\n",
            "288/288 [==============================] - 0s 554us/step - loss: 1.2031 - mean_absolute_error: 1.2031 - val_loss: 15.4489 - val_mean_absolute_error: 15.4489\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 15.22762\n",
            "Epoch 25/100\n",
            "288/288 [==============================] - 0s 556us/step - loss: 1.1420 - mean_absolute_error: 1.1420 - val_loss: 15.2239 - val_mean_absolute_error: 15.2239\n",
            "\n",
            "Epoch 00025: val_loss improved from 15.22762 to 15.22393, saving model to Weights-025--15.22393.hdf5\n",
            "Epoch 26/100\n",
            "288/288 [==============================] - 0s 574us/step - loss: 1.1748 - mean_absolute_error: 1.1748 - val_loss: 15.6621 - val_mean_absolute_error: 15.6621\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 15.22393\n",
            "Epoch 27/100\n",
            "288/288 [==============================] - 0s 610us/step - loss: 0.9130 - mean_absolute_error: 0.9130 - val_loss: 15.6215 - val_mean_absolute_error: 15.6215\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 15.22393\n",
            "Epoch 28/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 1.1000 - mean_absolute_error: 1.1000 - val_loss: 15.4585 - val_mean_absolute_error: 15.4585\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 15.22393\n",
            "Epoch 29/100\n",
            "288/288 [==============================] - 0s 577us/step - loss: 1.0902 - mean_absolute_error: 1.0902 - val_loss: 15.5482 - val_mean_absolute_error: 15.5482\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 15.22393\n",
            "Epoch 30/100\n",
            "288/288 [==============================] - 0s 537us/step - loss: 0.9070 - mean_absolute_error: 0.9070 - val_loss: 15.6748 - val_mean_absolute_error: 15.6748\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 15.22393\n",
            "Epoch 31/100\n",
            "288/288 [==============================] - 0s 557us/step - loss: 1.0430 - mean_absolute_error: 1.0430 - val_loss: 15.5890 - val_mean_absolute_error: 15.5890\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 15.22393\n",
            "Epoch 32/100\n",
            "288/288 [==============================] - 0s 579us/step - loss: 0.9737 - mean_absolute_error: 0.9737 - val_loss: 15.3126 - val_mean_absolute_error: 15.3126\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 15.22393\n",
            "Epoch 33/100\n",
            "288/288 [==============================] - 0s 601us/step - loss: 0.9832 - mean_absolute_error: 0.9832 - val_loss: 15.7322 - val_mean_absolute_error: 15.7322\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 15.22393\n",
            "Epoch 34/100\n",
            "288/288 [==============================] - 0s 544us/step - loss: 1.2089 - mean_absolute_error: 1.2089 - val_loss: 15.6030 - val_mean_absolute_error: 15.6030\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 15.22393\n",
            "Epoch 35/100\n",
            "288/288 [==============================] - 0s 565us/step - loss: 1.2943 - mean_absolute_error: 1.2943 - val_loss: 15.2658 - val_mean_absolute_error: 15.2658\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 15.22393\n",
            "Epoch 36/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 1.3463 - mean_absolute_error: 1.3463 - val_loss: 15.6404 - val_mean_absolute_error: 15.6404\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 15.22393\n",
            "Epoch 37/100\n",
            "288/288 [==============================] - 0s 554us/step - loss: 1.0585 - mean_absolute_error: 1.0585 - val_loss: 15.1512 - val_mean_absolute_error: 15.1512\n",
            "\n",
            "Epoch 00037: val_loss improved from 15.22393 to 15.15118, saving model to Weights-037--15.15118.hdf5\n",
            "Epoch 38/100\n",
            "288/288 [==============================] - 0s 559us/step - loss: 1.0290 - mean_absolute_error: 1.0290 - val_loss: 15.6941 - val_mean_absolute_error: 15.6941\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 15.15118\n",
            "Epoch 39/100\n",
            "288/288 [==============================] - 0s 539us/step - loss: 1.0275 - mean_absolute_error: 1.0275 - val_loss: 15.7011 - val_mean_absolute_error: 15.7011\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 15.15118\n",
            "Epoch 40/100\n",
            "288/288 [==============================] - 0s 535us/step - loss: 0.9354 - mean_absolute_error: 0.9354 - val_loss: 15.5901 - val_mean_absolute_error: 15.5901\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 15.15118\n",
            "Epoch 41/100\n",
            "288/288 [==============================] - 0s 562us/step - loss: 1.0201 - mean_absolute_error: 1.0201 - val_loss: 15.3660 - val_mean_absolute_error: 15.3660\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 15.15118\n",
            "Epoch 42/100\n",
            "288/288 [==============================] - 0s 532us/step - loss: 0.8681 - mean_absolute_error: 0.8681 - val_loss: 15.4096 - val_mean_absolute_error: 15.4096\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 15.15118\n",
            "Epoch 43/100\n",
            "288/288 [==============================] - 0s 544us/step - loss: 1.0188 - mean_absolute_error: 1.0188 - val_loss: 15.5784 - val_mean_absolute_error: 15.5784\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 15.15118\n",
            "Epoch 44/100\n",
            "288/288 [==============================] - 0s 563us/step - loss: 1.0098 - mean_absolute_error: 1.0098 - val_loss: 15.8611 - val_mean_absolute_error: 15.8611\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 15.15118\n",
            "Epoch 45/100\n",
            "288/288 [==============================] - 0s 568us/step - loss: 1.0155 - mean_absolute_error: 1.0155 - val_loss: 15.7906 - val_mean_absolute_error: 15.7906\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 15.15118\n",
            "Epoch 46/100\n",
            "288/288 [==============================] - 0s 557us/step - loss: 1.1360 - mean_absolute_error: 1.1360 - val_loss: 15.5145 - val_mean_absolute_error: 15.5145\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 15.15118\n",
            "Epoch 47/100\n",
            "288/288 [==============================] - 0s 541us/step - loss: 1.2290 - mean_absolute_error: 1.2290 - val_loss: 15.6884 - val_mean_absolute_error: 15.6884\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 15.15118\n",
            "Epoch 48/100\n",
            "288/288 [==============================] - 0s 528us/step - loss: 1.5857 - mean_absolute_error: 1.5857 - val_loss: 15.6184 - val_mean_absolute_error: 15.6184\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 15.15118\n",
            "Epoch 49/100\n",
            "288/288 [==============================] - 0s 532us/step - loss: 1.3685 - mean_absolute_error: 1.3685 - val_loss: 15.4656 - val_mean_absolute_error: 15.4656\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 15.15118\n",
            "Epoch 50/100\n",
            "288/288 [==============================] - 0s 555us/step - loss: 1.1275 - mean_absolute_error: 1.1275 - val_loss: 15.5004 - val_mean_absolute_error: 15.5004\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 15.15118\n",
            "Epoch 51/100\n",
            "288/288 [==============================] - 0s 561us/step - loss: 0.9665 - mean_absolute_error: 0.9665 - val_loss: 15.5049 - val_mean_absolute_error: 15.5049\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 15.15118\n",
            "Epoch 52/100\n",
            "288/288 [==============================] - 0s 556us/step - loss: 0.9920 - mean_absolute_error: 0.9920 - val_loss: 15.6552 - val_mean_absolute_error: 15.6552\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 15.15118\n",
            "Epoch 53/100\n",
            "288/288 [==============================] - 0s 550us/step - loss: 0.8400 - mean_absolute_error: 0.8400 - val_loss: 15.5543 - val_mean_absolute_error: 15.5543\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 15.15118\n",
            "Epoch 54/100\n",
            "288/288 [==============================] - 0s 565us/step - loss: 0.9020 - mean_absolute_error: 0.9020 - val_loss: 15.6023 - val_mean_absolute_error: 15.6023\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 15.15118\n",
            "Epoch 55/100\n",
            "288/288 [==============================] - 0s 533us/step - loss: 0.9974 - mean_absolute_error: 0.9974 - val_loss: 15.7842 - val_mean_absolute_error: 15.7842\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 15.15118\n",
            "Epoch 56/100\n",
            "288/288 [==============================] - 0s 602us/step - loss: 0.8744 - mean_absolute_error: 0.8744 - val_loss: 15.7354 - val_mean_absolute_error: 15.7354\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 15.15118\n",
            "Epoch 57/100\n",
            "288/288 [==============================] - 0s 558us/step - loss: 1.0484 - mean_absolute_error: 1.0484 - val_loss: 15.6374 - val_mean_absolute_error: 15.6374\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 15.15118\n",
            "Epoch 58/100\n",
            "288/288 [==============================] - 0s 563us/step - loss: 1.0103 - mean_absolute_error: 1.0103 - val_loss: 15.3086 - val_mean_absolute_error: 15.3086\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 15.15118\n",
            "Epoch 59/100\n",
            "288/288 [==============================] - 0s 534us/step - loss: 0.9494 - mean_absolute_error: 0.9494 - val_loss: 15.3928 - val_mean_absolute_error: 15.3928\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 15.15118\n",
            "Epoch 60/100\n",
            "288/288 [==============================] - 0s 576us/step - loss: 0.9871 - mean_absolute_error: 0.9871 - val_loss: 15.6259 - val_mean_absolute_error: 15.6259\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 15.15118\n",
            "Epoch 61/100\n",
            "288/288 [==============================] - 0s 563us/step - loss: 1.0982 - mean_absolute_error: 1.0982 - val_loss: 15.4930 - val_mean_absolute_error: 15.4930\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 15.15118\n",
            "Epoch 62/100\n",
            "288/288 [==============================] - 0s 532us/step - loss: 0.9540 - mean_absolute_error: 0.9540 - val_loss: 15.8150 - val_mean_absolute_error: 15.8150\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 15.15118\n",
            "Epoch 63/100\n",
            "288/288 [==============================] - 0s 548us/step - loss: 0.9482 - mean_absolute_error: 0.9482 - val_loss: 15.4510 - val_mean_absolute_error: 15.4510\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 15.15118\n",
            "Epoch 64/100\n",
            "288/288 [==============================] - 0s 537us/step - loss: 1.2128 - mean_absolute_error: 1.2128 - val_loss: 15.8265 - val_mean_absolute_error: 15.8265\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 15.15118\n",
            "Epoch 65/100\n",
            "288/288 [==============================] - 0s 551us/step - loss: 1.3801 - mean_absolute_error: 1.3801 - val_loss: 15.9678 - val_mean_absolute_error: 15.9678\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 15.15118\n",
            "Epoch 66/100\n",
            "288/288 [==============================] - 0s 537us/step - loss: 1.1619 - mean_absolute_error: 1.1619 - val_loss: 15.5020 - val_mean_absolute_error: 15.5020\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 15.15118\n",
            "Epoch 67/100\n",
            "288/288 [==============================] - 0s 574us/step - loss: 1.0669 - mean_absolute_error: 1.0669 - val_loss: 15.6355 - val_mean_absolute_error: 15.6355\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 15.15118\n",
            "Epoch 68/100\n",
            "288/288 [==============================] - 0s 540us/step - loss: 1.0885 - mean_absolute_error: 1.0885 - val_loss: 15.6654 - val_mean_absolute_error: 15.6654\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 15.15118\n",
            "Epoch 69/100\n",
            "288/288 [==============================] - 0s 586us/step - loss: 0.8417 - mean_absolute_error: 0.8417 - val_loss: 15.4122 - val_mean_absolute_error: 15.4122\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 15.15118\n",
            "Epoch 70/100\n",
            "288/288 [==============================] - 0s 539us/step - loss: 1.0389 - mean_absolute_error: 1.0389 - val_loss: 15.6794 - val_mean_absolute_error: 15.6794\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 15.15118\n",
            "Epoch 71/100\n",
            "288/288 [==============================] - 0s 563us/step - loss: 1.0442 - mean_absolute_error: 1.0442 - val_loss: 15.4676 - val_mean_absolute_error: 15.4676\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 15.15118\n",
            "Epoch 72/100\n",
            "288/288 [==============================] - 0s 575us/step - loss: 1.2486 - mean_absolute_error: 1.2486 - val_loss: 15.4416 - val_mean_absolute_error: 15.4416\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 15.15118\n",
            "Epoch 73/100\n",
            "288/288 [==============================] - 0s 544us/step - loss: 1.0232 - mean_absolute_error: 1.0232 - val_loss: 15.2798 - val_mean_absolute_error: 15.2798\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 15.15118\n",
            "Epoch 74/100\n",
            "288/288 [==============================] - 0s 563us/step - loss: 0.9810 - mean_absolute_error: 0.9810 - val_loss: 15.5997 - val_mean_absolute_error: 15.5997\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 15.15118\n",
            "Epoch 75/100\n",
            "288/288 [==============================] - 0s 541us/step - loss: 1.2275 - mean_absolute_error: 1.2275 - val_loss: 15.4460 - val_mean_absolute_error: 15.4460\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 15.15118\n",
            "Epoch 76/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 1.0855 - mean_absolute_error: 1.0855 - val_loss: 15.5432 - val_mean_absolute_error: 15.5432\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 15.15118\n",
            "Epoch 77/100\n",
            "288/288 [==============================] - 0s 534us/step - loss: 1.0116 - mean_absolute_error: 1.0116 - val_loss: 15.3597 - val_mean_absolute_error: 15.3597\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 15.15118\n",
            "Epoch 78/100\n",
            "288/288 [==============================] - 0s 535us/step - loss: 0.9813 - mean_absolute_error: 0.9813 - val_loss: 15.4633 - val_mean_absolute_error: 15.4633\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 15.15118\n",
            "Epoch 79/100\n",
            "288/288 [==============================] - 0s 541us/step - loss: 1.0188 - mean_absolute_error: 1.0188 - val_loss: 15.8187 - val_mean_absolute_error: 15.8187\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 15.15118\n",
            "Epoch 80/100\n",
            "288/288 [==============================] - 0s 537us/step - loss: 0.9533 - mean_absolute_error: 0.9533 - val_loss: 15.6501 - val_mean_absolute_error: 15.6501\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 15.15118\n",
            "Epoch 81/100\n",
            "288/288 [==============================] - 0s 557us/step - loss: 1.0223 - mean_absolute_error: 1.0223 - val_loss: 15.3393 - val_mean_absolute_error: 15.3393\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 15.15118\n",
            "Epoch 82/100\n",
            "288/288 [==============================] - 0s 565us/step - loss: 1.0716 - mean_absolute_error: 1.0716 - val_loss: 15.0119 - val_mean_absolute_error: 15.0119\n",
            "\n",
            "Epoch 00082: val_loss improved from 15.15118 to 15.01186, saving model to Weights-082--15.01186.hdf5\n",
            "Epoch 83/100\n",
            "288/288 [==============================] - 0s 548us/step - loss: 0.9560 - mean_absolute_error: 0.9560 - val_loss: 15.7886 - val_mean_absolute_error: 15.7886\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 15.01186\n",
            "Epoch 84/100\n",
            "288/288 [==============================] - 0s 552us/step - loss: 0.9902 - mean_absolute_error: 0.9902 - val_loss: 15.2387 - val_mean_absolute_error: 15.2387\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 15.01186\n",
            "Epoch 85/100\n",
            "288/288 [==============================] - 0s 559us/step - loss: 0.9292 - mean_absolute_error: 0.9292 - val_loss: 15.4584 - val_mean_absolute_error: 15.4584\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 15.01186\n",
            "Epoch 86/100\n",
            "288/288 [==============================] - 0s 535us/step - loss: 1.0166 - mean_absolute_error: 1.0166 - val_loss: 15.3641 - val_mean_absolute_error: 15.3641\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 15.01186\n",
            "Epoch 87/100\n",
            "288/288 [==============================] - 0s 574us/step - loss: 1.0703 - mean_absolute_error: 1.0703 - val_loss: 15.6946 - val_mean_absolute_error: 15.6946\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 15.01186\n",
            "Epoch 88/100\n",
            "288/288 [==============================] - 0s 566us/step - loss: 1.0040 - mean_absolute_error: 1.0040 - val_loss: 15.5179 - val_mean_absolute_error: 15.5179\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 15.01186\n",
            "Epoch 89/100\n",
            "288/288 [==============================] - 0s 553us/step - loss: 0.9606 - mean_absolute_error: 0.9606 - val_loss: 15.8764 - val_mean_absolute_error: 15.8764\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 15.01186\n",
            "Epoch 90/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 0.9591 - mean_absolute_error: 0.9591 - val_loss: 15.4507 - val_mean_absolute_error: 15.4507\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 15.01186\n",
            "Epoch 91/100\n",
            "288/288 [==============================] - 0s 544us/step - loss: 0.9421 - mean_absolute_error: 0.9421 - val_loss: 15.5609 - val_mean_absolute_error: 15.5609\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 15.01186\n",
            "Epoch 92/100\n",
            "288/288 [==============================] - 0s 567us/step - loss: 1.0102 - mean_absolute_error: 1.0102 - val_loss: 15.5893 - val_mean_absolute_error: 15.5893\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 15.01186\n",
            "Epoch 93/100\n",
            "288/288 [==============================] - 0s 576us/step - loss: 1.3097 - mean_absolute_error: 1.3097 - val_loss: 15.5028 - val_mean_absolute_error: 15.5028\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 15.01186\n",
            "Epoch 94/100\n",
            "288/288 [==============================] - 0s 574us/step - loss: 1.2228 - mean_absolute_error: 1.2228 - val_loss: 15.4971 - val_mean_absolute_error: 15.4971\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 15.01186\n",
            "Epoch 95/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 1.1890 - mean_absolute_error: 1.1890 - val_loss: 15.6455 - val_mean_absolute_error: 15.6455\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 15.01186\n",
            "Epoch 96/100\n",
            "288/288 [==============================] - 0s 575us/step - loss: 0.9460 - mean_absolute_error: 0.9460 - val_loss: 15.5329 - val_mean_absolute_error: 15.5329\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 15.01186\n",
            "Epoch 97/100\n",
            "288/288 [==============================] - 0s 547us/step - loss: 0.8109 - mean_absolute_error: 0.8109 - val_loss: 15.4904 - val_mean_absolute_error: 15.4904\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 15.01186\n",
            "Epoch 98/100\n",
            "288/288 [==============================] - 0s 552us/step - loss: 1.0873 - mean_absolute_error: 1.0873 - val_loss: 15.2842 - val_mean_absolute_error: 15.2842\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 15.01186\n",
            "Epoch 99/100\n",
            "288/288 [==============================] - 0s 535us/step - loss: 0.8980 - mean_absolute_error: 0.8980 - val_loss: 15.5724 - val_mean_absolute_error: 15.5724\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 15.01186\n",
            "Epoch 100/100\n",
            "288/288 [==============================] - 0s 554us/step - loss: 0.9462 - mean_absolute_error: 0.9462 - val_loss: 15.4194 - val_mean_absolute_error: 15.4194\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 15.01186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82a7d38ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "metadata": {
        "id": "2T5s7sEDlIhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4df7d37b-505f-49fa-bc2d-ff7225397399"
      },
      "cell_type": "code",
      "source": [
        "predictions = NN_model.predict(X_train)\n",
        "type(predictions)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "metadata": {
        "id": "URQn2PFqgAy-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions=predictions.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8VNLYjb4ohzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "accuracy=r2_score(Y_train,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJq56_i8ov3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "67820bae-7195-486e-a551-a3028e777dd5"
      },
      "cell_type": "code",
      "source": [
        "print('train')\n",
        "print(accuracy)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "0.8852522673048104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6qxJkWUWzo6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6851
        },
        "outputId": "b44f1e60-917f-47df-f0e1-eec4897af3ab"
      },
      "cell_type": "code",
      "source": [
        "NN_model.fit(X_test, Y_test, epochs=100, batch_size=32, validation_split = 0.3,callbacks=callbacks_list) "
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 109 samples, validate on 47 samples\n",
            "Epoch 1/100\n",
            "109/109 [==============================] - 0s 751us/step - loss: 14.5450 - mean_absolute_error: 14.5450 - val_loss: 12.9778 - val_mean_absolute_error: 12.9778\n",
            "\n",
            "Epoch 00001: val_loss improved from 15.01186 to 12.97783, saving model to Weights-001--12.97783.hdf5\n",
            "Epoch 2/100\n",
            "109/109 [==============================] - 0s 584us/step - loss: 12.7144 - mean_absolute_error: 12.7144 - val_loss: 12.3036 - val_mean_absolute_error: 12.3036\n",
            "\n",
            "Epoch 00002: val_loss improved from 12.97783 to 12.30356, saving model to Weights-002--12.30356.hdf5\n",
            "Epoch 3/100\n",
            "109/109 [==============================] - 0s 588us/step - loss: 10.4903 - mean_absolute_error: 10.4903 - val_loss: 11.7108 - val_mean_absolute_error: 11.7108\n",
            "\n",
            "Epoch 00003: val_loss improved from 12.30356 to 11.71082, saving model to Weights-003--11.71082.hdf5\n",
            "Epoch 4/100\n",
            "109/109 [==============================] - 0s 581us/step - loss: 8.9189 - mean_absolute_error: 8.9189 - val_loss: 11.5599 - val_mean_absolute_error: 11.5599\n",
            "\n",
            "Epoch 00004: val_loss improved from 11.71082 to 11.55993, saving model to Weights-004--11.55993.hdf5\n",
            "Epoch 5/100\n",
            "109/109 [==============================] - 0s 599us/step - loss: 6.4185 - mean_absolute_error: 6.4185 - val_loss: 11.4541 - val_mean_absolute_error: 11.4541\n",
            "\n",
            "Epoch 00005: val_loss improved from 11.55993 to 11.45406, saving model to Weights-005--11.45406.hdf5\n",
            "Epoch 6/100\n",
            "109/109 [==============================] - 0s 697us/step - loss: 5.6473 - mean_absolute_error: 5.6473 - val_loss: 11.4798 - val_mean_absolute_error: 11.4798\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 11.45406\n",
            "Epoch 7/100\n",
            "109/109 [==============================] - 0s 603us/step - loss: 4.9906 - mean_absolute_error: 4.9906 - val_loss: 11.2927 - val_mean_absolute_error: 11.2927\n",
            "\n",
            "Epoch 00007: val_loss improved from 11.45406 to 11.29267, saving model to Weights-007--11.29267.hdf5\n",
            "Epoch 8/100\n",
            "109/109 [==============================] - 0s 564us/step - loss: 4.4241 - mean_absolute_error: 4.4241 - val_loss: 11.1243 - val_mean_absolute_error: 11.1243\n",
            "\n",
            "Epoch 00008: val_loss improved from 11.29267 to 11.12433, saving model to Weights-008--11.12433.hdf5\n",
            "Epoch 9/100\n",
            "109/109 [==============================] - 0s 592us/step - loss: 3.5041 - mean_absolute_error: 3.5041 - val_loss: 11.0363 - val_mean_absolute_error: 11.0363\n",
            "\n",
            "Epoch 00009: val_loss improved from 11.12433 to 11.03631, saving model to Weights-009--11.03631.hdf5\n",
            "Epoch 10/100\n",
            "109/109 [==============================] - 0s 623us/step - loss: 3.3128 - mean_absolute_error: 3.3128 - val_loss: 11.1536 - val_mean_absolute_error: 11.1536\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 11.03631\n",
            "Epoch 11/100\n",
            "109/109 [==============================] - 0s 603us/step - loss: 2.7239 - mean_absolute_error: 2.7239 - val_loss: 11.3481 - val_mean_absolute_error: 11.3481\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 11.03631\n",
            "Epoch 12/100\n",
            "109/109 [==============================] - 0s 592us/step - loss: 3.1042 - mean_absolute_error: 3.1042 - val_loss: 11.1233 - val_mean_absolute_error: 11.1233\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 11.03631\n",
            "Epoch 13/100\n",
            "109/109 [==============================] - 0s 600us/step - loss: 2.6345 - mean_absolute_error: 2.6345 - val_loss: 11.2809 - val_mean_absolute_error: 11.2809\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 11.03631\n",
            "Epoch 14/100\n",
            "109/109 [==============================] - 0s 581us/step - loss: 2.6233 - mean_absolute_error: 2.6233 - val_loss: 11.0077 - val_mean_absolute_error: 11.0077\n",
            "\n",
            "Epoch 00014: val_loss improved from 11.03631 to 11.00771, saving model to Weights-014--11.00771.hdf5\n",
            "Epoch 15/100\n",
            "109/109 [==============================] - 0s 631us/step - loss: 2.5993 - mean_absolute_error: 2.5993 - val_loss: 11.1648 - val_mean_absolute_error: 11.1648\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 11.00771\n",
            "Epoch 16/100\n",
            "109/109 [==============================] - 0s 645us/step - loss: 2.6306 - mean_absolute_error: 2.6306 - val_loss: 11.5840 - val_mean_absolute_error: 11.5840\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 11.00771\n",
            "Epoch 17/100\n",
            "109/109 [==============================] - 0s 580us/step - loss: 2.5351 - mean_absolute_error: 2.5351 - val_loss: 11.2836 - val_mean_absolute_error: 11.2836\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 11.00771\n",
            "Epoch 18/100\n",
            "109/109 [==============================] - 0s 642us/step - loss: 2.3138 - mean_absolute_error: 2.3138 - val_loss: 11.1534 - val_mean_absolute_error: 11.1534\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 11.00771\n",
            "Epoch 19/100\n",
            "109/109 [==============================] - 0s 680us/step - loss: 2.1262 - mean_absolute_error: 2.1262 - val_loss: 11.3384 - val_mean_absolute_error: 11.3384\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 11.00771\n",
            "Epoch 20/100\n",
            "109/109 [==============================] - 0s 611us/step - loss: 2.5761 - mean_absolute_error: 2.5761 - val_loss: 11.3234 - val_mean_absolute_error: 11.3234\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 11.00771\n",
            "Epoch 21/100\n",
            "109/109 [==============================] - 0s 565us/step - loss: 2.6500 - mean_absolute_error: 2.6500 - val_loss: 11.5068 - val_mean_absolute_error: 11.5068\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 11.00771\n",
            "Epoch 22/100\n",
            "109/109 [==============================] - 0s 661us/step - loss: 1.6758 - mean_absolute_error: 1.6758 - val_loss: 11.4100 - val_mean_absolute_error: 11.4100\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 11.00771\n",
            "Epoch 23/100\n",
            "109/109 [==============================] - 0s 583us/step - loss: 2.0601 - mean_absolute_error: 2.0601 - val_loss: 11.3783 - val_mean_absolute_error: 11.3783\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 11.00771\n",
            "Epoch 24/100\n",
            "109/109 [==============================] - 0s 618us/step - loss: 1.9775 - mean_absolute_error: 1.9775 - val_loss: 11.4585 - val_mean_absolute_error: 11.4585\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 11.00771\n",
            "Epoch 25/100\n",
            "109/109 [==============================] - 0s 581us/step - loss: 1.9944 - mean_absolute_error: 1.9944 - val_loss: 11.4317 - val_mean_absolute_error: 11.4317\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 11.00771\n",
            "Epoch 26/100\n",
            "109/109 [==============================] - 0s 582us/step - loss: 1.8837 - mean_absolute_error: 1.8837 - val_loss: 11.3379 - val_mean_absolute_error: 11.3379\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 11.00771\n",
            "Epoch 27/100\n",
            "109/109 [==============================] - 0s 680us/step - loss: 2.2579 - mean_absolute_error: 2.2579 - val_loss: 11.7658 - val_mean_absolute_error: 11.7658\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 11.00771\n",
            "Epoch 28/100\n",
            "109/109 [==============================] - 0s 595us/step - loss: 2.6877 - mean_absolute_error: 2.6877 - val_loss: 11.4114 - val_mean_absolute_error: 11.4114\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 11.00771\n",
            "Epoch 29/100\n",
            "109/109 [==============================] - 0s 587us/step - loss: 3.0816 - mean_absolute_error: 3.0816 - val_loss: 11.5058 - val_mean_absolute_error: 11.5058\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 11.00771\n",
            "Epoch 30/100\n",
            "109/109 [==============================] - 0s 538us/step - loss: 2.7346 - mean_absolute_error: 2.7346 - val_loss: 11.4660 - val_mean_absolute_error: 11.4660\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 11.00771\n",
            "Epoch 31/100\n",
            "109/109 [==============================] - 0s 674us/step - loss: 1.9355 - mean_absolute_error: 1.9355 - val_loss: 11.2073 - val_mean_absolute_error: 11.2073\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 11.00771\n",
            "Epoch 32/100\n",
            "109/109 [==============================] - 0s 616us/step - loss: 1.5729 - mean_absolute_error: 1.5729 - val_loss: 11.3308 - val_mean_absolute_error: 11.3308\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 11.00771\n",
            "Epoch 33/100\n",
            "109/109 [==============================] - 0s 614us/step - loss: 1.2749 - mean_absolute_error: 1.2749 - val_loss: 11.1965 - val_mean_absolute_error: 11.1965\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 11.00771\n",
            "Epoch 34/100\n",
            "109/109 [==============================] - 0s 625us/step - loss: 1.6407 - mean_absolute_error: 1.6407 - val_loss: 11.4021 - val_mean_absolute_error: 11.4021\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 11.00771\n",
            "Epoch 35/100\n",
            "109/109 [==============================] - 0s 647us/step - loss: 1.7451 - mean_absolute_error: 1.7451 - val_loss: 11.1831 - val_mean_absolute_error: 11.1831\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 11.00771\n",
            "Epoch 36/100\n",
            "109/109 [==============================] - 0s 657us/step - loss: 1.8124 - mean_absolute_error: 1.8124 - val_loss: 11.5237 - val_mean_absolute_error: 11.5237\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 11.00771\n",
            "Epoch 37/100\n",
            "109/109 [==============================] - 0s 610us/step - loss: 2.1962 - mean_absolute_error: 2.1962 - val_loss: 11.2116 - val_mean_absolute_error: 11.2116\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 11.00771\n",
            "Epoch 38/100\n",
            "109/109 [==============================] - 0s 685us/step - loss: 2.6626 - mean_absolute_error: 2.6626 - val_loss: 11.3034 - val_mean_absolute_error: 11.3034\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 11.00771\n",
            "Epoch 39/100\n",
            "109/109 [==============================] - 0s 630us/step - loss: 2.3465 - mean_absolute_error: 2.3465 - val_loss: 11.3540 - val_mean_absolute_error: 11.3540\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 11.00771\n",
            "Epoch 40/100\n",
            "109/109 [==============================] - 0s 670us/step - loss: 1.9589 - mean_absolute_error: 1.9589 - val_loss: 11.2361 - val_mean_absolute_error: 11.2361\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 11.00771\n",
            "Epoch 41/100\n",
            "109/109 [==============================] - 0s 632us/step - loss: 1.4764 - mean_absolute_error: 1.4764 - val_loss: 11.5815 - val_mean_absolute_error: 11.5815\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 11.00771\n",
            "Epoch 42/100\n",
            "109/109 [==============================] - 0s 661us/step - loss: 1.4097 - mean_absolute_error: 1.4097 - val_loss: 11.5755 - val_mean_absolute_error: 11.5755\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 11.00771\n",
            "Epoch 43/100\n",
            "109/109 [==============================] - 0s 661us/step - loss: 1.5507 - mean_absolute_error: 1.5507 - val_loss: 11.5184 - val_mean_absolute_error: 11.5184\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 11.00771\n",
            "Epoch 44/100\n",
            "109/109 [==============================] - 0s 606us/step - loss: 1.4140 - mean_absolute_error: 1.4140 - val_loss: 11.2762 - val_mean_absolute_error: 11.2762\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 11.00771\n",
            "Epoch 45/100\n",
            "109/109 [==============================] - 0s 649us/step - loss: 1.3250 - mean_absolute_error: 1.3250 - val_loss: 11.4750 - val_mean_absolute_error: 11.4750\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 11.00771\n",
            "Epoch 46/100\n",
            "109/109 [==============================] - 0s 679us/step - loss: 1.0776 - mean_absolute_error: 1.0776 - val_loss: 11.2652 - val_mean_absolute_error: 11.2652\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 11.00771\n",
            "Epoch 47/100\n",
            "109/109 [==============================] - 0s 612us/step - loss: 1.4929 - mean_absolute_error: 1.4929 - val_loss: 11.4598 - val_mean_absolute_error: 11.4598\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 11.00771\n",
            "Epoch 48/100\n",
            "109/109 [==============================] - 0s 625us/step - loss: 1.9011 - mean_absolute_error: 1.9011 - val_loss: 11.0559 - val_mean_absolute_error: 11.0559\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 11.00771\n",
            "Epoch 49/100\n",
            "109/109 [==============================] - 0s 693us/step - loss: 2.1830 - mean_absolute_error: 2.1830 - val_loss: 11.4595 - val_mean_absolute_error: 11.4595\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 11.00771\n",
            "Epoch 50/100\n",
            "109/109 [==============================] - 0s 618us/step - loss: 2.4771 - mean_absolute_error: 2.4771 - val_loss: 11.2914 - val_mean_absolute_error: 11.2914\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 11.00771\n",
            "Epoch 51/100\n",
            "109/109 [==============================] - 0s 638us/step - loss: 2.4995 - mean_absolute_error: 2.4995 - val_loss: 11.6353 - val_mean_absolute_error: 11.6353\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 11.00771\n",
            "Epoch 52/100\n",
            "109/109 [==============================] - 0s 624us/step - loss: 1.8496 - mean_absolute_error: 1.8496 - val_loss: 11.7475 - val_mean_absolute_error: 11.7475\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 11.00771\n",
            "Epoch 53/100\n",
            "109/109 [==============================] - 0s 604us/step - loss: 1.5392 - mean_absolute_error: 1.5392 - val_loss: 11.5698 - val_mean_absolute_error: 11.5698\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 11.00771\n",
            "Epoch 54/100\n",
            "109/109 [==============================] - 0s 684us/step - loss: 1.4391 - mean_absolute_error: 1.4391 - val_loss: 11.8132 - val_mean_absolute_error: 11.8132\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 11.00771\n",
            "Epoch 55/100\n",
            "109/109 [==============================] - 0s 659us/step - loss: 1.1139 - mean_absolute_error: 1.1139 - val_loss: 11.4776 - val_mean_absolute_error: 11.4776\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 11.00771\n",
            "Epoch 56/100\n",
            "109/109 [==============================] - 0s 591us/step - loss: 1.3689 - mean_absolute_error: 1.3689 - val_loss: 11.7260 - val_mean_absolute_error: 11.7260\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 11.00771\n",
            "Epoch 57/100\n",
            "109/109 [==============================] - 0s 639us/step - loss: 1.8457 - mean_absolute_error: 1.8457 - val_loss: 11.3444 - val_mean_absolute_error: 11.3444\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 11.00771\n",
            "Epoch 58/100\n",
            "109/109 [==============================] - 0s 654us/step - loss: 1.5048 - mean_absolute_error: 1.5048 - val_loss: 11.6101 - val_mean_absolute_error: 11.6101\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 11.00771\n",
            "Epoch 59/100\n",
            "109/109 [==============================] - 0s 618us/step - loss: 1.5031 - mean_absolute_error: 1.5031 - val_loss: 11.5688 - val_mean_absolute_error: 11.5688\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 11.00771\n",
            "Epoch 60/100\n",
            "109/109 [==============================] - 0s 648us/step - loss: 1.3127 - mean_absolute_error: 1.3127 - val_loss: 11.5666 - val_mean_absolute_error: 11.5666\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 11.00771\n",
            "Epoch 61/100\n",
            "109/109 [==============================] - 0s 632us/step - loss: 1.2042 - mean_absolute_error: 1.2042 - val_loss: 11.4290 - val_mean_absolute_error: 11.4290\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 11.00771\n",
            "Epoch 62/100\n",
            "109/109 [==============================] - 0s 596us/step - loss: 1.0436 - mean_absolute_error: 1.0436 - val_loss: 11.5783 - val_mean_absolute_error: 11.5783\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 11.00771\n",
            "Epoch 63/100\n",
            "109/109 [==============================] - 0s 630us/step - loss: 1.0922 - mean_absolute_error: 1.0922 - val_loss: 11.5584 - val_mean_absolute_error: 11.5584\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 11.00771\n",
            "Epoch 64/100\n",
            "109/109 [==============================] - 0s 649us/step - loss: 1.5461 - mean_absolute_error: 1.5461 - val_loss: 11.6948 - val_mean_absolute_error: 11.6948\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 11.00771\n",
            "Epoch 65/100\n",
            "109/109 [==============================] - 0s 644us/step - loss: 1.5758 - mean_absolute_error: 1.5758 - val_loss: 11.6291 - val_mean_absolute_error: 11.6291\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 11.00771\n",
            "Epoch 66/100\n",
            "109/109 [==============================] - 0s 652us/step - loss: 1.6023 - mean_absolute_error: 1.6023 - val_loss: 11.7627 - val_mean_absolute_error: 11.7627\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 11.00771\n",
            "Epoch 67/100\n",
            "109/109 [==============================] - 0s 982us/step - loss: 1.5249 - mean_absolute_error: 1.5249 - val_loss: 11.4838 - val_mean_absolute_error: 11.4838\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 11.00771\n",
            "Epoch 68/100\n",
            "109/109 [==============================] - 0s 723us/step - loss: 1.5228 - mean_absolute_error: 1.5228 - val_loss: 11.6658 - val_mean_absolute_error: 11.6658\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 11.00771\n",
            "Epoch 69/100\n",
            "109/109 [==============================] - 0s 689us/step - loss: 1.0734 - mean_absolute_error: 1.0734 - val_loss: 11.5067 - val_mean_absolute_error: 11.5067\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 11.00771\n",
            "Epoch 70/100\n",
            "109/109 [==============================] - 0s 645us/step - loss: 0.9835 - mean_absolute_error: 0.9835 - val_loss: 11.5298 - val_mean_absolute_error: 11.5298\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 11.00771\n",
            "Epoch 71/100\n",
            "109/109 [==============================] - 0s 685us/step - loss: 1.1071 - mean_absolute_error: 1.1071 - val_loss: 11.5409 - val_mean_absolute_error: 11.5409\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 11.00771\n",
            "Epoch 72/100\n",
            "109/109 [==============================] - 0s 666us/step - loss: 1.0732 - mean_absolute_error: 1.0732 - val_loss: 11.4022 - val_mean_absolute_error: 11.4022\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 11.00771\n",
            "Epoch 73/100\n",
            "109/109 [==============================] - 0s 680us/step - loss: 1.2276 - mean_absolute_error: 1.2276 - val_loss: 11.6748 - val_mean_absolute_error: 11.6748\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 11.00771\n",
            "Epoch 74/100\n",
            "109/109 [==============================] - 0s 606us/step - loss: 1.2652 - mean_absolute_error: 1.2652 - val_loss: 11.6671 - val_mean_absolute_error: 11.6671\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 11.00771\n",
            "Epoch 75/100\n",
            "109/109 [==============================] - 0s 585us/step - loss: 1.1603 - mean_absolute_error: 1.1603 - val_loss: 11.4899 - val_mean_absolute_error: 11.4899\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 11.00771\n",
            "Epoch 76/100\n",
            "109/109 [==============================] - 0s 603us/step - loss: 1.4150 - mean_absolute_error: 1.4150 - val_loss: 11.7481 - val_mean_absolute_error: 11.7481\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 11.00771\n",
            "Epoch 77/100\n",
            "109/109 [==============================] - 0s 605us/step - loss: 1.3709 - mean_absolute_error: 1.3709 - val_loss: 11.5412 - val_mean_absolute_error: 11.5412\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 11.00771\n",
            "Epoch 78/100\n",
            "109/109 [==============================] - 0s 702us/step - loss: 1.1929 - mean_absolute_error: 1.1929 - val_loss: 11.5617 - val_mean_absolute_error: 11.5617\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 11.00771\n",
            "Epoch 79/100\n",
            "109/109 [==============================] - 0s 630us/step - loss: 1.1365 - mean_absolute_error: 1.1365 - val_loss: 11.4855 - val_mean_absolute_error: 11.4855\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 11.00771\n",
            "Epoch 80/100\n",
            "109/109 [==============================] - 0s 621us/step - loss: 1.4356 - mean_absolute_error: 1.4356 - val_loss: 11.8524 - val_mean_absolute_error: 11.8524\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 11.00771\n",
            "Epoch 81/100\n",
            "109/109 [==============================] - 0s 637us/step - loss: 1.2958 - mean_absolute_error: 1.2958 - val_loss: 11.4138 - val_mean_absolute_error: 11.4138\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 11.00771\n",
            "Epoch 82/100\n",
            "109/109 [==============================] - 0s 626us/step - loss: 1.6948 - mean_absolute_error: 1.6948 - val_loss: 11.8291 - val_mean_absolute_error: 11.8291\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 11.00771\n",
            "Epoch 83/100\n",
            "109/109 [==============================] - 0s 652us/step - loss: 2.2747 - mean_absolute_error: 2.2747 - val_loss: 11.3596 - val_mean_absolute_error: 11.3596\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 11.00771\n",
            "Epoch 84/100\n",
            "109/109 [==============================] - 0s 661us/step - loss: 1.8798 - mean_absolute_error: 1.8798 - val_loss: 11.4772 - val_mean_absolute_error: 11.4772\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 11.00771\n",
            "Epoch 85/100\n",
            "109/109 [==============================] - 0s 655us/step - loss: 1.7987 - mean_absolute_error: 1.7987 - val_loss: 11.6035 - val_mean_absolute_error: 11.6035\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 11.00771\n",
            "Epoch 86/100\n",
            "109/109 [==============================] - 0s 670us/step - loss: 1.4804 - mean_absolute_error: 1.4804 - val_loss: 11.6827 - val_mean_absolute_error: 11.6827\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 11.00771\n",
            "Epoch 87/100\n",
            "109/109 [==============================] - 0s 650us/step - loss: 1.6565 - mean_absolute_error: 1.6565 - val_loss: 11.7039 - val_mean_absolute_error: 11.7039\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 11.00771\n",
            "Epoch 88/100\n",
            "109/109 [==============================] - 0s 750us/step - loss: 1.8941 - mean_absolute_error: 1.8941 - val_loss: 11.5956 - val_mean_absolute_error: 11.5956\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 11.00771\n",
            "Epoch 89/100\n",
            "109/109 [==============================] - 0s 634us/step - loss: 1.2482 - mean_absolute_error: 1.2482 - val_loss: 11.6853 - val_mean_absolute_error: 11.6853\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 11.00771\n",
            "Epoch 90/100\n",
            "109/109 [==============================] - 0s 667us/step - loss: 1.2673 - mean_absolute_error: 1.2673 - val_loss: 11.6292 - val_mean_absolute_error: 11.6292\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 11.00771\n",
            "Epoch 91/100\n",
            "109/109 [==============================] - 0s 659us/step - loss: 1.1490 - mean_absolute_error: 1.1490 - val_loss: 11.6226 - val_mean_absolute_error: 11.6226\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 11.00771\n",
            "Epoch 92/100\n",
            "109/109 [==============================] - 0s 645us/step - loss: 1.1090 - mean_absolute_error: 1.1090 - val_loss: 11.8155 - val_mean_absolute_error: 11.8155\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 11.00771\n",
            "Epoch 93/100\n",
            "109/109 [==============================] - 0s 632us/step - loss: 1.0123 - mean_absolute_error: 1.0123 - val_loss: 11.6116 - val_mean_absolute_error: 11.6116\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 11.00771\n",
            "Epoch 94/100\n",
            "109/109 [==============================] - 0s 640us/step - loss: 0.9066 - mean_absolute_error: 0.9066 - val_loss: 11.6619 - val_mean_absolute_error: 11.6619\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 11.00771\n",
            "Epoch 95/100\n",
            "109/109 [==============================] - 0s 648us/step - loss: 0.9103 - mean_absolute_error: 0.9103 - val_loss: 11.5438 - val_mean_absolute_error: 11.5438\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 11.00771\n",
            "Epoch 96/100\n",
            "109/109 [==============================] - 0s 635us/step - loss: 0.7988 - mean_absolute_error: 0.7988 - val_loss: 11.6629 - val_mean_absolute_error: 11.6629\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 11.00771\n",
            "Epoch 97/100\n",
            "109/109 [==============================] - 0s 659us/step - loss: 1.1088 - mean_absolute_error: 1.1088 - val_loss: 11.7352 - val_mean_absolute_error: 11.7352\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 11.00771\n",
            "Epoch 98/100\n",
            "109/109 [==============================] - 0s 642us/step - loss: 1.0002 - mean_absolute_error: 1.0002 - val_loss: 11.8348 - val_mean_absolute_error: 11.8348\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 11.00771\n",
            "Epoch 99/100\n",
            "109/109 [==============================] - 0s 655us/step - loss: 0.8584 - mean_absolute_error: 0.8584 - val_loss: 11.8816 - val_mean_absolute_error: 11.8816\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 11.00771\n",
            "Epoch 100/100\n",
            "109/109 [==============================] - 0s 699us/step - loss: 1.0593 - mean_absolute_error: 1.0593 - val_loss: 11.5342 - val_mean_absolute_error: 11.5342\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 11.00771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82a7d38e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "metadata": {
        "id": "zo42w0OU9bB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6e0e6303-bf65-43fe-c7c1-b802a36cb186"
      },
      "cell_type": "code",
      "source": [
        "predictions = NN_model.predict(X_test)\n",
        "from sklearn.metrics import r2_score\n",
        "accuracy=r2_score(Y_test,predictions)\n",
        "print('test')\n",
        "print(accuracy)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n",
            "0.9001341767808023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ezr-69UyCLGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34187
        },
        "outputId": "0c9fd712-2b4c-41ab-b96b-e5fd44164ff8"
      },
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "def build_regressor():\n",
        "    regressor = Sequential()\n",
        "    regressor.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
        "    regressor.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    regressor.add(Dense(units = 1, kernel_initializer = 'uniform'))\n",
        "    regressor.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n",
        "    return regressor\n",
        "regressorr = KerasRegressor(build_fn = build_regressor, batch_size = 10, epochs = 100)\n",
        "accuracies = cross_val_score(estimator = regressorr, X = X_train, y = Y_train, cv = 10, n_jobs = 1)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "324/324 [==============================] - 3s 9ms/step - loss: 1737.2733 - mean_absolute_error: 29.6826\n",
            "Epoch 2/100\n",
            "324/324 [==============================] - 0s 360us/step - loss: 1653.5205 - mean_absolute_error: 28.4177\n",
            "Epoch 3/100\n",
            "324/324 [==============================] - 0s 328us/step - loss: 1225.6011 - mean_absolute_error: 22.0173\n",
            "Epoch 4/100\n",
            "324/324 [==============================] - 0s 333us/step - loss: 818.5316 - mean_absolute_error: 18.1947\n",
            "Epoch 5/100\n",
            "324/324 [==============================] - 0s 353us/step - loss: 637.2933 - mean_absolute_error: 16.4385\n",
            "Epoch 6/100\n",
            "324/324 [==============================] - 0s 369us/step - loss: 539.3444 - mean_absolute_error: 15.9601\n",
            "Epoch 7/100\n",
            "324/324 [==============================] - 0s 390us/step - loss: 449.8670 - mean_absolute_error: 14.6336\n",
            "Epoch 8/100\n",
            "324/324 [==============================] - 0s 365us/step - loss: 370.1855 - mean_absolute_error: 13.3362\n",
            "Epoch 9/100\n",
            "324/324 [==============================] - 0s 356us/step - loss: 295.3127 - mean_absolute_error: 12.0228\n",
            "Epoch 10/100\n",
            "324/324 [==============================] - 0s 376us/step - loss: 229.6858 - mean_absolute_error: 10.9281\n",
            "Epoch 11/100\n",
            "324/324 [==============================] - 0s 377us/step - loss: 176.4145 - mean_absolute_error: 9.4113\n",
            "Epoch 12/100\n",
            "324/324 [==============================] - 0s 354us/step - loss: 136.4273 - mean_absolute_error: 8.4286\n",
            "Epoch 13/100\n",
            "324/324 [==============================] - 0s 371us/step - loss: 107.1588 - mean_absolute_error: 7.2988\n",
            "Epoch 14/100\n",
            "324/324 [==============================] - 0s 368us/step - loss: 88.4428 - mean_absolute_error: 6.6184\n",
            "Epoch 15/100\n",
            "324/324 [==============================] - 0s 380us/step - loss: 72.1925 - mean_absolute_error: 5.7776\n",
            "Epoch 16/100\n",
            "324/324 [==============================] - 0s 385us/step - loss: 61.8700 - mean_absolute_error: 5.4207\n",
            "Epoch 17/100\n",
            "324/324 [==============================] - 0s 366us/step - loss: 53.4165 - mean_absolute_error: 4.9907\n",
            "Epoch 18/100\n",
            "324/324 [==============================] - 0s 368us/step - loss: 44.5948 - mean_absolute_error: 4.5089\n",
            "Epoch 19/100\n",
            "324/324 [==============================] - 0s 360us/step - loss: 38.5262 - mean_absolute_error: 4.1515\n",
            "Epoch 20/100\n",
            "324/324 [==============================] - 0s 320us/step - loss: 32.6884 - mean_absolute_error: 3.7803\n",
            "Epoch 21/100\n",
            "324/324 [==============================] - 0s 331us/step - loss: 28.1356 - mean_absolute_error: 3.5034\n",
            "Epoch 22/100\n",
            "324/324 [==============================] - 0s 335us/step - loss: 24.3204 - mean_absolute_error: 3.2659\n",
            "Epoch 23/100\n",
            "324/324 [==============================] - 0s 348us/step - loss: 21.3689 - mean_absolute_error: 3.0175\n",
            "Epoch 24/100\n",
            "324/324 [==============================] - 0s 355us/step - loss: 17.6542 - mean_absolute_error: 2.7668\n",
            "Epoch 25/100\n",
            "324/324 [==============================] - 0s 358us/step - loss: 15.3336 - mean_absolute_error: 2.5504\n",
            "Epoch 26/100\n",
            "324/324 [==============================] - 0s 332us/step - loss: 12.9196 - mean_absolute_error: 2.2830\n",
            "Epoch 27/100\n",
            "324/324 [==============================] - 0s 352us/step - loss: 11.3732 - mean_absolute_error: 2.1930\n",
            "Epoch 28/100\n",
            "324/324 [==============================] - 0s 366us/step - loss: 9.5576 - mean_absolute_error: 2.0375\n",
            "Epoch 29/100\n",
            "324/324 [==============================] - 0s 335us/step - loss: 8.2783 - mean_absolute_error: 1.8548\n",
            "Epoch 30/100\n",
            "324/324 [==============================] - 0s 355us/step - loss: 6.8411 - mean_absolute_error: 1.7212\n",
            "Epoch 31/100\n",
            "324/324 [==============================] - 0s 373us/step - loss: 5.9048 - mean_absolute_error: 1.5562\n",
            "Epoch 32/100\n",
            "324/324 [==============================] - 0s 403us/step - loss: 5.0296 - mean_absolute_error: 1.4578\n",
            "Epoch 33/100\n",
            "324/324 [==============================] - 0s 389us/step - loss: 4.2623 - mean_absolute_error: 1.2926\n",
            "Epoch 34/100\n",
            "324/324 [==============================] - 0s 329us/step - loss: 3.5733 - mean_absolute_error: 1.1917\n",
            "Epoch 35/100\n",
            "324/324 [==============================] - 0s 340us/step - loss: 2.8946 - mean_absolute_error: 1.0762\n",
            "Epoch 36/100\n",
            "324/324 [==============================] - 0s 340us/step - loss: 2.5381 - mean_absolute_error: 1.0028\n",
            "Epoch 37/100\n",
            "324/324 [==============================] - 0s 338us/step - loss: 2.1703 - mean_absolute_error: 0.9445\n",
            "Epoch 38/100\n",
            "324/324 [==============================] - 0s 359us/step - loss: 1.8378 - mean_absolute_error: 0.8429\n",
            "Epoch 39/100\n",
            "324/324 [==============================] - 0s 369us/step - loss: 2.1589 - mean_absolute_error: 0.8296\n",
            "Epoch 40/100\n",
            "324/324 [==============================] - 0s 338us/step - loss: 1.3488 - mean_absolute_error: 0.7137\n",
            "Epoch 41/100\n",
            "324/324 [==============================] - 0s 349us/step - loss: 1.1569 - mean_absolute_error: 0.6465\n",
            "Epoch 42/100\n",
            "324/324 [==============================] - 0s 331us/step - loss: 0.9588 - mean_absolute_error: 0.5960\n",
            "Epoch 43/100\n",
            "324/324 [==============================] - 0s 330us/step - loss: 0.7953 - mean_absolute_error: 0.5332\n",
            "Epoch 44/100\n",
            "324/324 [==============================] - 0s 330us/step - loss: 0.6449 - mean_absolute_error: 0.4615\n",
            "Epoch 45/100\n",
            "324/324 [==============================] - 0s 330us/step - loss: 0.5582 - mean_absolute_error: 0.4344\n",
            "Epoch 46/100\n",
            "324/324 [==============================] - 0s 328us/step - loss: 0.4776 - mean_absolute_error: 0.3845\n",
            "Epoch 47/100\n",
            "324/324 [==============================] - 0s 331us/step - loss: 0.4618 - mean_absolute_error: 0.3729\n",
            "Epoch 48/100\n",
            "324/324 [==============================] - 0s 329us/step - loss: 0.4153 - mean_absolute_error: 0.3790\n",
            "Epoch 49/100\n",
            "324/324 [==============================] - 0s 383us/step - loss: 0.3476 - mean_absolute_error: 0.3184\n",
            "Epoch 50/100\n",
            "324/324 [==============================] - 0s 370us/step - loss: 0.3264 - mean_absolute_error: 0.2910\n",
            "Epoch 51/100\n",
            "324/324 [==============================] - 0s 375us/step - loss: 0.3053 - mean_absolute_error: 0.2769\n",
            "Epoch 52/100\n",
            "324/324 [==============================] - 0s 360us/step - loss: 0.2480 - mean_absolute_error: 0.2460\n",
            "Epoch 53/100\n",
            "324/324 [==============================] - 0s 365us/step - loss: 0.2326 - mean_absolute_error: 0.2259\n",
            "Epoch 54/100\n",
            "324/324 [==============================] - 0s 361us/step - loss: 0.2302 - mean_absolute_error: 0.2235\n",
            "Epoch 55/100\n",
            "324/324 [==============================] - 0s 339us/step - loss: 0.2108 - mean_absolute_error: 0.2080\n",
            "Epoch 56/100\n",
            "324/324 [==============================] - 0s 402us/step - loss: 0.1937 - mean_absolute_error: 0.2063\n",
            "Epoch 57/100\n",
            "324/324 [==============================] - 0s 353us/step - loss: 0.1882 - mean_absolute_error: 0.2077\n",
            "Epoch 58/100\n",
            "324/324 [==============================] - 0s 361us/step - loss: 0.2127 - mean_absolute_error: 0.2062\n",
            "Epoch 59/100\n",
            "324/324 [==============================] - 0s 381us/step - loss: 0.2084 - mean_absolute_error: 0.2069\n",
            "Epoch 60/100\n",
            "324/324 [==============================] - 0s 328us/step - loss: 0.1773 - mean_absolute_error: 0.1858\n",
            "Epoch 61/100\n",
            "324/324 [==============================] - 0s 325us/step - loss: 0.1865 - mean_absolute_error: 0.1885\n",
            "Epoch 62/100\n",
            "324/324 [==============================] - 0s 362us/step - loss: 0.2089 - mean_absolute_error: 0.2308\n",
            "Epoch 63/100\n",
            "324/324 [==============================] - 0s 371us/step - loss: 0.2236 - mean_absolute_error: 0.2223\n",
            "Epoch 64/100\n",
            "324/324 [==============================] - 0s 372us/step - loss: 0.2730 - mean_absolute_error: 0.2282\n",
            "Epoch 65/100\n",
            "324/324 [==============================] - 0s 332us/step - loss: 0.3421 - mean_absolute_error: 0.2551\n",
            "Epoch 66/100\n",
            "324/324 [==============================] - 0s 374us/step - loss: 0.4153 - mean_absolute_error: 0.2872\n",
            "Epoch 67/100\n",
            "324/324 [==============================] - 0s 330us/step - loss: 0.4136 - mean_absolute_error: 0.2744\n",
            "Epoch 68/100\n",
            "324/324 [==============================] - 0s 370us/step - loss: 0.4788 - mean_absolute_error: 0.2999\n",
            "Epoch 69/100\n",
            "324/324 [==============================] - 0s 389us/step - loss: 0.3620 - mean_absolute_error: 0.3152\n",
            "Epoch 70/100\n",
            "324/324 [==============================] - 0s 384us/step - loss: 0.2897 - mean_absolute_error: 0.3046\n",
            "Epoch 71/100\n",
            "324/324 [==============================] - 0s 332us/step - loss: 0.3390 - mean_absolute_error: 0.3099\n",
            "Epoch 72/100\n",
            "324/324 [==============================] - 0s 340us/step - loss: 0.2700 - mean_absolute_error: 0.2969\n",
            "Epoch 73/100\n",
            "324/324 [==============================] - 0s 361us/step - loss: 0.2246 - mean_absolute_error: 0.2510\n",
            "Epoch 74/100\n",
            "324/324 [==============================] - 0s 361us/step - loss: 0.2332 - mean_absolute_error: 0.2590\n",
            "Epoch 75/100\n",
            "324/324 [==============================] - 0s 363us/step - loss: 0.2658 - mean_absolute_error: 0.2726\n",
            "Epoch 76/100\n",
            "324/324 [==============================] - 0s 363us/step - loss: 0.2668 - mean_absolute_error: 0.2727\n",
            "Epoch 77/100\n",
            "324/324 [==============================] - 0s 373us/step - loss: 0.2848 - mean_absolute_error: 0.2906\n",
            "Epoch 78/100\n",
            "324/324 [==============================] - 0s 352us/step - loss: 0.3712 - mean_absolute_error: 0.3491\n",
            "Epoch 79/100\n",
            "324/324 [==============================] - 0s 326us/step - loss: 0.4732 - mean_absolute_error: 0.3815\n",
            "Epoch 80/100\n",
            "324/324 [==============================] - 0s 370us/step - loss: 0.5514 - mean_absolute_error: 0.4204\n",
            "Epoch 81/100\n",
            "324/324 [==============================] - 0s 366us/step - loss: 0.7557 - mean_absolute_error: 0.4791\n",
            "Epoch 82/100\n",
            "324/324 [==============================] - 0s 377us/step - loss: 0.8463 - mean_absolute_error: 0.5705\n",
            "Epoch 83/100\n",
            "324/324 [==============================] - 0s 370us/step - loss: 0.9464 - mean_absolute_error: 0.5736\n",
            "Epoch 84/100\n",
            "324/324 [==============================] - 0s 361us/step - loss: 0.9275 - mean_absolute_error: 0.5677\n",
            "Epoch 85/100\n",
            "324/324 [==============================] - 0s 335us/step - loss: 0.7355 - mean_absolute_error: 0.4986\n",
            "Epoch 86/100\n",
            "324/324 [==============================] - 0s 363us/step - loss: 0.8400 - mean_absolute_error: 0.5680\n",
            "Epoch 87/100\n",
            "324/324 [==============================] - 0s 361us/step - loss: 0.8023 - mean_absolute_error: 0.5470\n",
            "Epoch 88/100\n",
            "324/324 [==============================] - 0s 333us/step - loss: 0.8156 - mean_absolute_error: 0.5548\n",
            "Epoch 89/100\n",
            "324/324 [==============================] - 0s 336us/step - loss: 0.9436 - mean_absolute_error: 0.5280\n",
            "Epoch 90/100\n",
            "324/324 [==============================] - 0s 323us/step - loss: 0.8559 - mean_absolute_error: 0.5520\n",
            "Epoch 91/100\n",
            "324/324 [==============================] - 0s 361us/step - loss: 0.8352 - mean_absolute_error: 0.5101\n",
            "Epoch 92/100\n",
            "324/324 [==============================] - 0s 332us/step - loss: 1.0696 - mean_absolute_error: 0.6476\n",
            "Epoch 93/100\n",
            "324/324 [==============================] - 0s 340us/step - loss: 1.1641 - mean_absolute_error: 0.6817\n",
            "Epoch 94/100\n",
            "324/324 [==============================] - 0s 336us/step - loss: 1.3094 - mean_absolute_error: 0.6895\n",
            "Epoch 95/100\n",
            "324/324 [==============================] - 0s 344us/step - loss: 1.2806 - mean_absolute_error: 0.7098\n",
            "Epoch 96/100\n",
            "324/324 [==============================] - 0s 342us/step - loss: 1.6225 - mean_absolute_error: 0.8166\n",
            "Epoch 97/100\n",
            "324/324 [==============================] - 0s 316us/step - loss: 2.2536 - mean_absolute_error: 0.8859\n",
            "Epoch 98/100\n",
            "324/324 [==============================] - 0s 332us/step - loss: 1.8060 - mean_absolute_error: 0.8227\n",
            "Epoch 99/100\n",
            "324/324 [==============================] - 0s 348us/step - loss: 1.6990 - mean_absolute_error: 0.7318\n",
            "Epoch 100/100\n",
            "324/324 [==============================] - 0s 318us/step - loss: 1.8339 - mean_absolute_error: 0.6929\n",
            "37/37 [==============================] - 1s 28ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 8ms/step - loss: 1747.1217 - mean_absolute_error: 29.8430\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 1655.5680 - mean_absolute_error: 28.3818\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 1243.1689 - mean_absolute_error: 22.4540\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 815.7792 - mean_absolute_error: 18.1376\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 643.9755 - mean_absolute_error: 16.6325\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 544.3305 - mean_absolute_error: 15.8441\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 421.9351 - mean_absolute_error: 14.0852\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 322.9639 - mean_absolute_error: 12.3097\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 323us/step - loss: 236.8590 - mean_absolute_error: 10.4823\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 171.2497 - mean_absolute_error: 9.0650\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 128.1548 - mean_absolute_error: 7.7960\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 99.9377 - mean_absolute_error: 6.7753\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 81.3108 - mean_absolute_error: 6.1494\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 64.4901 - mean_absolute_error: 5.3850\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 55.2563 - mean_absolute_error: 4.7687\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 47.7425 - mean_absolute_error: 4.5323\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 39.7485 - mean_absolute_error: 4.0862\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 34.1060 - mean_absolute_error: 3.6627\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 29.3577 - mean_absolute_error: 3.4527\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 25.8054 - mean_absolute_error: 3.1464\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 22.0794 - mean_absolute_error: 3.0213\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 354us/step - loss: 17.9160 - mean_absolute_error: 2.6526\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 15.0383 - mean_absolute_error: 2.4389\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 349us/step - loss: 12.7960 - mean_absolute_error: 2.2042\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 10.3680 - mean_absolute_error: 1.9966\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 9.2258 - mean_absolute_error: 1.9350\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 7.6579 - mean_absolute_error: 1.7082\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 5.9345 - mean_absolute_error: 1.4980\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 343us/step - loss: 4.9444 - mean_absolute_error: 1.4143\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 3.9862 - mean_absolute_error: 1.2540\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 3.4276 - mean_absolute_error: 1.1548\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 2.7483 - mean_absolute_error: 1.0291\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 2.4095 - mean_absolute_error: 0.9673\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 2.1172 - mean_absolute_error: 0.9170\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 1.8769 - mean_absolute_error: 0.8751\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 1.7834 - mean_absolute_error: 0.8472\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 1.6408 - mean_absolute_error: 0.7646\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 1.4946 - mean_absolute_error: 0.7421\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 1.3893 - mean_absolute_error: 0.7239\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 1.0823 - mean_absolute_error: 0.6691\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 0.8249 - mean_absolute_error: 0.5741\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 344us/step - loss: 0.8586 - mean_absolute_error: 0.5343\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 0.6314 - mean_absolute_error: 0.4945\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 0.6581 - mean_absolute_error: 0.4565\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 0.7479 - mean_absolute_error: 0.5727\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 0.9537 - mean_absolute_error: 0.5767\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 0.8416 - mean_absolute_error: 0.5203\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 1.0326 - mean_absolute_error: 0.5504\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 0.9740 - mean_absolute_error: 0.5929\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 349us/step - loss: 0.9837 - mean_absolute_error: 0.6317\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 0.7286 - mean_absolute_error: 0.5431\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 385us/step - loss: 0.7136 - mean_absolute_error: 0.4798\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 390us/step - loss: 0.7910 - mean_absolute_error: 0.5028\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 415us/step - loss: 0.9176 - mean_absolute_error: 0.4774\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 1.1952 - mean_absolute_error: 0.5703\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 1.5883 - mean_absolute_error: 0.6640\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 1.4030 - mean_absolute_error: 0.6620\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 1.7606 - mean_absolute_error: 0.7892\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 1.6874 - mean_absolute_error: 0.7426\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 1.5989 - mean_absolute_error: 0.8077\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 2.4914 - mean_absolute_error: 0.8540\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 1.5917 - mean_absolute_error: 0.8107\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 1.3628 - mean_absolute_error: 0.6839\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 1.3777 - mean_absolute_error: 0.6724\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 0.7309 - mean_absolute_error: 0.5567\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 0.6486 - mean_absolute_error: 0.5175\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 0.4993 - mean_absolute_error: 0.4527\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 0.5379 - mean_absolute_error: 0.4489\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 0.5389 - mean_absolute_error: 0.4711\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 0.5567 - mean_absolute_error: 0.4319\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 383us/step - loss: 0.7473 - mean_absolute_error: 0.4586\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 0.8028 - mean_absolute_error: 0.4517\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 0.7525 - mean_absolute_error: 0.5103\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 1.4254 - mean_absolute_error: 0.7316\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 2.0109 - mean_absolute_error: 0.8887\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 1.6401 - mean_absolute_error: 0.8203\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 1.6720 - mean_absolute_error: 0.8325\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 374us/step - loss: 1.8705 - mean_absolute_error: 0.8793\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 1.7912 - mean_absolute_error: 0.8211\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 1.7941 - mean_absolute_error: 0.8121\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 2.2816 - mean_absolute_error: 1.0403\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 2.3421 - mean_absolute_error: 1.0233\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 3.6448 - mean_absolute_error: 1.3282\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 5.4472 - mean_absolute_error: 1.2386\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 2.4414 - mean_absolute_error: 1.0531\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 1.8271 - mean_absolute_error: 0.9071\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 1.3965 - mean_absolute_error: 0.7902\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 398us/step - loss: 1.1163 - mean_absolute_error: 0.6801\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 1.0859 - mean_absolute_error: 0.6039\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 0.9342 - mean_absolute_error: 0.6230\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 412us/step - loss: 0.8619 - mean_absolute_error: 0.5613\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 0.8221 - mean_absolute_error: 0.5463\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 0.9949 - mean_absolute_error: 0.6368\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 1.2698 - mean_absolute_error: 0.7277\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 1.1041 - mean_absolute_error: 0.6459\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 1.0878 - mean_absolute_error: 0.6155\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 1.1967 - mean_absolute_error: 0.7050\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 343us/step - loss: 1.2460 - mean_absolute_error: 0.7375\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 1.4491 - mean_absolute_error: 0.7536\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 1.5904 - mean_absolute_error: 0.8647\n",
            "36/36 [==============================] - 1s 30ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 8ms/step - loss: 1661.1473 - mean_absolute_error: 29.3157\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 1551.7867 - mean_absolute_error: 27.5926\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 374us/step - loss: 1114.5351 - mean_absolute_error: 20.7801\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 732.7593 - mean_absolute_error: 16.9870\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 598.3218 - mean_absolute_error: 15.9999\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 505.8041 - mean_absolute_error: 14.5240\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 421.0374 - mean_absolute_error: 13.5136\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 343.1887 - mean_absolute_error: 12.8638\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 262.3417 - mean_absolute_error: 11.0964\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 198.8852 - mean_absolute_error: 9.7759\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 144.3121 - mean_absolute_error: 8.1861\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 106.5364 - mean_absolute_error: 7.1374\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 80.5166 - mean_absolute_error: 6.1204\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 64.5151 - mean_absolute_error: 5.5067\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 54.7037 - mean_absolute_error: 4.9448\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 45.8071 - mean_absolute_error: 4.5958\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 39.2960 - mean_absolute_error: 4.1654\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 33.8021 - mean_absolute_error: 3.8077\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 28.2736 - mean_absolute_error: 3.4508\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 24.4066 - mean_absolute_error: 3.1320\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 349us/step - loss: 21.1107 - mean_absolute_error: 2.9371\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 18.3228 - mean_absolute_error: 2.7133\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 354us/step - loss: 15.7233 - mean_absolute_error: 2.5002\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 13.7344 - mean_absolute_error: 2.3201\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 11.5887 - mean_absolute_error: 2.0940\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 9.9228 - mean_absolute_error: 1.8711\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 9.0074 - mean_absolute_error: 1.8134\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 7.3920 - mean_absolute_error: 1.6098\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 6.4638 - mean_absolute_error: 1.4920\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 5.7958 - mean_absolute_error: 1.4254\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 4.9501 - mean_absolute_error: 1.2813\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 4.2552 - mean_absolute_error: 1.1756\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 3.9549 - mean_absolute_error: 1.1226\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 3.4209 - mean_absolute_error: 1.0447\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 2.8488 - mean_absolute_error: 0.9292\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 2.4941 - mean_absolute_error: 0.8435\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 2.2763 - mean_absolute_error: 0.8373\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 1.9997 - mean_absolute_error: 0.7495\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 1.8065 - mean_absolute_error: 0.6938\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 1.6238 - mean_absolute_error: 0.6276\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 1.5943 - mean_absolute_error: 0.6175\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 1.3777 - mean_absolute_error: 0.5833\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 339us/step - loss: 1.3923 - mean_absolute_error: 0.6256\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 1.2988 - mean_absolute_error: 0.5844\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 338us/step - loss: 1.2617 - mean_absolute_error: 0.5742\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 1.2313 - mean_absolute_error: 0.5808\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 417us/step - loss: 1.1493 - mean_absolute_error: 0.5256\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 1.1814 - mean_absolute_error: 0.5663\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 1.2355 - mean_absolute_error: 0.5682\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 1.2843 - mean_absolute_error: 0.5921\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 392us/step - loss: 1.3066 - mean_absolute_error: 0.5662\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 1.4337 - mean_absolute_error: 0.5857\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 1.6429 - mean_absolute_error: 0.7037\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 1.7952 - mean_absolute_error: 0.7489\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 1.9924 - mean_absolute_error: 0.7523\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 401us/step - loss: 2.5153 - mean_absolute_error: 0.8938\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 389us/step - loss: 2.9920 - mean_absolute_error: 0.9910\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 2.1920 - mean_absolute_error: 0.8244\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 1.6046 - mean_absolute_error: 0.7109\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 1.4612 - mean_absolute_error: 0.6270\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 1.8956 - mean_absolute_error: 0.7106\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 1.4863 - mean_absolute_error: 0.6897\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 1.3552 - mean_absolute_error: 0.5908\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 1.2526 - mean_absolute_error: 0.5430\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 1.0217 - mean_absolute_error: 0.5045\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 0.9796 - mean_absolute_error: 0.4744\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 0.9885 - mean_absolute_error: 0.4960\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 1.4934 - mean_absolute_error: 0.6372\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 1.2787 - mean_absolute_error: 0.6137\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 1.4363 - mean_absolute_error: 0.5991\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 405us/step - loss: 1.2244 - mean_absolute_error: 0.5533\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 383us/step - loss: 1.1795 - mean_absolute_error: 0.5928\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 1.1513 - mean_absolute_error: 0.5874\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 0.9964 - mean_absolute_error: 0.5034\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 1.2174 - mean_absolute_error: 0.5647\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 1.5039 - mean_absolute_error: 0.6440\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 1.6763 - mean_absolute_error: 0.6891\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 375us/step - loss: 1.7469 - mean_absolute_error: 0.7356\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 1.8560 - mean_absolute_error: 0.7515\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 2.1542 - mean_absolute_error: 0.8691\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 2.1306 - mean_absolute_error: 0.8370\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 323us/step - loss: 1.7003 - mean_absolute_error: 0.7577\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 340us/step - loss: 1.4018 - mean_absolute_error: 0.6420\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 323us/step - loss: 1.4223 - mean_absolute_error: 0.6421\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 1.4062 - mean_absolute_error: 0.6226\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 2.0362 - mean_absolute_error: 0.8452\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 2.5468 - mean_absolute_error: 0.9685\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 2.6144 - mean_absolute_error: 0.9377\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 381us/step - loss: 2.5126 - mean_absolute_error: 0.9763\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 2.5497 - mean_absolute_error: 0.9927\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 2.1215 - mean_absolute_error: 0.9388\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 1.8474 - mean_absolute_error: 0.8716\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 1.8155 - mean_absolute_error: 0.8339\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 1.8932 - mean_absolute_error: 0.8294\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 2.2197 - mean_absolute_error: 0.9593\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 416us/step - loss: 2.7005 - mean_absolute_error: 1.0282\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 2.3828 - mean_absolute_error: 0.9779\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 1.7486 - mean_absolute_error: 0.7901\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 1.8272 - mean_absolute_error: 0.8212\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 2.2775 - mean_absolute_error: 0.9502\n",
            "36/36 [==============================] - 1s 30ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 9ms/step - loss: 1603.8786 - mean_absolute_error: 29.7530\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 381us/step - loss: 1495.7165 - mean_absolute_error: 28.0127\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 390us/step - loss: 1106.5963 - mean_absolute_error: 21.9070\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 738.5512 - mean_absolute_error: 18.1389\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 384us/step - loss: 597.8821 - mean_absolute_error: 16.4901\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 508.2087 - mean_absolute_error: 15.5454\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 430.1822 - mean_absolute_error: 14.3217\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 401us/step - loss: 367.4640 - mean_absolute_error: 13.6601\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 305.6108 - mean_absolute_error: 12.1255\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 249.5771 - mean_absolute_error: 11.4143\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 204.6753 - mean_absolute_error: 10.4458\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 399us/step - loss: 163.4655 - mean_absolute_error: 8.9772\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 128.4151 - mean_absolute_error: 8.1294\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 103.5317 - mean_absolute_error: 7.1957\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 384us/step - loss: 84.9382 - mean_absolute_error: 6.3688\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 71.0324 - mean_absolute_error: 5.9108\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 59.5717 - mean_absolute_error: 5.2565\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 51.2373 - mean_absolute_error: 4.9000\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 44.1717 - mean_absolute_error: 4.5113\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 391us/step - loss: 37.9957 - mean_absolute_error: 4.1150\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 32.6764 - mean_absolute_error: 3.8784\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 410us/step - loss: 28.3741 - mean_absolute_error: 3.5389\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 392us/step - loss: 25.2048 - mean_absolute_error: 3.3520\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 343us/step - loss: 21.3996 - mean_absolute_error: 3.1643\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 354us/step - loss: 18.5924 - mean_absolute_error: 2.8846\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 338us/step - loss: 15.7227 - mean_absolute_error: 2.6078\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 13.8042 - mean_absolute_error: 2.4382\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 384us/step - loss: 11.6135 - mean_absolute_error: 2.2490\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 9.6878 - mean_absolute_error: 2.0690\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 8.3364 - mean_absolute_error: 1.9162\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 7.2594 - mean_absolute_error: 1.7594\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 6.2087 - mean_absolute_error: 1.6356\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 5.3794 - mean_absolute_error: 1.5176\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 4.4032 - mean_absolute_error: 1.3455\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 3.7418 - mean_absolute_error: 1.2375\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 3.1725 - mean_absolute_error: 1.1433\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 395us/step - loss: 2.6909 - mean_absolute_error: 1.0431\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 2.3025 - mean_absolute_error: 0.9525\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 2.3455 - mean_absolute_error: 1.0287\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 1.9755 - mean_absolute_error: 0.9590\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 2.2994 - mean_absolute_error: 0.9140\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 387us/step - loss: 1.9368 - mean_absolute_error: 0.8862\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 383us/step - loss: 1.4589 - mean_absolute_error: 0.7241\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 408us/step - loss: 0.8767 - mean_absolute_error: 0.6006\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 0.7328 - mean_absolute_error: 0.5256\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 395us/step - loss: 0.6268 - mean_absolute_error: 0.4833\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 0.5454 - mean_absolute_error: 0.4526\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 0.4829 - mean_absolute_error: 0.4127\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 0.4163 - mean_absolute_error: 0.3801\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 0.4222 - mean_absolute_error: 0.3836\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 0.4065 - mean_absolute_error: 0.3493\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 405us/step - loss: 0.4033 - mean_absolute_error: 0.3203\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 0.3640 - mean_absolute_error: 0.3005\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 401us/step - loss: 0.3076 - mean_absolute_error: 0.2779\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 0.2623 - mean_absolute_error: 0.2599\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 0.2079 - mean_absolute_error: 0.2458\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 0.1574 - mean_absolute_error: 0.2140\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 349us/step - loss: 0.1482 - mean_absolute_error: 0.1945\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 0.1487 - mean_absolute_error: 0.1965\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 375us/step - loss: 0.1535 - mean_absolute_error: 0.1936\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 0.1734 - mean_absolute_error: 0.1986\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 0.1758 - mean_absolute_error: 0.2033\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 0.1906 - mean_absolute_error: 0.1939\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 383us/step - loss: 0.2079 - mean_absolute_error: 0.2044\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 344us/step - loss: 0.2235 - mean_absolute_error: 0.2176\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 354us/step - loss: 0.1492 - mean_absolute_error: 0.2033\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 0.1735 - mean_absolute_error: 0.1937\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 0.2017 - mean_absolute_error: 0.2084\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 406us/step - loss: 0.2550 - mean_absolute_error: 0.2532\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 383us/step - loss: 0.2835 - mean_absolute_error: 0.2474\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 0.2771 - mean_absolute_error: 0.2429\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 0.3210 - mean_absolute_error: 0.2843\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 0.3486 - mean_absolute_error: 0.2806\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 0.4352 - mean_absolute_error: 0.3348\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 0.4599 - mean_absolute_error: 0.3579\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 0.4483 - mean_absolute_error: 0.3462\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 0.5341 - mean_absolute_error: 0.3854\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 0.6997 - mean_absolute_error: 0.4056\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 0.4281 - mean_absolute_error: 0.4066\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 0.8402 - mean_absolute_error: 0.4476\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 1.2515 - mean_absolute_error: 0.4496\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 0.7156 - mean_absolute_error: 0.4567\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 0.6751 - mean_absolute_error: 0.4985\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 0.7000 - mean_absolute_error: 0.5009\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 375us/step - loss: 0.6628 - mean_absolute_error: 0.5082\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 410us/step - loss: 0.7837 - mean_absolute_error: 0.5430\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 0.7103 - mean_absolute_error: 0.5215\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 374us/step - loss: 0.5776 - mean_absolute_error: 0.4827\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 0.7011 - mean_absolute_error: 0.4928\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 389us/step - loss: 0.9387 - mean_absolute_error: 0.5172\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 0.9935 - mean_absolute_error: 0.6557\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 0.9747 - mean_absolute_error: 0.6260\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 1.3281 - mean_absolute_error: 0.7288\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 401us/step - loss: 1.3049 - mean_absolute_error: 0.7085\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 0.9849 - mean_absolute_error: 0.6145\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 1.6325 - mean_absolute_error: 0.8134\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 1.7765 - mean_absolute_error: 0.8470\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 2.2075 - mean_absolute_error: 0.9199\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 1.2730 - mean_absolute_error: 0.7849\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 0.9416 - mean_absolute_error: 0.6521\n",
            "36/36 [==============================] - 1s 31ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 9ms/step - loss: 1715.7260 - mean_absolute_error: 29.5824\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 1535.5660 - mean_absolute_error: 26.9351\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 1048.5152 - mean_absolute_error: 20.2818\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 748.2496 - mean_absolute_error: 17.7150\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 610.8846 - mean_absolute_error: 16.5061\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 509.4058 - mean_absolute_error: 15.6616\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 411.3046 - mean_absolute_error: 13.9510\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 329.5868 - mean_absolute_error: 12.6067\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 257.7961 - mean_absolute_error: 11.6981\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 192.8218 - mean_absolute_error: 9.7430\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 152.0970 - mean_absolute_error: 9.0017\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 111.7554 - mean_absolute_error: 7.4332\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 87.9630 - mean_absolute_error: 6.5852\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 70.8217 - mean_absolute_error: 5.9257\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 59.5416 - mean_absolute_error: 5.3084\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 51.0000 - mean_absolute_error: 4.9691\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 42.6018 - mean_absolute_error: 4.4664\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 36.3366 - mean_absolute_error: 4.0491\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 31.9622 - mean_absolute_error: 3.7535\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 27.7967 - mean_absolute_error: 3.5202\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 23.7488 - mean_absolute_error: 3.2156\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 20.4893 - mean_absolute_error: 2.9640\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 17.9718 - mean_absolute_error: 2.7612\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 15.9194 - mean_absolute_error: 2.6135\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 13.3134 - mean_absolute_error: 2.3750\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 11.5125 - mean_absolute_error: 2.2090\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 9.7420 - mean_absolute_error: 1.9940\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 8.4206 - mean_absolute_error: 1.9015\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 7.4047 - mean_absolute_error: 1.7627\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 6.0445 - mean_absolute_error: 1.4795\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 5.2686 - mean_absolute_error: 1.3928\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 4.5571 - mean_absolute_error: 1.2771\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 389us/step - loss: 3.9340 - mean_absolute_error: 1.1585\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 3.2983 - mean_absolute_error: 1.0556\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 2.8106 - mean_absolute_error: 0.9438\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 2.5429 - mean_absolute_error: 0.9054\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 2.1355 - mean_absolute_error: 0.8318\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 344us/step - loss: 1.7751 - mean_absolute_error: 0.7543\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 1.5710 - mean_absolute_error: 0.7868\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 1.3392 - mean_absolute_error: 0.6841\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 1.2304 - mean_absolute_error: 0.6342\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 1.2926 - mean_absolute_error: 0.6024\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 0.9883 - mean_absolute_error: 0.5702\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 0.6954 - mean_absolute_error: 0.4850\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 0.5959 - mean_absolute_error: 0.4261\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 0.6273 - mean_absolute_error: 0.4301\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 0.6087 - mean_absolute_error: 0.4097\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 347us/step - loss: 0.7057 - mean_absolute_error: 0.4212\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 0.8673 - mean_absolute_error: 0.4817\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 1.1228 - mean_absolute_error: 0.5055\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 1.1191 - mean_absolute_error: 0.4820\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 0.7493 - mean_absolute_error: 0.4822\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 0.6402 - mean_absolute_error: 0.4557\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 0.6198 - mean_absolute_error: 0.4555\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 0.6632 - mean_absolute_error: 0.4478\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 0.6601 - mean_absolute_error: 0.4222\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 0.6214 - mean_absolute_error: 0.4518\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 0.7097 - mean_absolute_error: 0.4495\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 0.7501 - mean_absolute_error: 0.4182\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 0.6867 - mean_absolute_error: 0.4110\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 375us/step - loss: 0.4246 - mean_absolute_error: 0.3740\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 0.2771 - mean_absolute_error: 0.2925\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 0.2786 - mean_absolute_error: 0.2783\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 0.3086 - mean_absolute_error: 0.3015\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 0.3484 - mean_absolute_error: 0.3067\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 0.5122 - mean_absolute_error: 0.3386\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 0.5456 - mean_absolute_error: 0.3452\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 393us/step - loss: 0.5991 - mean_absolute_error: 0.4029\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 0.6545 - mean_absolute_error: 0.4193\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 0.7064 - mean_absolute_error: 0.4224\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 0.8572 - mean_absolute_error: 0.4825\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 1.1553 - mean_absolute_error: 0.5334\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 1.3814 - mean_absolute_error: 0.5848\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 340us/step - loss: 1.2670 - mean_absolute_error: 0.6589\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 1.5227 - mean_absolute_error: 0.7165\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 1.3322 - mean_absolute_error: 0.6847\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 387us/step - loss: 1.0662 - mean_absolute_error: 0.6357\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 1.1037 - mean_absolute_error: 0.6278\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 0.9010 - mean_absolute_error: 0.5873\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 384us/step - loss: 0.7967 - mean_absolute_error: 0.5208\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 0.7271 - mean_absolute_error: 0.5062\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 0.7325 - mean_absolute_error: 0.5077\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 0.6541 - mean_absolute_error: 0.4570\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 0.7447 - mean_absolute_error: 0.4774\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 1.0373 - mean_absolute_error: 0.5954\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 0.8736 - mean_absolute_error: 0.5669\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 391us/step - loss: 0.9529 - mean_absolute_error: 0.6314\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 0.9784 - mean_absolute_error: 0.6496\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 0.7923 - mean_absolute_error: 0.5812\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 0.7116 - mean_absolute_error: 0.5060\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 1.1484 - mean_absolute_error: 0.6057\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 5.1677 - mean_absolute_error: 0.6946\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 1.0687 - mean_absolute_error: 0.7047\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 381us/step - loss: 1.0074 - mean_absolute_error: 0.6739\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 0.9990 - mean_absolute_error: 0.6081\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 354us/step - loss: 1.1284 - mean_absolute_error: 0.6043\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 1.5659 - mean_absolute_error: 0.7676\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 2.1623 - mean_absolute_error: 0.9497\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 2.2438 - mean_absolute_error: 0.9454\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 3.3353 - mean_absolute_error: 1.1613\n",
            "36/36 [==============================] - 1s 31ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 9ms/step - loss: 1715.3689 - mean_absolute_error: 29.6724\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 1480.4535 - mean_absolute_error: 26.5839\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 970.5079 - mean_absolute_error: 19.5903\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 738.3373 - mean_absolute_error: 18.0046\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 601.0792 - mean_absolute_error: 16.2161\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 497.5356 - mean_absolute_error: 15.2708\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 414.2019 - mean_absolute_error: 14.5210\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 327.9929 - mean_absolute_error: 12.4326\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 247.0396 - mean_absolute_error: 10.8432\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 392us/step - loss: 181.1888 - mean_absolute_error: 9.2711\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 129.3588 - mean_absolute_error: 7.9297\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 375us/step - loss: 95.0411 - mean_absolute_error: 6.5943\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 76.2379 - mean_absolute_error: 5.7563\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 63.6420 - mean_absolute_error: 5.2407\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 52.0171 - mean_absolute_error: 4.6286\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 46.0340 - mean_absolute_error: 4.3832\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 40.1823 - mean_absolute_error: 4.2107\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 33.5845 - mean_absolute_error: 3.6980\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 348us/step - loss: 28.1040 - mean_absolute_error: 3.4497\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 24.4996 - mean_absolute_error: 3.1547\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 21.4493 - mean_absolute_error: 2.9794\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 17.8279 - mean_absolute_error: 2.7199\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 14.7110 - mean_absolute_error: 2.5406\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 12.0992 - mean_absolute_error: 2.1870\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 345us/step - loss: 10.5865 - mean_absolute_error: 2.0897\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 8.8374 - mean_absolute_error: 1.8749\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 7.5278 - mean_absolute_error: 1.7851\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 6.6775 - mean_absolute_error: 1.6977\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 354us/step - loss: 5.6502 - mean_absolute_error: 1.5216\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 5.2787 - mean_absolute_error: 1.4322\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 392us/step - loss: 3.9309 - mean_absolute_error: 1.2413\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 399us/step - loss: 3.3377 - mean_absolute_error: 1.1377\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 2.7945 - mean_absolute_error: 1.0082\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 2.2399 - mean_absolute_error: 0.9215\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 347us/step - loss: 1.9990 - mean_absolute_error: 0.8815\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 1.9241 - mean_absolute_error: 0.8350\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 1.9232 - mean_absolute_error: 0.8059\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 347us/step - loss: 1.2885 - mean_absolute_error: 0.6941\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 1.0894 - mean_absolute_error: 0.6592\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 1.1890 - mean_absolute_error: 0.6288\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 0.9140 - mean_absolute_error: 0.5618\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 0.9331 - mean_absolute_error: 0.5652\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 1.0495 - mean_absolute_error: 0.5571\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 0.7974 - mean_absolute_error: 0.5232\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 387us/step - loss: 0.8819 - mean_absolute_error: 0.5214\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 1.1592 - mean_absolute_error: 0.5896\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 1.0678 - mean_absolute_error: 0.5840\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 1.3103 - mean_absolute_error: 0.5992\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 2.0047 - mean_absolute_error: 0.7087\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 1.9206 - mean_absolute_error: 0.7686\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 1.9212 - mean_absolute_error: 0.7466\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 1.7438 - mean_absolute_error: 0.7182\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 1.9613 - mean_absolute_error: 0.7864\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 3.6307 - mean_absolute_error: 1.0702\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 367us/step - loss: 1.9907 - mean_absolute_error: 0.9245\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 1.5392 - mean_absolute_error: 0.8232\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 1.5286 - mean_absolute_error: 0.7530\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 1.1431 - mean_absolute_error: 0.6178\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 1.1442 - mean_absolute_error: 0.6035\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 1.1356 - mean_absolute_error: 0.5301\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 398us/step - loss: 0.6601 - mean_absolute_error: 0.4512\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 1.8795 - mean_absolute_error: 0.7200\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 1.3867 - mean_absolute_error: 0.7626\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 1.2610 - mean_absolute_error: 0.7714\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 1.0529 - mean_absolute_error: 0.6416\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 1.2500 - mean_absolute_error: 0.6546\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 3.8570 - mean_absolute_error: 0.7414\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 1.6518 - mean_absolute_error: 0.7644\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 1.3604 - mean_absolute_error: 0.6685\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 1.3566 - mean_absolute_error: 0.7250\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 1.7043 - mean_absolute_error: 0.7492\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 1.4702 - mean_absolute_error: 0.7598\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 385us/step - loss: 1.4102 - mean_absolute_error: 0.7919\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 1.2624 - mean_absolute_error: 0.7038\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 409us/step - loss: 1.2862 - mean_absolute_error: 0.7111\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 1.1653 - mean_absolute_error: 0.6464\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 1.0122 - mean_absolute_error: 0.6355\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 390us/step - loss: 1.1933 - mean_absolute_error: 0.7083\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 398us/step - loss: 1.2122 - mean_absolute_error: 0.6854\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 394us/step - loss: 1.2322 - mean_absolute_error: 0.7198\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 393us/step - loss: 1.9400 - mean_absolute_error: 0.8339\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 397us/step - loss: 1.8246 - mean_absolute_error: 0.9429\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 400us/step - loss: 2.0240 - mean_absolute_error: 0.9699\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 1.9127 - mean_absolute_error: 0.8455\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 374us/step - loss: 2.8133 - mean_absolute_error: 1.0587\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 2.9931 - mean_absolute_error: 1.1156\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 2.5179 - mean_absolute_error: 1.0495\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 391us/step - loss: 2.2170 - mean_absolute_error: 0.9789\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 1.9401 - mean_absolute_error: 0.8494\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 387us/step - loss: 1.4819 - mean_absolute_error: 0.7635\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 1.1948 - mean_absolute_error: 0.7010\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 1.4872 - mean_absolute_error: 0.8233\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 1.7305 - mean_absolute_error: 0.8727\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 1.2262 - mean_absolute_error: 0.7624\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 0.9094 - mean_absolute_error: 0.6216\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 0.8455 - mean_absolute_error: 0.5942\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 0.8559 - mean_absolute_error: 0.6113\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 381us/step - loss: 0.9799 - mean_absolute_error: 0.6412\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 1.0836 - mean_absolute_error: 0.6614\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 1.0170 - mean_absolute_error: 0.6415\n",
            "36/36 [==============================] - 1s 32ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 9ms/step - loss: 1577.8811 - mean_absolute_error: 28.9624\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 1500.7613 - mean_absolute_error: 27.7483\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 1120.5496 - mean_absolute_error: 22.0269\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 706.1821 - mean_absolute_error: 17.1431\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 575.5136 - mean_absolute_error: 16.2122\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 371us/step - loss: 471.4923 - mean_absolute_error: 14.9088\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 351us/step - loss: 378.1787 - mean_absolute_error: 12.8922\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 282.4602 - mean_absolute_error: 11.5110\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 202.6164 - mean_absolute_error: 9.7807\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 343us/step - loss: 142.5146 - mean_absolute_error: 8.1193\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 97.2043 - mean_absolute_error: 6.7385\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 71.9725 - mean_absolute_error: 5.6428\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 57.4117 - mean_absolute_error: 5.1288\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 44.3921 - mean_absolute_error: 4.5026\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 36.1318 - mean_absolute_error: 3.9556\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 28.3427 - mean_absolute_error: 3.4539\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 344us/step - loss: 23.7332 - mean_absolute_error: 3.2686\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 319us/step - loss: 19.4301 - mean_absolute_error: 2.9557\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 15.5382 - mean_absolute_error: 2.5048\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 12.6025 - mean_absolute_error: 2.2799\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 9.7583 - mean_absolute_error: 1.9750\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 384us/step - loss: 7.9413 - mean_absolute_error: 1.7639\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 6.4584 - mean_absolute_error: 1.6176\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 5.5474 - mean_absolute_error: 1.4533\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 4.9471 - mean_absolute_error: 1.4133\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 383us/step - loss: 3.8656 - mean_absolute_error: 1.2106\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 391us/step - loss: 3.1318 - mean_absolute_error: 1.0776\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 429us/step - loss: 2.7143 - mean_absolute_error: 1.0146\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 396us/step - loss: 2.2767 - mean_absolute_error: 0.8851\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 415us/step - loss: 2.0957 - mean_absolute_error: 0.7983\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 2.0737 - mean_absolute_error: 0.7782\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 2.2119 - mean_absolute_error: 0.7960\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 395us/step - loss: 2.0351 - mean_absolute_error: 0.7865\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 387us/step - loss: 1.4893 - mean_absolute_error: 0.6740\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 1.2835 - mean_absolute_error: 0.5850\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 412us/step - loss: 1.2482 - mean_absolute_error: 0.5273\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 1.3370 - mean_absolute_error: 0.5181\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 1.3910 - mean_absolute_error: 0.5377\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 366us/step - loss: 1.2808 - mean_absolute_error: 0.5279\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 1.0709 - mean_absolute_error: 0.4818\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 385us/step - loss: 1.0585 - mean_absolute_error: 0.4687\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 362us/step - loss: 1.0984 - mean_absolute_error: 0.4645\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 0.9824 - mean_absolute_error: 0.3922\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 0.8485 - mean_absolute_error: 0.3430\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 377us/step - loss: 0.8876 - mean_absolute_error: 0.3504\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 0.9044 - mean_absolute_error: 0.3549\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 0.9426 - mean_absolute_error: 0.3684\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 1.0556 - mean_absolute_error: 0.3968\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 368us/step - loss: 1.2479 - mean_absolute_error: 0.4837\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 387us/step - loss: 1.5745 - mean_absolute_error: 0.5959\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 1.7628 - mean_absolute_error: 0.5997\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 403us/step - loss: 1.9361 - mean_absolute_error: 0.6570\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 411us/step - loss: 1.7146 - mean_absolute_error: 0.6426\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 375us/step - loss: 1.6499 - mean_absolute_error: 0.6672\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 1.5805 - mean_absolute_error: 0.7095\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 1.5499 - mean_absolute_error: 0.6937\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 384us/step - loss: 1.8510 - mean_absolute_error: 0.7587\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 347us/step - loss: 1.9972 - mean_absolute_error: 0.8192\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 2.2566 - mean_absolute_error: 0.8386\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 2.1495 - mean_absolute_error: 0.7678\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 384us/step - loss: 2.0196 - mean_absolute_error: 0.7450\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 2.1259 - mean_absolute_error: 0.7108\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 2.1361 - mean_absolute_error: 0.6802\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 364us/step - loss: 2.6860 - mean_absolute_error: 0.7993\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 3.2303 - mean_absolute_error: 1.0784\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 2.3425 - mean_absolute_error: 0.9224\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 381us/step - loss: 1.7864 - mean_absolute_error: 0.7999\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 389us/step - loss: 1.6988 - mean_absolute_error: 0.6916\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 386us/step - loss: 1.2650 - mean_absolute_error: 0.5508\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 388us/step - loss: 1.3522 - mean_absolute_error: 0.5748\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 381us/step - loss: 1.2782 - mean_absolute_error: 0.5555\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 374us/step - loss: 1.3387 - mean_absolute_error: 0.6060\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 372us/step - loss: 1.2930 - mean_absolute_error: 0.5847\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 1.3365 - mean_absolute_error: 0.5944\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 1.6298 - mean_absolute_error: 0.6373\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 1.9067 - mean_absolute_error: 0.6949\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 354us/step - loss: 2.1712 - mean_absolute_error: 0.8233\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 380us/step - loss: 2.1910 - mean_absolute_error: 0.8719\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 370us/step - loss: 2.0140 - mean_absolute_error: 0.8303\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 1.5598 - mean_absolute_error: 0.7278\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 2.2527 - mean_absolute_error: 0.8157\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 1.5824 - mean_absolute_error: 0.6950\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 339us/step - loss: 1.5445 - mean_absolute_error: 0.7136\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 2.0181 - mean_absolute_error: 0.7040\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 1.4499 - mean_absolute_error: 0.6489\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 389us/step - loss: 1.4571 - mean_absolute_error: 0.6230\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 374us/step - loss: 1.5286 - mean_absolute_error: 0.6697\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 352us/step - loss: 1.7810 - mean_absolute_error: 0.7219\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 2.8598 - mean_absolute_error: 1.0255\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 3.1160 - mean_absolute_error: 1.0531\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 2.6461 - mean_absolute_error: 1.0181\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 2.7269 - mean_absolute_error: 0.9970\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 2.9768 - mean_absolute_error: 1.1383\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 404us/step - loss: 3.1736 - mean_absolute_error: 1.1199\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 390us/step - loss: 4.0272 - mean_absolute_error: 1.2720\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 3.7145 - mean_absolute_error: 1.2641\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 2.9458 - mean_absolute_error: 1.1531\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 2.2692 - mean_absolute_error: 1.0010\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 2.1047 - mean_absolute_error: 0.8956\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 1.6018 - mean_absolute_error: 0.7529\n",
            "36/36 [==============================] - 1s 33ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 9ms/step - loss: 1579.2290 - mean_absolute_error: 28.4566\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 1481.1179 - mean_absolute_error: 26.8162\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 376us/step - loss: 1017.0831 - mean_absolute_error: 20.0581\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 734.6554 - mean_absolute_error: 17.4614\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 360us/step - loss: 596.5249 - mean_absolute_error: 15.4836\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 515.4239 - mean_absolute_error: 15.0519\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 373us/step - loss: 424.5388 - mean_absolute_error: 13.4546\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 394us/step - loss: 348.3506 - mean_absolute_error: 12.4496\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 382us/step - loss: 283.7057 - mean_absolute_error: 11.6007\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 413us/step - loss: 221.2334 - mean_absolute_error: 10.0342\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 401us/step - loss: 174.4872 - mean_absolute_error: 9.2653\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 397us/step - loss: 134.5782 - mean_absolute_error: 8.0458\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 413us/step - loss: 107.0300 - mean_absolute_error: 7.3113\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 415us/step - loss: 85.8902 - mean_absolute_error: 6.5014\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 458us/step - loss: 70.9345 - mean_absolute_error: 5.8858\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 432us/step - loss: 60.3951 - mean_absolute_error: 5.2873\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 440us/step - loss: 52.3407 - mean_absolute_error: 4.8586\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 462us/step - loss: 45.2373 - mean_absolute_error: 4.5045\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 444us/step - loss: 40.1113 - mean_absolute_error: 4.1727\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 438us/step - loss: 34.5989 - mean_absolute_error: 3.8653\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 429us/step - loss: 30.5849 - mean_absolute_error: 3.5554\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 444us/step - loss: 26.9164 - mean_absolute_error: 3.3449\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 463us/step - loss: 23.6829 - mean_absolute_error: 3.1108\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 440us/step - loss: 20.3006 - mean_absolute_error: 2.8482\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 430us/step - loss: 18.3473 - mean_absolute_error: 2.7038\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 421us/step - loss: 15.6452 - mean_absolute_error: 2.4354\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 469us/step - loss: 13.8573 - mean_absolute_error: 2.4305\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 433us/step - loss: 12.2706 - mean_absolute_error: 2.2319\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 455us/step - loss: 10.3668 - mean_absolute_error: 2.0226\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 439us/step - loss: 9.0135 - mean_absolute_error: 1.9147\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 433us/step - loss: 8.1765 - mean_absolute_error: 1.7725\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 437us/step - loss: 6.9488 - mean_absolute_error: 1.6625\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 416us/step - loss: 6.1402 - mean_absolute_error: 1.5044\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 421us/step - loss: 5.0964 - mean_absolute_error: 1.4080\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 447us/step - loss: 4.4715 - mean_absolute_error: 1.2975\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 456us/step - loss: 3.7893 - mean_absolute_error: 1.1721\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 469us/step - loss: 3.8042 - mean_absolute_error: 1.1684\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 495us/step - loss: 2.9429 - mean_absolute_error: 1.0564\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 469us/step - loss: 2.5086 - mean_absolute_error: 0.9651\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 487us/step - loss: 2.1377 - mean_absolute_error: 0.8800\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 567us/step - loss: 1.8760 - mean_absolute_error: 0.8295\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 599us/step - loss: 1.7399 - mean_absolute_error: 0.8044\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 623us/step - loss: 1.5322 - mean_absolute_error: 0.7403\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 658us/step - loss: 1.1608 - mean_absolute_error: 0.6402\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 641us/step - loss: 1.0409 - mean_absolute_error: 0.6027\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 641us/step - loss: 0.9594 - mean_absolute_error: 0.5640\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 649us/step - loss: 1.0299 - mean_absolute_error: 0.5924\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 577us/step - loss: 1.0467 - mean_absolute_error: 0.5822\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 597us/step - loss: 0.9432 - mean_absolute_error: 0.5517\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 618us/step - loss: 0.9064 - mean_absolute_error: 0.5052\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 628us/step - loss: 0.7271 - mean_absolute_error: 0.4784\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 605us/step - loss: 0.5807 - mean_absolute_error: 0.4586\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 617us/step - loss: 0.4590 - mean_absolute_error: 0.4008\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 629us/step - loss: 0.4192 - mean_absolute_error: 0.3639\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 626us/step - loss: 0.3936 - mean_absolute_error: 0.3430\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 658us/step - loss: 0.4452 - mean_absolute_error: 0.3829\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 597us/step - loss: 0.4837 - mean_absolute_error: 0.3648\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 577us/step - loss: 0.5219 - mean_absolute_error: 0.3507\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 572us/step - loss: 0.5381 - mean_absolute_error: 0.3614\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 565us/step - loss: 0.5428 - mean_absolute_error: 0.3545\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 554us/step - loss: 0.4429 - mean_absolute_error: 0.3291\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 603us/step - loss: 0.3590 - mean_absolute_error: 0.3014\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 587us/step - loss: 0.3343 - mean_absolute_error: 0.2837\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 636us/step - loss: 0.3302 - mean_absolute_error: 0.2699\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 625us/step - loss: 0.3731 - mean_absolute_error: 0.2956\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 610us/step - loss: 0.3600 - mean_absolute_error: 0.3184\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 613us/step - loss: 0.3802 - mean_absolute_error: 0.3040\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 580us/step - loss: 0.5183 - mean_absolute_error: 0.3430\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 587us/step - loss: 0.5260 - mean_absolute_error: 0.3434\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 573us/step - loss: 0.5136 - mean_absolute_error: 0.3289\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 564us/step - loss: 0.5081 - mean_absolute_error: 0.3355\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 443us/step - loss: 0.5041 - mean_absolute_error: 0.3267\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 387us/step - loss: 0.5570 - mean_absolute_error: 0.3548\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 0.6171 - mean_absolute_error: 0.3745\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 353us/step - loss: 0.8102 - mean_absolute_error: 0.4279\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 0.9942 - mean_absolute_error: 0.4504\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 0.6338 - mean_absolute_error: 0.4461\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 0.8499 - mean_absolute_error: 0.5873\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 334us/step - loss: 0.9752 - mean_absolute_error: 0.6012\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 1.0192 - mean_absolute_error: 0.5846\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 1.5983 - mean_absolute_error: 0.8086\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 1.7899 - mean_absolute_error: 0.8742\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 1.5131 - mean_absolute_error: 0.7376\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 312us/step - loss: 1.2312 - mean_absolute_error: 0.6326\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 0.9788 - mean_absolute_error: 0.5920\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 1.1629 - mean_absolute_error: 0.6390\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 1.5973 - mean_absolute_error: 0.7209\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 1.7608 - mean_absolute_error: 0.7511\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 1.4256 - mean_absolute_error: 0.7792\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 1.8781 - mean_absolute_error: 0.7787\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 321us/step - loss: 1.2422 - mean_absolute_error: 0.7703\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 1.2500 - mean_absolute_error: 0.7537\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 321us/step - loss: 0.8776 - mean_absolute_error: 0.6269\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 0.6205 - mean_absolute_error: 0.5063\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 0.7233 - mean_absolute_error: 0.5082\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 0.7229 - mean_absolute_error: 0.5332\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 321us/step - loss: 0.5031 - mean_absolute_error: 0.4532\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 356us/step - loss: 0.4242 - mean_absolute_error: 0.4138\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 321us/step - loss: 0.4073 - mean_absolute_error: 0.4054\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 345us/step - loss: 0.4357 - mean_absolute_error: 0.4063\n",
            "36/36 [==============================] - 1s 33ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 9ms/step - loss: 1545.1922 - mean_absolute_error: 28.8910\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 1372.7252 - mean_absolute_error: 26.4173\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 879.8508 - mean_absolute_error: 19.0459\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 323us/step - loss: 644.6684 - mean_absolute_error: 16.7295\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 337us/step - loss: 532.6121 - mean_absolute_error: 15.2060\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 443.9718 - mean_absolute_error: 13.8133\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 368.0036 - mean_absolute_error: 13.0808\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 300.2672 - mean_absolute_error: 11.6530\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 358us/step - loss: 236.8206 - mean_absolute_error: 10.3418\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 194.2177 - mean_absolute_error: 9.7106\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 143.9364 - mean_absolute_error: 8.3757\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 378us/step - loss: 115.6081 - mean_absolute_error: 7.5532\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 92.2920 - mean_absolute_error: 6.6965\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 75.1448 - mean_absolute_error: 5.9708\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 64.6153 - mean_absolute_error: 5.4509\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 55.1423 - mean_absolute_error: 5.0320\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 48.2479 - mean_absolute_error: 4.7415\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 323us/step - loss: 41.1483 - mean_absolute_error: 4.2414\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 338us/step - loss: 35.9463 - mean_absolute_error: 3.9786\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 32.0335 - mean_absolute_error: 3.6770\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 363us/step - loss: 28.0396 - mean_absolute_error: 3.4332\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 24.3405 - mean_absolute_error: 3.2379\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 20.7688 - mean_absolute_error: 3.0053\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 18.1023 - mean_absolute_error: 2.7474\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 15.9092 - mean_absolute_error: 2.5607\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 13.9857 - mean_absolute_error: 2.4539\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 369us/step - loss: 11.7955 - mean_absolute_error: 2.1817\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 361us/step - loss: 10.4159 - mean_absolute_error: 2.1193\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 9.0788 - mean_absolute_error: 1.9215\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 7.7547 - mean_absolute_error: 1.7860\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 6.8523 - mean_absolute_error: 1.6661\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 5.5555 - mean_absolute_error: 1.4987\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 319us/step - loss: 5.3867 - mean_absolute_error: 1.5021\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 4.2529 - mean_absolute_error: 1.3183\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 3.7869 - mean_absolute_error: 1.2391\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 3.2023 - mean_absolute_error: 1.1484\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 2.6628 - mean_absolute_error: 1.0274\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 2.3845 - mean_absolute_error: 0.9803\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 1.9794 - mean_absolute_error: 0.8947\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 322us/step - loss: 1.7669 - mean_absolute_error: 0.8399\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 303us/step - loss: 1.5826 - mean_absolute_error: 0.7600\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 1.5285 - mean_absolute_error: 0.7448\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 1.3012 - mean_absolute_error: 0.7048\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 1.1494 - mean_absolute_error: 0.6995\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 0.9719 - mean_absolute_error: 0.6303\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 0.8764 - mean_absolute_error: 0.5814\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 0.7255 - mean_absolute_error: 0.5254\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 0.6941 - mean_absolute_error: 0.5338\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 0.6064 - mean_absolute_error: 0.4614\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 309us/step - loss: 0.5790 - mean_absolute_error: 0.4060\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 0.5402 - mean_absolute_error: 0.4082\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 0.5755 - mean_absolute_error: 0.4297\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 0.5181 - mean_absolute_error: 0.3886\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 0.5156 - mean_absolute_error: 0.3930\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 322us/step - loss: 0.6683 - mean_absolute_error: 0.4436\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 0.6270 - mean_absolute_error: 0.4507\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 304us/step - loss: 0.6502 - mean_absolute_error: 0.4599\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 0.7568 - mean_absolute_error: 0.4563\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 306us/step - loss: 0.9767 - mean_absolute_error: 0.4778\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 339us/step - loss: 1.0996 - mean_absolute_error: 0.5900\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 309us/step - loss: 1.2783 - mean_absolute_error: 0.5784\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 1.5212 - mean_absolute_error: 0.6786\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 312us/step - loss: 0.6486 - mean_absolute_error: 0.5310\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 310us/step - loss: 0.4973 - mean_absolute_error: 0.4507\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 0.3948 - mean_absolute_error: 0.3952\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 0.3233 - mean_absolute_error: 0.3321\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 295us/step - loss: 0.1749 - mean_absolute_error: 0.2610\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 290us/step - loss: 0.2098 - mean_absolute_error: 0.2556\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 0.2592 - mean_absolute_error: 0.3430\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 0.2642 - mean_absolute_error: 0.3254\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 301us/step - loss: 0.2388 - mean_absolute_error: 0.3072\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 307us/step - loss: 0.2028 - mean_absolute_error: 0.2645\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 0.2418 - mean_absolute_error: 0.2526\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 0.3702 - mean_absolute_error: 0.3186\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 0.6896 - mean_absolute_error: 0.4564\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 319us/step - loss: 1.1163 - mean_absolute_error: 0.5877\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 313us/step - loss: 1.2187 - mean_absolute_error: 0.6203\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 1.3084 - mean_absolute_error: 0.7095\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 1.2542 - mean_absolute_error: 0.7068\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 0.9873 - mean_absolute_error: 0.5918\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 303us/step - loss: 0.8193 - mean_absolute_error: 0.5753\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 1.1898 - mean_absolute_error: 0.6140\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 1.3852 - mean_absolute_error: 0.6382\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 1.5032 - mean_absolute_error: 0.7501\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 338us/step - loss: 1.5325 - mean_absolute_error: 0.7653\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 309us/step - loss: 1.6920 - mean_absolute_error: 0.8046\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 355us/step - loss: 1.8203 - mean_absolute_error: 0.8647\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 302us/step - loss: 1.8778 - mean_absolute_error: 0.8487\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 1.4782 - mean_absolute_error: 0.7480\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 322us/step - loss: 1.1130 - mean_absolute_error: 0.6822\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 310us/step - loss: 0.8335 - mean_absolute_error: 0.5950\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 309us/step - loss: 0.8533 - mean_absolute_error: 0.5701\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 1.1058 - mean_absolute_error: 0.6416\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 1.0555 - mean_absolute_error: 0.5974\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 319us/step - loss: 1.0539 - mean_absolute_error: 0.6324\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 1.0841 - mean_absolute_error: 0.6235\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 1.1844 - mean_absolute_error: 0.6988\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 319us/step - loss: 1.2678 - mean_absolute_error: 0.7104\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 1.4023 - mean_absolute_error: 0.7304\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 1.7814 - mean_absolute_error: 0.8871\n",
            "36/36 [==============================] - 1s 33ms/step\n",
            "Epoch 1/100\n",
            "325/325 [==============================] - 3s 9ms/step - loss: 1680.7270 - mean_absolute_error: 29.2291\n",
            "Epoch 2/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 1567.8678 - mean_absolute_error: 27.4782\n",
            "Epoch 3/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 1158.7478 - mean_absolute_error: 21.6310\n",
            "Epoch 4/100\n",
            "325/325 [==============================] - 0s 310us/step - loss: 764.8485 - mean_absolute_error: 17.3700\n",
            "Epoch 5/100\n",
            "325/325 [==============================] - 0s 323us/step - loss: 627.8714 - mean_absolute_error: 16.5058\n",
            "Epoch 6/100\n",
            "325/325 [==============================] - 0s 357us/step - loss: 532.8459 - mean_absolute_error: 15.1016\n",
            "Epoch 7/100\n",
            "325/325 [==============================] - 0s 347us/step - loss: 448.4141 - mean_absolute_error: 14.4503\n",
            "Epoch 8/100\n",
            "325/325 [==============================] - 0s 350us/step - loss: 370.6590 - mean_absolute_error: 13.0903\n",
            "Epoch 9/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 305.5959 - mean_absolute_error: 11.9194\n",
            "Epoch 10/100\n",
            "325/325 [==============================] - 0s 329us/step - loss: 249.1942 - mean_absolute_error: 11.1447\n",
            "Epoch 11/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 195.1751 - mean_absolute_error: 9.8550\n",
            "Epoch 12/100\n",
            "325/325 [==============================] - 0s 328us/step - loss: 152.6161 - mean_absolute_error: 8.4815\n",
            "Epoch 13/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 117.2327 - mean_absolute_error: 7.6421\n",
            "Epoch 14/100\n",
            "325/325 [==============================] - 0s 321us/step - loss: 94.4447 - mean_absolute_error: 6.8368\n",
            "Epoch 15/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 76.6134 - mean_absolute_error: 5.9606\n",
            "Epoch 16/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 65.8407 - mean_absolute_error: 5.4844\n",
            "Epoch 17/100\n",
            "325/325 [==============================] - 0s 339us/step - loss: 55.1573 - mean_absolute_error: 4.9482\n",
            "Epoch 18/100\n",
            "325/325 [==============================] - 0s 331us/step - loss: 47.7347 - mean_absolute_error: 4.5274\n",
            "Epoch 19/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 42.1191 - mean_absolute_error: 4.1602\n",
            "Epoch 20/100\n",
            "325/325 [==============================] - 0s 308us/step - loss: 36.4389 - mean_absolute_error: 3.9374\n",
            "Epoch 21/100\n",
            "325/325 [==============================] - 0s 307us/step - loss: 31.0272 - mean_absolute_error: 3.5559\n",
            "Epoch 22/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 28.3671 - mean_absolute_error: 3.3859\n",
            "Epoch 23/100\n",
            "325/325 [==============================] - 0s 305us/step - loss: 24.2973 - mean_absolute_error: 3.1100\n",
            "Epoch 24/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 19.9532 - mean_absolute_error: 2.7991\n",
            "Epoch 25/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 17.8446 - mean_absolute_error: 2.5912\n",
            "Epoch 26/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 14.7652 - mean_absolute_error: 2.3809\n",
            "Epoch 27/100\n",
            "325/325 [==============================] - 0s 318us/step - loss: 13.0800 - mean_absolute_error: 2.2073\n",
            "Epoch 28/100\n",
            "325/325 [==============================] - 0s 310us/step - loss: 11.2967 - mean_absolute_error: 2.0818\n",
            "Epoch 29/100\n",
            "325/325 [==============================] - 0s 336us/step - loss: 9.6518 - mean_absolute_error: 1.8548\n",
            "Epoch 30/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 8.4805 - mean_absolute_error: 1.7339\n",
            "Epoch 31/100\n",
            "325/325 [==============================] - 0s 326us/step - loss: 7.2318 - mean_absolute_error: 1.5840\n",
            "Epoch 32/100\n",
            "325/325 [==============================] - 0s 319us/step - loss: 6.2037 - mean_absolute_error: 1.4166\n",
            "Epoch 33/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 5.4335 - mean_absolute_error: 1.3587\n",
            "Epoch 34/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 4.5848 - mean_absolute_error: 1.2426\n",
            "Epoch 35/100\n",
            "325/325 [==============================] - 0s 333us/step - loss: 3.9859 - mean_absolute_error: 1.1683\n",
            "Epoch 36/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 3.5011 - mean_absolute_error: 1.1063\n",
            "Epoch 37/100\n",
            "325/325 [==============================] - 0s 322us/step - loss: 2.8031 - mean_absolute_error: 0.9801\n",
            "Epoch 38/100\n",
            "325/325 [==============================] - 0s 309us/step - loss: 2.3711 - mean_absolute_error: 0.9069\n",
            "Epoch 39/100\n",
            "325/325 [==============================] - 0s 335us/step - loss: 2.0119 - mean_absolute_error: 0.8199\n",
            "Epoch 40/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 1.8104 - mean_absolute_error: 0.8184\n",
            "Epoch 41/100\n",
            "325/325 [==============================] - 0s 309us/step - loss: 1.5957 - mean_absolute_error: 0.7541\n",
            "Epoch 42/100\n",
            "325/325 [==============================] - 0s 305us/step - loss: 1.3168 - mean_absolute_error: 0.6774\n",
            "Epoch 43/100\n",
            "325/325 [==============================] - 0s 322us/step - loss: 1.1521 - mean_absolute_error: 0.6528\n",
            "Epoch 44/100\n",
            "325/325 [==============================] - 0s 309us/step - loss: 0.9770 - mean_absolute_error: 0.6198\n",
            "Epoch 45/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 0.7738 - mean_absolute_error: 0.5210\n",
            "Epoch 46/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 0.6816 - mean_absolute_error: 0.4744\n",
            "Epoch 47/100\n",
            "325/325 [==============================] - 0s 323us/step - loss: 0.6100 - mean_absolute_error: 0.4529\n",
            "Epoch 48/100\n",
            "325/325 [==============================] - 0s 322us/step - loss: 0.5640 - mean_absolute_error: 0.4274\n",
            "Epoch 49/100\n",
            "325/325 [==============================] - 0s 365us/step - loss: 0.5198 - mean_absolute_error: 0.3977\n",
            "Epoch 50/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 0.6819 - mean_absolute_error: 0.4130\n",
            "Epoch 51/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 0.7949 - mean_absolute_error: 0.4279\n",
            "Epoch 52/100\n",
            "325/325 [==============================] - 0s 310us/step - loss: 0.6407 - mean_absolute_error: 0.4630\n",
            "Epoch 53/100\n",
            "325/325 [==============================] - 0s 340us/step - loss: 0.5832 - mean_absolute_error: 0.4088\n",
            "Epoch 54/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 0.6309 - mean_absolute_error: 0.4009\n",
            "Epoch 55/100\n",
            "325/325 [==============================] - 0s 341us/step - loss: 0.6699 - mean_absolute_error: 0.4128\n",
            "Epoch 56/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 0.8183 - mean_absolute_error: 0.4569\n",
            "Epoch 57/100\n",
            "325/325 [==============================] - 0s 308us/step - loss: 0.8326 - mean_absolute_error: 0.4720\n",
            "Epoch 58/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 0.8398 - mean_absolute_error: 0.4750\n",
            "Epoch 59/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 0.8951 - mean_absolute_error: 0.5331\n",
            "Epoch 60/100\n",
            "325/325 [==============================] - 0s 311us/step - loss: 1.3554 - mean_absolute_error: 0.6423\n",
            "Epoch 61/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 0.9393 - mean_absolute_error: 0.5276\n",
            "Epoch 62/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 0.9337 - mean_absolute_error: 0.5109\n",
            "Epoch 63/100\n",
            "325/325 [==============================] - 0s 321us/step - loss: 0.8746 - mean_absolute_error: 0.4613\n",
            "Epoch 64/100\n",
            "325/325 [==============================] - 0s 313us/step - loss: 0.7722 - mean_absolute_error: 0.4326\n",
            "Epoch 65/100\n",
            "325/325 [==============================] - 0s 317us/step - loss: 1.8099 - mean_absolute_error: 0.5026\n",
            "Epoch 66/100\n",
            "325/325 [==============================] - 0s 304us/step - loss: 0.5314 - mean_absolute_error: 0.4105\n",
            "Epoch 67/100\n",
            "325/325 [==============================] - 0s 305us/step - loss: 0.3789 - mean_absolute_error: 0.3533\n",
            "Epoch 68/100\n",
            "325/325 [==============================] - 0s 307us/step - loss: 0.3254 - mean_absolute_error: 0.3033\n",
            "Epoch 69/100\n",
            "325/325 [==============================] - 0s 308us/step - loss: 0.3020 - mean_absolute_error: 0.2571\n",
            "Epoch 70/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 0.3310 - mean_absolute_error: 0.2599\n",
            "Epoch 71/100\n",
            "325/325 [==============================] - 0s 299us/step - loss: 0.4033 - mean_absolute_error: 0.2981\n",
            "Epoch 72/100\n",
            "325/325 [==============================] - 0s 313us/step - loss: 0.4319 - mean_absolute_error: 0.3145\n",
            "Epoch 73/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 0.4444 - mean_absolute_error: 0.3372\n",
            "Epoch 74/100\n",
            "325/325 [==============================] - 0s 338us/step - loss: 0.4986 - mean_absolute_error: 0.3621\n",
            "Epoch 75/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 0.5322 - mean_absolute_error: 0.4253\n",
            "Epoch 76/100\n",
            "325/325 [==============================] - 0s 320us/step - loss: 0.6601 - mean_absolute_error: 0.4498\n",
            "Epoch 77/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 0.8155 - mean_absolute_error: 0.5083\n",
            "Epoch 78/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 0.7665 - mean_absolute_error: 0.5037\n",
            "Epoch 79/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 0.6500 - mean_absolute_error: 0.4229\n",
            "Epoch 80/100\n",
            "325/325 [==============================] - 0s 325us/step - loss: 0.8019 - mean_absolute_error: 0.4752\n",
            "Epoch 81/100\n",
            "325/325 [==============================] - 0s 315us/step - loss: 0.9706 - mean_absolute_error: 0.5372\n",
            "Epoch 82/100\n",
            "325/325 [==============================] - 0s 327us/step - loss: 0.9579 - mean_absolute_error: 0.5345\n",
            "Epoch 83/100\n",
            "325/325 [==============================] - 0s 330us/step - loss: 1.0025 - mean_absolute_error: 0.6002\n",
            "Epoch 84/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 1.1507 - mean_absolute_error: 0.6542\n",
            "Epoch 85/100\n",
            "325/325 [==============================] - 0s 313us/step - loss: 1.2901 - mean_absolute_error: 0.7192\n",
            "Epoch 86/100\n",
            "325/325 [==============================] - 0s 302us/step - loss: 1.0093 - mean_absolute_error: 0.5896\n",
            "Epoch 87/100\n",
            "325/325 [==============================] - 0s 340us/step - loss: 0.7770 - mean_absolute_error: 0.5453\n",
            "Epoch 88/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 0.9375 - mean_absolute_error: 0.5805\n",
            "Epoch 89/100\n",
            "325/325 [==============================] - 0s 307us/step - loss: 0.9506 - mean_absolute_error: 0.6065\n",
            "Epoch 90/100\n",
            "325/325 [==============================] - 0s 297us/step - loss: 0.7456 - mean_absolute_error: 0.5458\n",
            "Epoch 91/100\n",
            "325/325 [==============================] - 0s 312us/step - loss: 0.8928 - mean_absolute_error: 0.5613\n",
            "Epoch 92/100\n",
            "325/325 [==============================] - 0s 359us/step - loss: 0.8450 - mean_absolute_error: 0.5051\n",
            "Epoch 93/100\n",
            "325/325 [==============================] - 0s 316us/step - loss: 0.6997 - mean_absolute_error: 0.4841\n",
            "Epoch 94/100\n",
            "325/325 [==============================] - 0s 314us/step - loss: 0.6728 - mean_absolute_error: 0.4632\n",
            "Epoch 95/100\n",
            "325/325 [==============================] - 0s 308us/step - loss: 0.9434 - mean_absolute_error: 0.5846\n",
            "Epoch 96/100\n",
            "325/325 [==============================] - 0s 308us/step - loss: 0.8942 - mean_absolute_error: 0.5867\n",
            "Epoch 97/100\n",
            "325/325 [==============================] - 0s 319us/step - loss: 0.7502 - mean_absolute_error: 0.5374\n",
            "Epoch 98/100\n",
            "325/325 [==============================] - 0s 332us/step - loss: 0.7631 - mean_absolute_error: 0.5132\n",
            "Epoch 99/100\n",
            "325/325 [==============================] - 0s 324us/step - loss: 0.8473 - mean_absolute_error: 0.5005\n",
            "Epoch 100/100\n",
            "325/325 [==============================] - 0s 379us/step - loss: 0.9840 - mean_absolute_error: 0.5546\n",
            "36/36 [==============================] - 1s 35ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TaLPgvY6CLId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1662b933-d531-4121-dddb-aa61c3188a66"
      },
      "cell_type": "code",
      "source": [
        "print(accuracies)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-230.45125889 -222.92471059 -447.58772532 -574.98241001 -203.21721564\n",
            " -240.67563883 -553.83569845 -787.60904948 -598.80497233 -392.23950577]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QMryyxG-9bEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c786804-deb6-441e-dd4e-bea7254c6020"
      },
      "cell_type": "code",
      "source": [
        "print(mean)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-425.2328185313457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EZdjooMMR8cj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "3722016f-73a2-431d-ea0a-0be41ca98170"
      },
      "cell_type": "code",
      "source": [
        "regressor.fit(X_train,Y_train)\n",
        "predictions = regressor.predict(X_test)\n",
        "from sklearn.metrics import r2_score\n",
        "accuracy=r2_score(Y_test,predictions)\n",
        "print('test')\n",
        "print(accuracy)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-f9446af090ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 **self.filter_sk_params(self.build_fn.__call__))\n\u001b[1;32m    140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mloss_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-159-c5c478ae64d7>\u001b[0m in \u001b[0;36mbuild_regressor\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not iterable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uhiaBSVDSFLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}