{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network Regression LUSC on 200 Features.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zayeem00/MLProject_TeessideUni/blob/master/Neural_Network_Regression_LUSC_on_200_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mGX3ItvXTSC1",
        "colab_type": "code",
        "outputId": "1efe6d26-e2e3-450d-cc68-30e48cf62fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "meG1MtgTV4_G",
        "colab_type": "code",
        "outputId": "c46512d0-f75e-4079-a3fc-f8175f8c88d3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9857ff4e-71bf-4de5-9de0-8baab4d4ac52\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9857ff4e-71bf-4de5-9de0-8baab4d4ac52\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving survival_LUSC.xlsx to survival_LUSC.xlsx\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'survival_LUSC.xlsx': b'PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00b\\xee\\x9dh^\\x01\\x00\\x00\\x90\\x04\\x00\\x00\\x13\\x00\\x08\\x02[Content_Types].xml \\xa2\\x04\\x02(\\xa0\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x94\\xcbN\\xc30\\x10E\\xf7H\\xfcC\\xe4-J\\xdc\\xb2@\\x085\\xed\\x82\\xc7\\x12*Q>\\xc0\\xc4\\x93\\xc6\\xaac[\\x9eii\\xff\\x9e\\x89\\xfb\\x10B\\xa1\\x15j7\\xb1\\x12\\xcf\\xdc{2\\xf1\\xcdh\\xb2nm\\xb6\\x82\\x88\\xc6\\xbbR\\x0c\\x8b\\x81\\xc8\\xc0U^\\x1b7/\\xc5\\xc7\\xec%\\xbf\\x17\\x19\\x92rZY\\xef\\xa0\\x14\\x1b@1\\x19__\\x8df\\x9b\\x00\\x98q\\xb7\\xc3R4D\\xe1AJ\\xac\\x1ah\\x15\\x16>\\x80\\xe3\\x9d\\xda\\xc7V\\x11\\xdf\\xc6\\xb9\\x0c\\xaaZ\\xa89\\xc8\\xdb\\xc1\\xe0NV\\xde\\x118\\xca\\xa9\\xd3\\x10\\xe3\\xd1\\x13\\xd4ji){^\\xf3\\xe3-I\\x04\\x8b\"{\\xdc\\x16v^\\xa5P!XS)bR\\xb9r\\xfa\\x97K\\xbes(\\xb83\\xd5`c\\x02\\xde0\\x86\\x90\\xbd\\x0e\\xdd\\xce\\xdf\\x06\\xbb\\xbe7\\x1eM4\\x1a\\xb2\\xa9\\x8a\\xf4\\xaaZ\\xc6\\x90k+\\xbf|\\\\|z\\xbf(\\x8e\\x8b\\xf4P\\xfa\\xba6\\x15h_-[\\x9e@\\x81!\\x82\\xd2\\xd8\\x00Pk\\x8b\\xb4\\x16\\xad2n\\xcf}\\xc4?\\x15\\xa3L\\xcb\\xf0\\xc2 \\xdd\\xfb%\\xe1\\x13\\x1c\\xc4\\xdf\\x1bd\\xba\\x9e\\x8f\\x90dN\\x18\"m,\\xe0\\xa5\\xc7\\x9eDO97*\\x82~\\xa7\\xc8\\xc9\\xb88\\xc0O\\xedc\\x1c|n\\xa6\\xd1\\x07\\xe4\\x04E\\xf8\\xff\\x14\\xf6\\x11\\xe9\\xba\\xf3\\xc0B\\x10\\xc9\\xc0!$}\\x87\\xed\\xe0\\xc8\\xe9;{\\xec\\xd0\\xe5[\\x83\\xee\\xf1\\x96\\xe9\\x7f2\\xfe\\x06\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xb5U0#\\xf4\\x00\\x00\\x00L\\x02\\x00\\x00\\x0b\\x00\\x08\\x02_rels/.rels \\xa2\\x04\\x02(\\xa0\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x92MO\\xc30\\x0c\\x86\\xefH\\xfc\\x87\\xc8\\xf7\\xd5\\xdd\\x90\\x10BKwAH\\xbb!T~\\x80I\\xdc\\x0f\\xb5\\x8d\\xa3$\\x1b\\xdd\\xbf\\'\\x1c\\x10T\\x1a\\x83\\x03G\\x7f\\xbd~\\xfc\\xca\\xdb\\xdd<\\x8d\\xea\\xc8!\\xf6\\xe24\\xac\\x8b\\x12\\x14;#\\xb6w\\xad\\x86\\x97\\xfaqu\\x07*&r\\x96Fq\\xac\\xe1\\xc4\\x11v\\xd5\\xf5\\xd5\\xf6\\x99GJy(v\\xbd\\x8f*\\xab\\xb8\\xa8\\xa1K\\xc9\\xdf#F\\xd3\\xf1D\\xb1\\x10\\xcf.W\\x1a\\t\\x13\\xa5\\x1c\\x86\\x16=\\x99\\x81Z\\xc6MY\\xdeb\\xf8\\xae\\x01\\xd5BS\\xed\\xad\\x86\\xb0\\xb77\\xa0\\xea\\x93\\xcf\\x9b\\x7f\\xd7\\x96\\xa6\\xe9\\r?\\x889L\\xec\\xd2\\x99\\x15\\xc8sbg\\xd9\\xae|\\xc8l!\\xf5\\xf9\\x1aUSh9i\\xb0b\\x9er:\"y_dl\\xc0\\xf3D\\x9b\\xbf\\x13\\xfd|-N\\x9c\\xc8R\"4\\x12\\xf82\\xcfG\\xc7%\\xa0\\xf5\\x7fZ\\xb44\\xf1\\xcb\\x9dy\\xc47\\t\\xc3\\xab\\xc8\\xf0\\xc9\\x82\\x8b\\x1f\\xa8\\xde\\x01\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\x81>\\x94\\x97\\xf3\\x00\\x00\\x00\\xba\\x02\\x00\\x00\\x1a\\x00\\x08\\x01xl/_rels/workbook.xml.rels \\xa2\\x04\\x01(\\xa0\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xacRMK\\xc40\\x10\\xbd\\x0b\\xfe\\x870w\\x9bv\\x15\\x11\\xd9t/\"\\xecU\\xeb\\x0f\\x08\\xc9\\xb4)\\xdb&!3~\\xf4\\xdf\\x1b*\\xba]X\\xd6K/\\x03o\\x86y\\xef\\xcd\\xc7v\\xf75\\x0e\\xe2\\x03\\x13\\xf5\\xc1+\\xa8\\x8a\\x12\\x04z\\x13l\\xef;\\x05o\\xcd\\xf3\\xcd\\x03\\x08b\\xed\\xad\\x1e\\x82G\\x05\\x13\\x12\\xec\\xea\\xeb\\xab\\xed\\x0b\\x0e\\x9as\\x13\\xb9>\\x92\\xc8,\\x9e\\x148\\xe6\\xf8(%\\x19\\x87\\xa3\\xa6\"D\\xf4\\xb9\\xd2\\x864j\\xce0u2js\\xd0\\x1d\\xcaMY\\xde\\xcb\\xb4\\xe4\\x80\\xfa\\x84S\\xec\\xad\\x82\\xb4\\xb7\\xb7 \\x9a)f\\xe5\\xff\\xb9C\\xdb\\xf6\\x06\\x9f\\x82y\\x1f\\xd1\\xf3\\x19\\tI<\\ry\\x00\\xd1\\xe8\\xd4!+\\xf8\\xc1E\\xf6\\x08\\xf2\\xbc\\xfcfMy\\xcek\\xc1\\xa3\\xfa\\x0c\\xe5\\x1c\\xabK\\x1e\\xaa5=|\\x86t \\x87\\xc8G\\x1f\\x7f)\\x92s\\xe5\\xa2\\x99\\xbbU\\xef\\xe1tB\\xfb\\xca)\\xbf\\xdb\\xf2,\\xcb\\xf4\\xeff\\xe4\\xc9\\xc7\\xd5\\xdf\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00s\\xdf\\x1f\\xde-\\x02\\x00\\x00\\x82\\x04\\x00\\x00\\x0f\\x00\\x00\\x00xl/workbook.xml\\xacTM\\x8f\\xd30\\x10\\xbd#\\xf1\\x1f,\\xdf\\xdb|4\\xed\\xb6Q\\xd3\\xd5\\xf6\\x03\\xb1\\x12B\\xd5Rv/{q\\x9dIc\\xea\\xd8\\xc1vh+\\xc4\\x7fg\\x9cP(\\xf4\\xb2\\x08.\\x1e{l\\xbf\\x99yo\\xec\\xe9\\xed\\xb1\\x92\\xe4\\x0b\\x18+\\xb4\\xcah\\xd4\\x0f)\\x01\\xc5u.\\xd4.\\xa3\\x1f7ozcJ\\xacc*gR+\\xc8\\xe8\\t,\\xbd\\x9d\\xbd~5=h\\xb3\\xdfj\\xbd\\'\\x08\\xa0lFK\\xe7\\xea4\\x08,/\\xa1b\\xb6\\xafkP\\xb8ShS1\\x87K\\xb3\\x0blm\\x80\\xe5\\xb6\\x04p\\x95\\x0c\\xe20\\x1c\\x05\\x15\\x13\\x8av\\x08\\xa9y\\t\\x86.\\n\\xc1a\\xa9yS\\x81r\\x1d\\x88\\x01\\xc9\\x1c\\xa6oKQ\\xdb3Z\\xc5_\\x02W1\\xb3o\\xea\\x1e\\xd7U\\x8d\\x10[!\\x85;\\xb5\\xa0\\x94T<\\xbd\\xdf)m\\xd8Vb\\xd9\\xc7hxF\\xc6\\xe9\\x15t%\\xb8\\xd1V\\x17\\xae\\x8fPA\\x97\\xe4U\\xbdQ\\x18DQW\\xf2lZ\\x08\\t\\x8f\\x1d\\xed\\x84\\xd5\\xf5{V\\xf9(\\x92\\x12\\xc9\\xac[\\xe5\\xc2A\\x9e\\xd1\\x11.\\xf5\\x01~s\\x98\\xa6\\x9e7B\\xe2n\\x94$qH\\x83\\xd9O)\\xd6\\x86\\xe4P\\xb0F\\xba\\r\\x8ap\\x86\\xc7\\x83\\xa3$\\x8c\"\\x7f\\x12\\x8b\\xba\\x93\\x0e\\x8cb\\x0e\\x16Z9\\xe4\\xf0\\x07\\xfb\\xff\\xcaW\\x8b\\xbd(5\\xaaC\\x1e\\xe0s#\\x0c`Sx\\xdafS\\x1c\\x19O\\xd9\\xd6\\xae\\x99+Ic$vV\\xfa\\xbc6\\xfa\\x13p\\xb7d\\x8eYp\\xcf\\x17\\xf4\\xb2k\\xed\\xfe\\x82`\\xc6}\\xa5\\x01\\x96\\xda\\xa5\\xd3\\xcd\\xff,{6\\xf5\\xcd\\xfb(\\xe0`\\x7f\\x11\\xe8\\x97\\xe4\\xf8$T\\xae\\x0f\\x19\\xc5\\xa7p\\xba\\x98\\x1fZ\\xf7\\x93\\xc8]\\x99\\xd1x<\\x0eq\\xbf\\xf3\\xbd\\x05\\xb1+\\x1d\\xf2\\x1c\\x0f\\xd0\\xe9\\x83_`\\xb7\\xfd\\x8e1ZKT\\xab\\xf3\\x07\\xff\\x06\"|X\\xde\\xde{))1\\xa9\\xc0\\x89\\xb9\\xcf[\\xa1\\x82\\xf35\\xce$G]\\xbdi\\x0f\\x8e\\xe2I4\\xf01\\xe0\\xe8\\xdeY\\xd7Z\\xa4Td\\xf4k\\x94\\x84w7\\xe1$\\xe9\\x85\\xab\\xc1\\xb0\\x97\\x8c\\'qo\\x9c\\x0c\\xe2\\xde\"Y\\xc6\\xab\\xe1\\xcdj\\xb9\\x9a\\x0f\\xbf\\xfd\\xdf.Fe\\xd3\\xf3G\\xe0\\xb3,\\x99q\\x1b\\xc3\\xf8\\x1e\\xbf\\x8f\\x07(\\xe6\\xa8\\xac/\\xceS\\x82\\xf9vc\\x9bup\\xbe5\\xfb\\x0e\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xeb\\x1b\\xc7=\\xa5\\x00\\x00\\x00\\xc0\\x00\\x00\\x00\\x14\\x00\\x00\\x00xl/sharedStrings.xml4\\x8eA\\n\\xc20\\x10E\\xf7\\x82w\\x08\\xb3\\xb7\\xa9.D$M\\x17\\x82[\\x17\\xea\\x01B;\\xda@2\\xa9\\x99I\\xd1\\xdb\\x1b\\x17.\\x1f\\x8f\\xcf\\x7f\\xa6\\x7f\\xc7\\xa0\\x16\\xcc\\xec\\x13u\\xb0mZPHC\\x1a==;\\xb8\\xdf\\xce\\x9b\\x03(\\x16G\\xa3\\x0b\\x89\\xb0\\x83\\x0f2\\xf4v\\xbd2\\xcc\\xa2\\xea\\x96\\xb8\\x83Id>j\\xcd\\xc3\\x84\\xd1q\\x93f\\xa4j\\x1e)G\\'\\x15\\xf3S\\xf3\\x9c\\xd1\\x8d<!J\\x0cz\\xd7\\xb6{\\x1d\\x9d\\'PC*$\\xf5\\x17T!\\xff*x\\xfa\\xb35\\xec\\xad\\x11{\\xa9m.\\x04u-y\\xf1\\x8b\\x0bF\\x8b5\\xfa\\xe7t-\\xb0_\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00u>\\x99i\\x93\\x06\\x00\\x00\\x8c\\x1a\\x00\\x00\\x13\\x00\\x00\\x00xl/theme/theme1.xml\\xecY[\\x8b\\xdbF\\x14~/\\xf4?\\x08\\xbd;\\xbeI\\xb2\\xbd\\xc4\\x1bl\\xd9N\\xda\\xec&!\\xeb\\xa4\\xe4ql\\x8f\\xad\\xc9\\x8e4F3\\xde\\x8d\\t\\x81\\x92<\\xf5\\xa5PHK_\\n}\\xebC)\\r4\\xd0\\xd0\\x97\\xfe\\x98\\x85\\x846\\xfd\\x11=3\\x92\\xad\\x99\\xf58\\x9b\\xcb\\xa6\\xb4%kX\\xa4\\xd1w\\xce|s\\xce\\xd17\\x17]\\xbct/\\xa6\\xce\\x11N9aI\\xdb\\xad^\\xa8\\xb8\\x0eN\\xc6lB\\x92Y\\xdb\\xbd5\\x1c\\x94\\x9a\\xae\\xc3\\x05J&\\x88\\xb2\\x04\\xb7\\xdd%\\xe6\\xee\\xa5\\xdd\\x8f?\\xba\\x88vD\\x84c\\xec\\x80}\\xc2wP\\xdb\\x8d\\x84\\x98\\xef\\x94\\xcb|\\x0c\\xcd\\x88_`s\\x9c\\xc0\\xb3)Kc$\\xe06\\x9d\\x95\\'):\\x06\\xbf1-\\xd7*\\x95\\xa0\\x1c#\\x92\\xb8N\\x82bp{}:%c\\xec\\x0c\\xa5Kww\\xe5\\xbcO\\xe16\\x11\\\\6\\x8ciz ]c\\xc3Ba\\'\\x87U\\x89\\xe0K\\x1e\\xd2\\xd49B\\xb4\\xedB?\\x13v<\\xc4\\xf7\\x84\\xebP\\xc4\\x05<h\\xbb\\x15\\xf5\\xe7\\x96w/\\x96\\xd1NnD\\xc5\\x16[\\xcdn\\xa0\\xfer\\xbb\\xdc`rXS}\\xa6\\xb3\\xd1\\xbaS\\xcf\\xf3\\xbd\\xa0\\xb3\\xf6\\xaf\\x00Tl\\xe2\\xfa\\x8d~\\xd0\\x0f\\xd6\\xfe\\x14\\x00\\x8d\\xc70\\xd2\\x8c\\x8b\\xee\\xd3\\xef\\xb6\\xba=?\\xc7j\\xa0\\xec\\xd2\\xe2\\xbb\\xd7\\xe8\\xd5\\xab\\x06^\\xf3_\\xdf\\xe0\\xdc\\xf1\\xe5\\xcf\\xc0+P\\xe6\\xdf\\xdb\\xc0\\x0f\\x06!D\\xd1\\xc0+P\\x86\\xf7-1i\\xd4B\\xcf\\xc0+P\\x86\\x0f6\\xf0\\x8dJ\\xa7\\xe75\\x0c\\xbc\\x02E\\x94$\\x87\\x1b\\xe8\\x8a\\x1f\\xd4\\xc3\\xd5h\\xd7\\x90)\\xa3W\\xac\\xf0\\x96\\xef\\r\\x1a\\xb5\\xdcy\\x81\\x82jXW\\x97\\xecb\\xca\\x12\\xb1\\xad\\xd6bt\\x97\\xa5\\x03\\x00H E\\x82$\\x8eX\\xce\\xf1\\x14\\x8d\\xa1\\x8aCD\\xc9(%\\xce\\x1e\\x99EPxs\\x940\\x0e\\xcd\\x95ZeP\\xa9\\xc3\\x7f\\xf9\\xf3\\xd4\\x95\\x8a\\x08\\xda\\xc1H\\xb3\\x96\\xbc\\x80\\t\\xdfh\\x92|\\x1c>N\\xc9\\\\\\xb4\\xddO\\xc1\\xab\\xabA\\x9e?{v\\xf2\\xf0\\xe9\\xc9\\xc3_O\\x1e=:y\\xf8s\\xde\\xb7re\\xd8]A\\xc9L\\xb7{\\xf9\\xc3W\\x7f}\\xf7\\xb9\\xf3\\xe7/\\xdf\\xbf|\\xfcu\\xd6\\xf5i<\\xd7\\xf1/~\\xfa\\xe2\\xc5o\\xbf\\xbf\\xca=\\x8c\\xb8\\x08\\xc5\\xf3o\\x9e\\xbcx\\xfa\\xe4\\xf9\\xb7_\\xfe\\xf1\\xe3c\\x8b\\xf7N\\x8aF:|Hb\\xcc\\x9dk\\xf8\\xd8\\xb9\\xc9b\\x18\\xa0\\x85?\\x1e\\xa5of1\\x8c\\x101,P\\x04\\xbe-\\xae\\xfb\"2\\x80\\xd7\\x96\\x88\\xdap]l\\x86\\xf0v\\n*c\\x03^^\\xdc5\\xb8\\x1eD\\xe9B\\x10K\\xcfW\\xa3\\xd8\\x00\\xee3F\\xbb,\\xb5\\x06\\xe0\\xaa\\xecK\\x8b\\xf0p\\x91\\xcc\\xec\\x9d\\xa7\\x0b\\x1dw\\x13\\xa1#[\\xdf!J\\x8c\\x04\\xf7\\x17s\\x90Wbs\\x19F\\xd8\\xa0y\\x83\\xa2D\\xa0\\x19N\\xb0p\\xe43v\\x88\\xb1etw\\x081\\xe2\\xbaO\\xc6)\\xe3l*\\x9c;\\xc4\\xe9\"b\\r\\xc9\\x90\\x8c\\x8cB*\\x8c\\xae\\x90\\x18\\xf2\\xb2\\xb4\\x11\\x84T\\x1b\\xb1\\xd9\\xbf\\xedt\\x19\\xb5\\x8d\\xba\\x87\\x8fL$\\xbc\\x16\\x88Z\\xc8\\x0f15\\xc2x\\x19-\\x04\\x8am.\\x87(\\xa6z\\xc0\\xf7\\x90\\x88l$\\x0f\\x96\\xe9X\\xc7\\xf5\\xb9\\x80L\\xcf0eN\\x7f\\x829\\xb7\\xd9\\\\Oa\\xbcZ\\xd2\\xaf\\x82\\xc2\\xd8\\xd3\\xbeO\\x97\\xb1\\x89L\\x059\\xb4\\xf9\\xdcC\\x8c\\xe9\\xc8\\x1e;\\x0c#\\x14\\xcf\\xad\\x9cI\\x12\\xe9\\xd8O\\xf8!\\x94(rn0a\\x83\\xef3\\xf3\\r\\x91\\xf7\\x90\\x07\\x94lM\\xf7m\\x82\\x8dt\\x9f-\\x04\\xb7@\\\\uJE\\x81\\xc8\\'\\x8b\\xd4\\x92\\xcb\\xcb\\x98\\x99\\xef\\xe3\\x92N\\x11V*\\x03\\xdaoHzL\\x923\\xf5\\xfd\\x94\\xb2\\xfb\\xff\\x8c\\xb2\\xdb5\\xfa\\x1c4\\xdd\\xee\\xf8]\\xd4\\xbc\\x93\\x12\\xeb;u\\xe5\\x94\\x86o\\xc3\\xfd\\x07\\x95\\xbb\\x87\\x16\\xc9\\r\\x0c/\\xcb\\xe6\\xcc\\xf5A\\xb8?\\x08\\xb7\\xfb\\xbf\\x17\\xeem\\xef\\xf2\\xf9\\xcbu\\xa1\\xd0 \\xde\\xc5Z]\\xad\\xdc\\xe3\\xad\\x0b\\xf7)\\xa1\\xf4@,)\\xde\\xe3j\\xed\\xcea^\\x9a\\x0c\\xa0Qm*\\xd4\\xcer\\xbd\\x91\\x9bGp\\x99o\\x13\\x0c\\xdc,E\\xca\\xc6I\\x99\\xf8\\x8c\\x88\\xe8 BsX\\xe0W\\xd56t\\xc6s\\xd73\\xee\\xcc\\x19\\x87u\\xbfjV\\x1bb|\\xca\\xb7\\xda=,\\xe2}6\\xc9\\xf6\\xab\\xd5\\xaa\\xdc\\x9bf\\xe2\\xc1\\x91(\\xda+\\xfe\\xba\\x1d\\xf6\\x1a\"C\\x07\\x8db\\x0f\\xb6v\\xafv\\xb53\\xb5W^\\x11\\x90\\xb6oBB\\xeb\\xcc$Q\\xb7\\x90h\\xac\\x1a!\\x0b\\xaf\"\\xa1Fv.,Z\\x16\\x16M\\xe9~\\x95\\xaaU\\x16\\xd7\\xa1\\x00j\\xeb\\xac\\xc0\\xc2\\xc9\\x81\\xe5V\\xdb\\xf5\\xbd\\xec\\x1c\\x00\\xb6T\\x88\\xe2\\x89\\xccSv$\\xb0\\xca\\xaeL\\xce\\xb9fz[0\\xa9^\\x01\\xb0\\x8aXU@\\x91\\xe9\\x96\\xe4\\xbauxrtY\\xa9\\xbdF\\xa6\\r\\x12Z\\xb9\\x99$\\xb42\\x8c\\xd0\\x04\\xe7\\xd5\\xa9\\x1f\\x9c\\x9cg\\xae[EJ\\rz2\\x14\\xab\\xb7\\xa1\\xa0\\xd1h\\xbe\\x8f\\\\K\\x119\\xa5\\r4\\xd1\\x95\\x82&\\xceq\\xdb\\r\\xea>\\x9c\\x8d\\x8d\\xd1\\xbc\\xedNa\\xdf\\x0f\\x97\\xf1\\x1cj\\x87\\xcb\\x05/\\xa238<\\x1b\\x8b4{\\xe1\\xdfFY\\xe6)\\x17=\\xc4\\xa3,\\xe0Jt25\\x88\\x89\\xc0\\xa9CI\\xdcv\\xe5\\xf0\\xd7\\xd5@\\x13\\xa5!\\x8a[\\xb5\\x06\\x82\\xf0\\xaf%\\xd7\\x02Y\\xf9\\xb7\\x91\\x83\\xa4\\x9bI\\xc6\\xd3)\\x1e\\x0b=\\xedZ\\x8b\\x8ctv\\x0b\\n\\x9fi\\x85\\xf5\\xa92\\x7f{\\xb0\\xb4d\\x0bH\\xf7A49vFt\\x91\\xdeDPb~\\xa3*\\x038!\\x1c\\x8e\\x7f\\xaaY4\\'\\x04\\xce3\\xd7BV\\xd4\\xdf\\xa9\\x89)\\x97]\\xfd@Q\\xd5P\\xd6\\x8e\\xe8<B\\xf9\\x8c\\xa2\\x8by\\x06W\"\\xba\\xa6\\xa3\\xee\\xd61\\xd0\\xee\\xf21C@7C8\\x9a\\xc9\\t\\xf6\\x9dg\\xdd\\xb3\\xa7j\\x199M4\\x8b9\\xd3P\\x159k\\xda\\xc5\\xf4\\xfdM\\xf2\\x1a\\xabb\\x125Xe\\xd2\\xad\\xb6\\r\\xbc\\xd0\\xba\\xd6J\\xeb\\xa0P\\xad\\xb3\\xc4\\x19\\xb3\\xeekL\\x08\\x1a\\xb5\\xa23\\x83\\x9ad\\xbc)\\xc3R\\xb3\\xf3V\\x93\\xda9.\\x08\\xb4H\\x04[\\xe2\\xb6\\x9e#\\xac\\x91x\\xdb\\x99\\x1f\\xecNW\\xad\\x9c V\\xebJU\\xf8\\xea\\xc3\\x87\\xfem\\x82\\x8d\\xee\\x82x\\xf4\\xe0\\x14xA\\x05W\\xa9\\x84/\\x0f)\\x82E_v\\x8e\\x9c\\xc9\\x06\\xbc\"\\xf7D\\xbeF\\x84+g\\x91\\x92\\xb6{\\xbf\\xe2w\\xbc\\xb0\\xe6\\x87\\xa5J\\xd3\\xef\\x97\\xbc\\xbaW)5\\xfdN\\xbd\\xd4\\xf1\\xfdz\\xb5\\xefW+\\xbdn\\xed\\x01L,\"\\x8a\\xab~\\xf6\\xd1e\\x00\\x07Qt\\x99\\x7fzQ\\xed\\x1b\\x9f_\\xe2\\xd5Y\\xdb\\x851\\x8b\\xcbL}^)+\\xe2\\xea\\xf3K\\xb5\\xb6\\xfd\\xf3\\x8bC@t\\xee\\x07\\xb5A\\xab\\xde\\xea\\x06\\xa5V\\xbd3(y\\xbdn\\xb3\\xd4\\n\\x83n\\xa9\\x17\\x84\\x8d\\xde\\xa0\\x17\\xfa\\xcd\\xd6\\xe0\\x81\\xeb\\x1c)\\xb0\\xd7\\xa9\\x87^\\xd0o\\x96\\x82j\\x18\\x96\\xbc\\xa0\"\\xe97[\\xa5\\x86W\\xabu\\xbcF\\xa7\\xd9\\xf7:\\x0f\\xf2e\\x0c\\x8c<\\x93\\x8f<\\x16\\x10^\\xc5k\\xf7o\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xf1L/\\xb4@\\x03\\x00\\x00Y\\t\\x00\\x00\\r\\x00\\x00\\x00xl/styles.xml\\xacVmo\\x9b0\\x10\\xfe>i\\xff\\xc1\\xf2w\\n&\\x90B\\x14R-M\\x91*u\\xd3\\xa4v\\xd2\\xbe:`\\x12\\xab~A\\xe0t\\xc9\\xa6\\xfd\\xf7\\x9dy\\tl\\xed\\xd6\\xb4\\xdd\\x87*\\xf6q~\\xee\\xb9{\\xce\\xe7\\xce/\\xf6R\\xa0\\x07V\\xd5\\\\\\xab\\x04\\x933\\x0f#\\xa62\\x9ds\\xb5I\\xf0\\x97\\xbb\\xd4\\x890\\xaa\\rU9\\x15Z\\xb1\\x04\\x1fX\\x8d/\\x16\\xef\\xdf\\xcdks\\x10\\xecv\\xcb\\x98A\\x00\\xa1\\xea\\x04o\\x8d)g\\xae[g[&i}\\xa6K\\xa6\\xe0K\\xa1+I\\rl\\xab\\x8d[\\x97\\x15\\xa3ym\\x0fI\\xe1\\xfa\\x9e7u%\\xe5\\n\\xb7\\x083\\x99\\x9d\\x02\"iu\\xbf+\\x9dL\\xcb\\x92\\x1a\\xbe\\xe6\\x82\\x9bC\\x83\\x85\\x91\\xccf\\xd7\\x1b\\xa5+\\xba\\x16@uO\\x02\\x9a\\xa1=\\x99V~\\x1f\\xa11=\\n\"yV\\xe9Z\\x17\\xe6\\x0c@]]\\x14<c\\x8f\\xb9\\xc6n\\xec\\xd2l@\\x02\\xd8\\xd7!\\x91\\xd0\\xf5\\xfc6\\xf1\\xc5\\xbc\\xd0\\xca\\xd4(\\xd3;e\\x12lyZ\\xd2\\xb3{\\xa5\\xbf\\xa9\\xd4~\\x02Mp\\xeb\\xb5\\x98\\xd7\\xdf\\xd1\\x03\\x15`!\\xd8]\\xcc3-t\\x85\\x0c\\x14\\x1brm,\\x8aJ\\xd6z\\\\R\\xc1\\xd7\\x15\\xb7n\\x05\\x95\\\\\\x1cZ\\xb3o\\r\\x8d>\\x9d\\x9f\\xe4P-kt-\\x8f>\\xce\\xdaz\\xfd=\\x96g\\x0f\\xbc=V\\x13\\xb2\\x86\\x98\\\\\\x88c\\x05\\x02\\x9b,\\x18\\x16s\\x10\\xd7\\xb0J\\xa5\\xb0A\\xdd\\xfa\\xeePB\\xaa\\n\\xfa\\xb0\\xa5\\xdc\\xf8=\\xe3\\xbd\\xa9\\xe8\\x81\\xf8\\xe1\\xe9\\x07j-xnYl.\\xc7\\x05\\x0e02\\xdcj\\xe4\\x9d\\x9d\\xc7q\\x1c\\x91i\\x14Eq0!A\\xd0Tu\\xfd2ww\\x94\\x9e\\xad\\xfe)\\xa9\\xfc\\x95\\x19\\xc8\\xf1(\\xbc\\xd5\\xf4\\xa9\\x18M(\\xa8\\xfaZW9\\xdc\\xfa\\xa1\\xf3z\\xd3b.Xa\\xe0x\\xc57[\\xfbkti\\x03hc\\xe0r,\\xe69\\xa7\\x1b\\xad\\xa8\\xb0\\x01Z\\x90\\x7f\\x9d\\x84\\xd9\\x01c\"\\xc1f\\x0b\\xd7\\xfc\\x8f\\x9e\\x1d\\x95t\\x02%=\\x0f\\xc3($\\xb1\\x1f\\xc0_\\xd3\\xce.D\\xee\\x03\\xbf\\t\\xa7\\xe5\\xfe4\\xf5.\\x07\\xa8H\\xc6\\x84\\xb8\\xb5l\\xbf\\x16\\xc7\\xb2\\xd8\\xbb\\xb7/\\x90\\xda\\xc9T\\x9a\\xeb\\x1c\\xd4\\xc7\\xc8\\xde\\x94~\\t\\xb2u\\xcb\\xb6\\x14\\xed\\xc6\\x96f\\x8c\\xd6b\\x8f`m\\x97\\xbf\\x1c\\x16\\xed\\x8b#\\xfe\\t\\xa7a\\x98\\x0c\\xa4\\x08L\\x96\\xee4\\xa2e)\\x0ev\\xb8\\xd8\\xb1\\xd1\\xed \\x91a\\xb7lz\\xa3\\x1b*/\\xe5\\xf9L\\xa4\\x93\\xb0\\x01\\xc3\\xde\\x08\\x9b\\xed\\xe4\\xffd\\xd1\\xe8\\x01\\n\\x8cd\\xfeM\\xe4\\xa3\\\\\\xc8\\x0e\\xb6\\x04\\x7f\\xb2\\xaf\\x96\\x18\\x15m\\xbd\\xe3\\x02\\x06\\xc0\\x13\\x02\\x03f\\xbe\\x1fZ\\xa6\\x99\\x8e\\xc6\\xbe@M3\\x1d\\xa3@\\xe7\\xe4\\xac\\xa0;a\\xee\\x8e\\x1f\\x13<\\xac?\\xb2\\x9c\\xef$\\x88\\xd6y}\\xe6\\x0f\\xda4\\x10\\t\\x1e\\xd67\\xf6R\\x92\\xa9\\x1dglonj\\x98\\xd9\\xf0\\x8bv\\x15O\\xf0\\x8f\\xab\\xe5y\\xbc\\xbaJ}\\'\\xf2\\x96\\x91\\x13LX\\xe8\\xc4\\xe1r\\xe5\\x84\\xc1\\xe5r\\xb5Jc\\xcf\\xf7.\\x7f\\x8e\\x9e\\xc27<\\x84\\xcd\\xb3\\rML\\x82Y-\\xe0\\xb9\\xac\\xbad;\\xf2\\xb7\\x83-\\xc1\\xa3MK\\xbf\\x19\\xc6@{\\xcc=\\xf6\\xa7\\xde\\x87\\x90xN:\\xf1\\x88\\x13Li\\xe4D\\xd3I\\xe8\\xa4!\\xf1W\\xd3`y\\x15\\xa6\\xe1\\x88{\\xf8\\xca\\xa7\\xd7s\\t\\xe9\\x9f\\xde=\\tg\\x86K&\\xb8\\xea\\xb5\\xea\\x15\\x1a[A$\\xd8\\xfe#\\t\\xb7W\\xc2\\x1d\\xfe\\'Z\\xfc\\x02\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xa9\\xa5\\x93\\xeb%\\x17\\x00\\x00\\x9f\\xb0\\x00\\x00\\x18\\x00\\x00\\x00xl/worksheets/sheet1.xml\\x94\\x9d\\xddod\\xb7\\x91\\xc5\\xdf\\x03\\xe4\\x7f\\x10\\xfa\\xddW\\x97\\x1f\\xf7k0\\x9a\\xc0\\xd8Yc\\xf3\\xb0@\\x90\\xec&\\xcf=R\\xcf\\xa8aI\\xad\\xedn{\\xec\\xff~\\xcf%q\\xd9,Y\\xa7s\\x8f\\x80\\x043\\xf61\\x9bb\\xb3\\xc8b\\xd5\\x8f\\xc5\\x8f\\x7f\\xf9\\xed\\xf9\\xe9\\xe6\\xd7\\xdd\\xf1\\xb4?\\xbc\\xdcm\\\\\\xd3nnv/\\xf7\\x87\\x87\\xfd\\xcb\\xb7\\xbb\\xcd\\xff\\xfe\\xcfO?\\x8c\\x9b\\x9b\\xd3y\\xfb\\xf2\\xb0}:\\xbc\\xec\\xee6\\xbf\\xefN\\x9b\\xbf|\\xfa\\xf3\\x9f>~?\\x1c\\x7f>=\\xeev\\xe7\\x1b\\xb4\\xf0r\\xba\\xdb<\\x9e\\xcf\\xaf\\x1fnoO\\xf7\\x8f\\xbb\\xe7\\xed\\xa99\\xbc\\xee^\\xf0o\\xbe\\x1e\\x8e\\xcf\\xdb3\\xfez\\xfcv{z=\\xee\\xb6\\x0f\\xe9?z~\\xba\\xf5m\\xdb\\xdf>o\\xf7/\\x9b\\xdc\\xc2\\x87\\xe3\\x9a6\\x0e_\\xbf\\xee\\xefw\\x9f\\x0f\\xf7\\xbf<\\xef^\\xce\\xb9\\x91\\xe3\\xeei{F\\xffO\\x8f\\xfb\\xd7\\xd3\\xd2\\xda\\xf3\\xfd\\x9a\\xe6\\x9e\\xb7\\xc7\\x9f\\x7fy\\xfd\\xe1\\xfe\\xf0\\xfc\\x8a&\\xbe\\xec\\x9f\\xf6\\xe7\\xdfS\\xa3\\x9b\\x9b\\xe7\\xfb\\x0f\\x7f\\xfd\\xf6r8n\\xbf<\\xe1\\xf7\\xfe\\xcd\\xc5\\xed\\xfd\\xd2v\\xfa\\xcb\\x1f\\x9a\\x7f\\xde\\xdf\\x1f\\x0f\\xa7\\xc3\\xd7s\\x83\\xe6nsG\\xff\\xf8;O\\xb7\\xd3-Z\\xfa\\xf4\\xf1a\\x8f\\xdf`\\x1e\\xf6\\x9b\\xe3\\xee\\xeb\\xdd\\xe6G\\xf7\\xe1\\xc7\\xae\\xf5\\x9b\\xdbO\\x1f\\xd3\\x08\\xfds\\xbf\\xfb~\\xaa\\xfe|s\\xde~\\xf9\\xc7\\xeeiw\\x7f\\xde=\\xe0\\x8b\\xda\\xdc\\xcc_\\xc0\\x97\\xc3\\xe1\\xe7Y\\xf8W\\xfc\\xa3\\x16m\\x9e\\x92`ns{\\x7f\\xde\\xff\\xba\\xfb\\x8f\\xdd\\xd3\\xd3\\xdd\\xe6\\xf3\\x80\\xef\\xf0\\xff\\xd2\\xa7\\xe0\\x8f\\xf8\\x80\\xdb\\xf2\\t\\xf5\\x9f\\x97O\\xfb)}e\\x7f;\\xde<\\xec\\xben\\x7fy:\\xff\\xfd\\xf0\\xfd\\xbfv\\xfbo\\x8fg|l\\x87!\\x98G\\xe2\\xc3\\xc3\\xef\\x9fw\\xa7{|\\x05\\xf8\\xe0\\xc6ws\\xab\\xf7\\x87\\'4\\x81\\xff\\xbfy\\xde\\xcfs\\tC\\xb8\\xfd-wu\\xffp~\\xbc\\xdb\\xf8\\xb8\\xb9\\xf9\\xb2;\\x9d\\x7f\\xda\\xcfMmn\\xee\\x7f9\\x9d\\x0f\\xcf\\xff\\xca\\xff\\xd2\\xa5\\x8e\\xe56R\\xf7>o\\xcf\\xdbO\\x1f\\x8f\\x87\\xef7\\x98\\x18P\\x9f^\\xb7\\xf34s\\x1f\\xf0\\xe7\\xf7\\xfb\\x80\\x0f\\x9f\\xb5?\\xce\\xe2\\xbbM\\xd8\\xdc\\xe0cN\\x18\\x95_?\\xb5\\x1fo\\x7f\\xc5\\xafz\\x8f\\xff\\xa1\\xc1\\xd2\\xaaWZ\\x9d\\xc5sG\\xe6\\xf6\\x9co\\xdc\\xf4~\\x9b\\xf8\\xd8\\xf5=\\x9d\\xc5\\x18\\x97\\xd4fl\\xe2\\xf0~\\x93\\x18\\xb6\\xf5M\\xce\\xe2\\xa5\\x9bC\\xd7\\xf4\\xa4\\x9b\\xf8\"\\xd7\\xb79\\x8b\\x97n:\\x1f\\x9a\\x964\\xda+\\x8d\\xce\\xe2\\xa5\\xa3\\xb1\\x19\\xdf\\xff\\xd5\\xe7\\x99\\xbb\\xfa{\\x9f\\xc5\\xa5\\x9bnh\"it^\\xd2V7:\\x8b\\xcb\\xd7\\xee\\x9a\\x10\\xde\\xef\\xe8\\xa4\\xb49\\x8b\\x97\\x8eb8;\\xff~\\x9b\\x0e\\x8b\\xb10\\xebg\\xf5\\xd2\\xd3\\xc97-\\xf9\\xed\\x9dfK\\xd9\\x98\\xf2\\x14\\r\\xae\\x99z\\xd2W\\xc9\\x96\\\\mL\\xa1o\\xdaHZ\\x95\\xac\\xc9\\xd5\\xe6\\x84\\x11\\x08\\x1diU2(W[\\xd4\\xd87#\\x99\\xfc\\xf3\\xda(|[\\xb5M\\xf5\\x8dc_\\x96dR\\xae\\xb6)\\xdf\\x0c\\xac\\xa7\\x92U9cV]\\xc4\\xcaW\\xff\\x90!\\x96l\\xcc\\xd5F\\x16\\xa7\\x86\\xf5[22g\\xaclh\\x1cY]\\xbddeI}Y\\x0f\\xa6&\\x92I\\xe6%3K\\xea\\xb2\\x13`\\x92\\xb1V\\xb5-\\xab6\\xb3i\\xa2K\\x82\\x97\\xcc,\\xa9\\x97\\xbe\\x0em\\xe3\\x1c\\xd9]%3\\x9b\\x9d\\x83\\xb2|\\xf5m3\\x92%\\x01\\xbe\\x86`fI\\xbd\\xf45\\xc4f \\xcb\\x97\\x97\\xec,\\xa9\\xcb$\\xe8\\x9a\\x8eM-\\xc9\\xd0|mhXjY\\xa3\\x92iy\\xb3\\x7fy,\\x8al\\\\%\\xe3\\xf2\\xb5qMtW\\x0c\\x92m%um[\\x03\\x99YA\\xb2\\xad\\xa4.\\xb6\\x15\\x1a\\xb2\\xdb\\x06\\xc9\\xb4\\x92z\\xe9+\\xbe,2\\xaf\\x82\\xe6\\x0e\\xd6\\x1b\\xd88\\xd2m1H\\x96\\x95\\xd4\\xc5\\xd3BWY_%\\xcb\\n\\xf5\\x06\\x16\\x02[\\xb3\\x83dXI]f\\x00\\x16\\x016\\x01$\\xc3\\n\\xc6\\xb0\\xe0\\xbaw\\xf5\\x06F\\xcc,Hf\\x96\\xd4K\\xc7=\\xdc$\\xb2\\x9f\\x07\\xc9\\xca\\x92\\xba\\xac\\xb4\\xd4\\xca\\xa2deI]\\xc686\\x13\\x19\\xe3(\\x19YR_\\x8e2\\xc4K\\x8e\\x92\\x8d%\\xf5\\xd2\\xd3\\x1e\\x8e\\x07\\x99\\xb8Q2\\xb2\\xa4.\\xc7\\x84f \\xeba\\xd4N]\\xf5\\xee\\xe5\"=!E\\xc9\\xc6\\x92\\xba\\x1c\\x14\\xd8\\xe9P\\xb2\\xb0h\\x8e]c3\\x90\\x89\\x1a%\\x0bK\\xea\\xd2\\xd1\\x96\\x9fd%\\xa3\\x8a\\xf5\\xde\\x15:,\\x87+\\xec6J\\x16\\x96\\xd4e\\xda\\x8e\\x8dg\\xc7e\\xc9\\xc4\\xba\\xfa(\\xe61q\\x89\\x89u\\x92\\x89%\\xf5\\xd2\\xd7\\xb6\\xe9\\xfa\\xd6\\xfc\\x10\\x8f\\xb1\\x93L.\\xa9\\xcb\\xb666\\x8eXG\\'\\x99\\\\R\\xd7^\\x18\\xd9\\x82;\\xc9\\xe6\\x92\\xba,d\\x81\\x8f\\xb2\\x16\\xeb0\\xfb\\x1a\\xeb\\xa8ds]msnj<\\xb1\\xb9N\\xb2\\xb9\\xa4^\\x06u\\xc2tx\\xdf\\x0b\\xef$\\x93K\\xea*\\xdc\\xc1\\xfc\\xe5N\\xb2\\xb2\\xa4^\\xba\\n\\xdf\\xbe\\'\\xf6\\xd0KV\\x96\\xd4\\xd5\\xf60\\x92a\\xed%+K\\xea\\xd2W\\x04g\\xde\\x1f\\xd6^2\\xab\\xa4\\xae\\xf6\\xdc\\x9e\\xcc\\xab^2\\xab\\xa4.\\x8bWl<kU2\\xab\\xbe\\xde\\xcaZ\\x1aD\\xe8%\\xabJ\\xea\\xa5\\xab\\x08\\xa2x\\xb2\\xae\\xf4Z\\x08\\xb16\\xac\\xc1a\\xf5\\xaeWDb\\x10\\xbddeI]\\\\\\xb0\\x89.\\x88\\xbddfI]\\x9b\\x19\\xf1\\x97z\\xc9\\xcc\\x92\\xbax6c3\\x92\\xf90Hf\\x96\\xd4eKh\\xa9k3Hf\\x96\\xd4\\xc5\\xcc<\\xf5\\xf4\\x07\\xc9\\xce\\x92z\\xe9k\\x87iF|\\xa6A\\xb2\\xb3\\xa4.\\xe3\\xda7\\xccz\\x07\\xc9\\xce\\x92\\xba\\x1c\\x19\\x10\\xaf%&1H\\x86\\x96\\xd4\\xa5\\xaf~\\xa4\\xee\\xfd YZR_\\xf6\\xda\\xd0\\xf4\\xe1\\xdf\\x07\\x01\\x07-z_\\x9f\\xd2\\xa6\\xa1\\x19\\x89\\xfd\\x0e\\x92\\xa5%\\xf5\\xd2\\xf1\\x01\\xe1:6#$K\\x1b\\xea\\xf0\\x87\\x1b0\\x1c\\xc4o\\x1c%SK\\xea\\xcb\\xf17\\xd0\\xe0\\xe2(\\xd9ZR\\x979\\xd19\\x1c\\x80\\xdf\\xdf\\xd4F\\xc9\\xd8\\x92\\xba\\xda\\xd4Z\\xf2\\x85\\x8d\\x92\\xb1%\\xf5\\xd2YD\\x9b\\x89U\\x8c\\x92\\xad%u\\xe9j\\xd3\\xb1F%S\\x1bMZlj\\x06c\\x12,\\xf7$\\x99\\xddXop}h\\xc861J\\x86\\x96\\xd4\\xc5m`a\\xc6Q\\xcb\\x92\\xd5G\\xb5\\x96\\xf6S2\\xb2\\xb162$\\x89\\xc8\\xf4\\x9a$\\x13K\\xea\\xe246\\x13K\\xe7I\\x066\\xd5I2\\xe7Z\\xba\\xe8N\\x92\\x81%\\xf5\\xd2\\xd7\\x88c$1\\xdbI2\\xb0\\xa4.\\xab\\x81\\xc3\\x99\\x97,\\x89\\x93dbI]\\xb63\\xa4[\\x88\\x8dM\\x92\\x8d%\\xf5\\xd2Y\\x8f]\\x87d_%\\xab\\x9a\\x8cU9zt\\x9a$\\xb3J\\xear\\xc8\\x85G\\xc3\\xbe-\\xc9\\xb0\\xa6\\xda\\xb0\\xfa\\x0e\\x89\\xc2\\xda\\xc5e\\xdf\\x9c\\x96\\x8d\\xae\\xedl\\xc4\\xee\\xb0\\xe6#\\\\+&\\xa7\\xeb\\x90\\xc8\\x10\\xe9\\x11\\xd0\\xb5Zz:\\xc9/\\xa7\\xe0\\x96\\x8c\\x88\\x03Y#\\xe5\\xd2\\xeb\\xcc\\x19N\\xac\\x9e\\xa5\\x92[-C\\x9d\\xe4ew\\xc3I\\x98\\xa6\\xfe\\xb5\\x1cuk\\xe2\\x8f}3\\x81\\xae\\xba\\xfc\\xd0\\x0f\\xd1R\\xd6m\\xbd\\xe1a=b\\xe9\\x19\\xd7jY\\xeb$/\\'\\x0c\\xc4GX\\xfe\\xb7\\xd5\\x12\\xd7I~\\xd9\\xe8(\\xba\\xd0j\\xc9\\xea$\\xbf8\\x13\\xce\\x0cuKB\\x1b\\xae\\xd5\\x92\\xd7I^V\\x13\\xc4i\\x19w!B\"I^\\xf6\\x14\\x0e^\\xa8\\x94H\\xbd\\x03\\xfa\\xbe\\x89l\\x18\\x12\\xf9\\xb1\\x9e\\xe4\\xb2\\xa0H\\xc0\\xa1`E\\x08\\xd8%\\x10D\\xf8\\x10\\x8ba\\xd1\\xaf0\\x91 B\\xb36\\x90\\x12\\xc9\\xde\\xe5Dp$\\xc9\\x97\\x99\\x01W\\x8b\\x19K\\x82A\\x84\\xde\\x9aXJ3\\xb1\\x05/\\xe1 B\\xb3\\xf5\\xa9\\x0e\\xf1d:/4\\x1b4\\xc4\\x08H/j\\x1d\\x9a\\xd5\\x19f\\x04GP\\xb6pj\\xcc\\x08V\\x88\\x9am`\\xac\\x9b\\xd3\\x90\\x91,_1\\x11\\xbc\\x88f\\x196\\xab\\t\\x8cL\\xd2\\xa0\\x11`\\x83\\x15\\xf4\\x16\\x06\\x9c\\xc1\\xd6Xr\\xa2B\\xd6\\xcf6\\x03\\x91 \\xf8\\xc1\\x08@\\xa7Q$Y^\\xdcP\\xa4\\xd0\\xe9\\xa0h;\\x9f\\x05I\\x90Eg.\\xbeKp\\x880\\x10\\x06\\xda\\x9a\\x17d\\xdac\\xcd\\xf0,O\\xe2\\x10se\\x1eWBD\\x84\\x1e\\xd7^h?6\\x1d[.5\\xa6\\xc4\\x19\\xa8\\xa4\\x83\\x03\\xcd\\x96\\n\\r*q\\x86*\\x19=_\\x824\\xae\\xc4\\x19\\xb0\\x04nk\\xc3VL\\r-qI^\\x8e}\\xdcw\\xd3\\xd8\\x12g\\xe0\\x12\\xa4:\\x03\\xed\\xae\\xe6k\\x1a\\xbc\\x04<\\x05\\x05D5\\x833\\x80\\t\\xfczj\\x16\\x89\\x19Y?{-b\\xd27a\\x95\\x9f\\xa21&\\xce@&0\\x11\\xc6\";\\x8d2\\xc9\\xf2ej \\x8d\\xe2\\xd8\\xb6\\xa7\\x81&\\xce\\x90&0=\\xca9k\\xa8\\x893\\xac\\t8\\xef\\x91-A\\x1an\\xe2\\x0co\\x82v[\\xca\\x0fk\\x87>C\\x9cD\\x00\\x93l-\\xd6\\x98\\x13\\x97\\xe4U\\x06\\x81\\xc5\\xb6\\x9dF\\x9ddyu\\xf6\\xe8C}\\x98d\\xebr\"K\\xd6[\\x8c\\x01Q\\x02\\xd8T\\x93\\xbb\\xa3\\x1f\\xa2\\x9d\\x00\\r\\x97\\x82\\xed\\x952\\xf1\\t5\\x11:o\\xa8JX\"\\xdbT4\\x1a\\xc5\\x19\\x1c\\xc5cfS\\xd6\\\\\\x8b\\xbeX \\xc5s\\x8a_#R\\x9cAR\\xb0\\xfaSK\\xd4(\\x14g0\\x14\\xe0t\\x0c\\xb2v\\x1a\\x87\\x92\\xe5%\\xfcr\\xa5Y-\\xfabH\\x14\\x1cR\\xd9\\x1e\\xd8\\x89W\\x04\\xde$\\x18\\xba\\xa1v\\x99\\xd9\"\\x92h\\x93\\xf5S\\xd9\\xc0)C3\\xb05O\\x83S\\x9c\\xa1S\\x06\\x1c\\xfc\\xd8\\x9e\\xa2\\xf1)\\xce\\x02*\\xc8\\x88\\xb0\\xb5_#T\\x80\\x84\\x9a\\xcbC<\\xde\\xa21*\\xce@*0<6\\x0c\\x1a\\xa4\\xe2\\x0c\\xa5\\x02/\\x9c\\xad?\\x1a\\xa6\\xe2\\x0c\\xa7\\x82\\x8c\\x96\\rm\\xd1\\x0f\\xd1\\xb6C\\x83\\xad\\x80\\x95d7\\xe9\\\\\"Q\\xd6\\xcfd\\x03\\xae\\xe0\\x86^K\\xc7Z3C\\xc3\\xae8\\x9a\\x84s\\x1a\\xbb\\x92\\xe5%\\x8c\\x08?\\x9f\\x0e\\xaf\\xb6\\xe7\\x19b\\x05\\xdd\\xa5\\xe1,\\rYq\\x86Y\\x99xlDcV\\x9c\\x81V\\x90\\xeee;\\xbf\\x06\\xad8C\\xad #\\xd7\\xb3\\x03\\x84\\x86\\xad8\\xc3\\xad`t\\xe9\\x15+-\\xe8b\\xc0\\x95+A=\\r\\\\\\x01\\x99P\\xadj\\xd7\\x9a\\xd5\\xf6;\\x8b\\xae\\xe0\\xbe\\x16\\xfd\\xd24K3\\xf0\\nNg\\x13[\\xdc5x\\xc5\\x19z\\x05a\\xa1\\xc0V\\x06\\x8dXqI\\xbe\\xc2\\x9d\\xd0\\x90\\x15g\\x98\\x95\\x80\\xaf\\x8dvW\\x8bn\\x1aj\\x05)\\x05f\\x12\\x1a\\xb4\\x82ta\\x15\\xdd\\x84\\xd3JMX\\xa3V\\x9c\\xc1Vf\\x92\\x89\\xf6W\\xb35\\xcb\\xad r\\xc3\\x86W\\x03W\\x9c!W\\xe6\\xbc:\\xb3\\n\\x8d]q\\x06^\\x813\\xcc8W\\x97\\x80\\x94\\xf5\\xfb\\xa5\\xe1W\\xc8\\ry\\x97\\x08\\x14\\xa1\\xcd7\\xa83\\xb5`\\rYq\\x86Y\\xc1\\xd8\\xd2\\x80\\xbf\\x86\\xad\\xb8$\\xaf\\x18&\\x1a\\xa2H0\\x8a0\\x0e\\x86]A\\x94\\x94\\xcd]\\x8d^q\\x06_\\x99\\x97\\x06zUVK\\xa3[\\x82e%\\xcb\\xe44\\x9c%\\xcb/\\xec\\r;ih8\\x8b3<\\x0b\\x82+\\xe4B\\xb2F\\xb38\\x83\\xb3\\\\\\xa9%\\xe04\\x9e%\\xcbKl\\x05\\x97\\x11i\\x7f\\xb5\\xc0\\xa6aZ\\xe0T\\xd3\\xfc\\x9d\\x06\\xb58C\\xb5\\xa0Y\\xb6\\xa6%Ne\\xbd}\\x18\\xacevR\\xa9}h[\\xdcd\\xec\\x8e\\x86wq\\xae\\x91n\\x11\\'y!\\x14\\xf8\\x9do\\r^\\xf1\\x06^A\\x92\\x86\\x85O\\xbcF\\xafdyY\\xd5@\\x83\\x90X\\x81\\xd7\\xe8\\x95,/9\\x04\\xdcrc\\xb7\\xa9\\x13\\x8e\\xb2z6xK\\xaf\\xd0\\xba\\r>\\x01(B\\xb3&~\\xd2\\xd3\\xd8\\x86\\xd7x\\x95,\\xbf\\x00 ,\\xdc\\xe35\\\\%\\xcbK\\x92\\x11\\xe5\\x1b\\x88\\xady\\x8dW\\xc9\\xf2\\xd2]\\x84\\xa7\\xc8u}\\x8dP\\xf1\\x86P\\xc1\\t\\x8bU\\xdb\\xf0\\x1a\\xa1\\x92\\xe5Kwq\\xb6 \\xde\\x99\\xd7\\x08\\x95,/\\xa3\\x0b\\xa2k\\r\\xad\\xeb5\\\\%\\xcb\\xcbPwt\\xa85@\\xc5\\x9b\\xc2&\\x1e\\xe7\"634B\\xc5\\x9b\\xd2&\\xf3\\xc54:\\xd6\\xd29\\xce\\x1bD\\x85GL\\xbc\\x86\\xa8dy\\x19]D\\xc7Y)\\x03\\x8dQ\\xf1\\xa6\\xc2\\t\\x1cl\\x168\\xf2\\x89:Y\\xbf\\xfe\\x18H\\x05N+5\\xe8\\x84\\x9d\\x08\\xed\\xd6\\x9b\\x1c<\\x1e6\\x1b\\xc4\\xca&\\x86R\\x81\\x81\\xb0\\xc5]\\xaclbJ\\x9b\\xc03\\xa1\\x05S4N\\xc5\\'\\xf92\\x1bp-\\x89\\x8e\\x82V\\xdc\\xc4p*\\x98\\xbb\\xccs\\xf7\\x1a\\x9a\\x92\\xe5\\xd5*\\xcc\\xf8\\x06/\\x1681\\x15N<\\x8f\\xc5x\\xb1\\xc4\\x89AS\\xe6\\xa409ix\\x8dL\\xc9\\xf2\\xe2C\\x00L\\xa1\\xd3L\\xabsb\\xc0\\x94k\\x95^4.\\xc5\\x9bJ\\'#.\\xa1\\xb3\\xca<\\x1a\\x97\\xe2\\r\\x97\\x82\\xfe\\xb2d\\xbe\\xd7\\xb8\\x94,/1)zk\\xc2kXJ\\x96\\x17\\x0f\\xb8\\xe3[\\x85\\x86\\xa5x\\x83\\xa5`\\xfa\\xb2\\x8c\\xa7\\xd7\\xb8\\x94,/\\xb9q\\xac\\xe9l\\xfa&\\xd0d\\xfd\\xdak\\xcb\\x9e4\\x1ds\\xa6\\xb4\\xba\\'\\xde\\x16>\\xf1W\\x86W+)d\\xb8\\x148i\\xdedtX\\xe1&\\x8dK\\xf1\\x86K\\x99\\xae\\xf8\\xd9\\x1a\\x97\\x02\\x0f\\xcd\"\\x83S\\xbb\\x02\\x19\\xf4\\x1a\\xa4\\x92\\xe5\\xc5\\xdd\\xc4\\x15\\x13\\xe6Th\\x90\\x8a7\\x90\\xca\\\\\\x11\\x86V\\xc9\\xd2Jz\\x19He\\xe4\\x11 \\xafUE\\xc9\\xf2\\x02\\xcb\\xe3\\xa2$\\x1d\\x07\\xb1\\xac\\x97\\xa9\\xeb5\\x83\\x88\\xf5\\x97\\xc8\\x16S\\x8dX\\xf1\\xa6PJD\\x95Ff\\xed\\x1a\\xa4\\xe2\\r\\xa4B\\xdb\\xd4L\\xd20)\\xb8\\xa3L\\x0f\\xfd\\x1a\\x93\\xe2M\\xb5\\x94\\x8e\\xde\\xc1\\xf3\\x1a\\x92\\x92\\xe5%`\\xd5\\xd1d\\x84O\\x8c\\xc9\\xfa\\x85\\xd4 )\\x91Gt\\xbd\\x86\\xa4dy\\x99\\xc7\\xb0gfw\\x1a\\x92\\xe2me\\x14\\xee\\x0eiD\\x8a\\xb7\\xa5Q\\xb0\\xf0\\xb3Y\\xa6\\xd5F\\xf1\\x06I\\x99\\xf7U\\xe6\\x1ekL\\x8aO\\xf2j:\\xac\\xba\\x90\\xe15&%\\xcb\\xcb\\xe6\\x8d\\x1a\\x1cl\\x99\\xd0\\xa0\\x14o\\xa0\\x949\\xdeF\\xe7\\x86\\xe6{\\xae\\xad\\x9a\\xe25(%\\xcb\\x97\\xc1f5\\x8c5 \\xc5\\x1b \\x85\\xb6\\xa9\\x15\\xaf44J\\xc7\\xafiz\\rG\\xc9\\xf2e\\x1e\\xa0\\xd2dk\\xa8Z6\\xa3\\xb5\"*\\xde\\xe0(\\xf8\\x10F\\x14y\\rG\\xc9\\xf2\\x92\\xed@1?S6\\x95\\xed\\xaeZY\\x15o\\xd8\\x14 \\xb6\\xf4\\xd0\\xae\\xc1)>\\xc9\\xcb\\xe1\\x9a\\x04\\x0c\\xb5J*\\xde\\x82) \\xdb\\xd9\\xc9O\\x03S\\xbc\\x01S\\xf0\\x05v\\xebf\\x89\\x94X\\xf0\\x06S\\xe9{zg\\xd8k\\x9cJ\\x96/\\x03\\xdd\\x8d|\\xbb\\xd28\\x15o8\\x95\\xeeJ\\x18U+\\xb0\\xe2\\r\\xa8\\x12\\xc1t3+\\xd4H\\x15ok\\xac \\xd1\\xc4\\x0eVZ\\x91\\x15oP\\x15\\x80r\\xf4\\x98\\xad\\x95Y\\xf1\\x06U\\xf1W\\x0cOCU\\xbcAU\\xc6+!Y\\rU\\xf1\\x06U\\x01\\x8fIC\\xbd\\x1a\\xab\\xe2\\r\\xab\\xe2\\xe6\\x8b\\x0fd\\xb1H\\xf0\\xc9z\\xf7\\xd0\\xb0*l\\x9b\\xd2@\\x15o@\\x15\\x14\\x94`%\\n\\xbc\\x06\\xaady\\t\\xb9 \\xc1\\xc2\\x16w\\xad\\xbe\\x8a7\\xa0\\nBD\\xac\\xc4\\x9b\\xd7@\\x95,\\xafBp,W\\xec5P%\\xcb\\x97\\xb5,\"\\xa9K+\\x18k\\xe9\\x04\\x03\\xaa\\xb0\\x14\\x85\\x06\\xaaxSYe\\x8e\\xf9\\xb3y\\xab\\x81*\\xde\\x80*\\xc8\\xd40w^\\xe3T\\xbc\\xe5Tx\\x86B\\xc3T\\xbc)\\xb1\\xc2FVCTP\\x83\\xad\\xc2\\xc10\\x02\\xb4Y\\xcd\\xc74\\x84\\xca\\x95\\x81\\xd5\\x98\\x14ok\\xac\\x80\\xe7d\\xf3@\\xa3R\\xbc\\xa1R\\xe6\\x94\\x1d\\x9b\\x08\\x1a\\x97\\xe2m\\x99\\x15\\x84\\xa3\\xd9\\xfe\\xabq)\\xde\\x14Z\\xb9\\xd6]\\tK\\xf1\\x06K\\xa13A\\x0b\\xa0\\x18$\\xe5J\\xb6NCR\\xbcARhW5\\xaf\\xd1\\xe0(\\xa4M\\\\\\xaeRX\\x94,/\\x87\"\\xbap\\x05\\x8dE\\xc9\\xf2\\x92\\x1f\\'\\xb3*h J\\x96\\x97\\xa4\\x17\\x10N\\x13\\xfcc5\\xb25*%\\x98\\x9a*\\xd8u\\xe88kU\\xd3\\r\\x94B\\xdb\\x94v\\xb2`*\\xa8 #\\xbcr<$\\x9b\\x0b\\xa6\\x9c\\xca\\x95{\\x10A\\xe3S\\xb2\\xbc\\x84~x\\x89s\\rO\\t\\xb6\\x9c\\n\\x02\\xe5d\\xa5\\x0c\\x1a\\x9f\\x92\\xe5eBst-h|J\\x96\\x979M\\x19\\xf5\\xa0\\xf1)Y^\\xba\\xdbQ\\x80/hHJ\\x96\\x97\\xc4\\x1d\\x0e\\x82\\xe6\\x12\\x19+~\\xaf\\x01*\\xc1\\x02*\\xfc~a\\xd0\\x00\\x95,/\\x1e%\\x7f\\x0fA+\\xa1\\x12\\x0c\\x9f\\x8271\\xd8\\xa6\\x1f4@%\\xcb\\xcba\\x9e\\xee\\xcdA\\xe3S\\xb2\\xbcL\\r|\\x85\\xccB4>%X>\\x05\\xe1\\xb5\\xae\\xbe\\xe7\\xcb\\x9e^\\xd0`\\x95`J\\xaa\\xa0\\x9e##=\\x83F\\xabdy55Xw5Z%\\xd8\\x87xp-\\x99\\xb6+>\\x17RWU\\xe1l_\\xd0\\xaa\\xaad\\xf925\\x90\\xec\\'\\'\\xcf\\xa0\\xc1*Y^b\\x97\\x947\\x0b\\x1a\\xab\\x92\\xe5Ko\\x11\\x11e\\xbb\\xaa\\x86\\xaa\\x847UT\\xe8\\x137ACU\\xb2\\xbc:\\'\\xb3\\x1bz!\\xb1\\'\\xabc\\x1bY^v\\x10\\xd4Q\\xa3\\xe3 y\\x9b\\xc1\\xa0*$f\\x12\\xc4\\'y\\xec\\x9b<\\xb8TI\\xceGA|\\x93\\xc7\\x94O\\x81\\x93\\xc2\\xee8\\x05\\xf1Y\\x1eS>\\x05y/\\x16\\x9f\\x0c\\xea\\xcb<\\xf5M:\\x14\\xfc\\xe5\\xe3\\xa0y\\x99\\xa6~\\n\\xdekd\\xe7\\xb9\\xa0q*Y~q\\xe7ib \\x88\\x0f\\xf4XP\\x05X\\x18\\xf3 \\xb4\\x02*\\xc1\\x82*\\xa8\\x07a\\xfd\\x94\\xf7\\x83\\x8aA|\\xa4\\xc7\\xbe\\xd2\\x83\\xda\\xf6t2k\\x86g@\\x15\\x14\\x95a\\xfb\\xb3\\x86\\xa6\\x04S?\\x05s\\x8e\\xae?\\x1a\\x9a\\x12\\x0c\\x9a\\xb26\\xad\\x16\\xb4b*Y^\\x16\\xb9\\x96s\\x0eA\\x03U\\xb2\\xbc$E\\xaf\\x0c\\x8af\\x88\\xa6\\x98J\\xe4\\x10S\\xd0\\xd0\\x94,/\\xa7&\\x8f\"8+\\x8a\\xa9\\x04\\x8dS\\xc9\\xf2\\xcb~M\\xd2\\xe5A{\\xd4\\'\\xcb\\xcbv\\x8d!!V\\xa8\\x81*\\xc1\\x82*x\\xa2\\x96\\xc5\\x014R%\\x98\\xe2)\\xa80\\xc7\\x9e6\\n\\x1a\\xa9\\x92\\xe5e&\\x83\\xd8a\\xe5\\xc3\\x83\\x86\\xaady\\xf1/05\\xd8@h\\xa8J0\\xa8J\\x87|\\x04mW{\\x9d\\xce\\xb2*\\xf0\\x8d\\xd9\\xf2\\xa9\\xb1*\\xc1>\\xe4\\x83/\\x8e\\xf9\\xdc\\x1a\\xab\\x12\\x0c\\xab\\xe2Z\\x9c=h\\xc3Z\\x84\\xc5>\\xe8\\x83\\x02\\xd2,\\\\\\xa6\\xf1)\\xe1-\\x9f\\xc2j/\\x06\\xadhJ\\x96/3\\x187\\xb4\\x19\\xe2\\x104>%\\xcb/\\xa7\\x0fz\\x8c\\xd6\\x10\\x95`\\x10\\x95\\xf9{\\xa3\\xfb\\xb4V5%\\xd8\\xb7}\\xc0\"\\x9a\\x87\\x00\\xd9\\xd9I\\x83V\\x82\\xa9\\xa1\\x82\\x03/\\x1dl\\x8dS\\t\\x86SA%p\\x06h\\x06\\x8dS\\xc9\\xf2\\xb2w\\xf0\\x92xACS\\xb2\\xbcl|\\xc8\\x0c3#\\xd1\\xd0\\x94`\\xd0\\x14\\x07\\x8a\\x8b\\x15\\xf5\\n\\x1a\\x9f\\x92\\xe5e9\\x0e`\\xf1\\xd8\\xfa\\xa6\\x01*\\xc1\\xbe\\xf6\\x034\\x93\\xdd\\xd8A\\xd5@)\\x15`\\xa0\\x14\\xc4\\xb1X-\\xce\\xa0A)Y~\\x89c1t9hLJ\\x96\\x97\\xe5\\x02\\x85\\x0c\\x98\\x9f\\xac1)\\xc12)8\\xa32\\xffGcR\\x82aRp\\xd9\\x9e:\\x14\\x1a\\x93\\x12l\\xf9\\x94\\x81\\xbe\\x1d\\xab!)\\xc1 )\\x01\\xcf?1\\x83\\xd3\\x90\\x94`\\x90\\x14\\x16V\\xd0p\\x94`p\\x94\\x89?I\\xab\\xd1(\\xc1\\xd0(3\\x07\\xce,X\\xa3Q\\x82\\xa1QP\\xc1\\x91\\xdd\\xda\\x0f\\x1a\\x91\\x92\\xe5\\x17\\xbf\\x9dUM\\x0f\\x1a\\x90\\x92\\xe5\\xd5\\xd2\\xcb\\xecA\\xe3Q\\xc2[\\x1e\\x85\\xa5\\xf6\\x83\\xc6\\xa3dy\\xe9./)\\x1c4\\x1e%\\xcbK\\xea\\x03/r\\x98\\x87-\\x98uhUT\\x82}\\x05\\x88\\x96\\xcb\\x0e\\x1a\\x9f\\x92\\xe5\\x97,\\x13*C\\x9a\\x1fr\\x12\\xd3`\\x95``\\x95+w\\x07\\x83F\\xabdy\\xe9<\\xd2\\x9ck^\\xa3\\x08\\x1a\\xbb\\x92\\xe5U\\xc2\\x8cn\\xaa\\x1a\\xbf\\x12\\x0c\\xbf\\x82K\\xd14J\\x9d\\x88\\x94\\xf5Q_\\x03\\xb0\\\\y\\x146h\\x04K\\x96\\x97Y\\x8e\\xc3\\x08\\xf3c5\\x82%X\\x82\\x05\\xa7\\x7fv\\xfc\\xd7\\x08\\x96`\\x08\\x16\\xec\\xaa\\xec\\t\\x94\\xa0\\x11,Y^V\\x11\\xdc3f\\xdeEbR\\x84\\xef\\xcd\\x944\\xe2\\xef\\x1f\\x06\\xad\\xb2J\\x96_\\xe2X\\xf4\\x8c\\xa3a,\\xe1me\\x15\\xbaU%6E\\x18\\x86\\xfa2\\x1e|Mv\\xfc\\x88\\x1a\\xce\\x92\\xe5e\\xad\\xe0q\\xb7\\xa8\\xf1,Y\\xbe\\x0c/\\xf8\\\\2\\x1b\\xa2F\\xb4dy\\xb16\\xfe\\xce\\xb7\\xc6\\xb0D\\xcb\\xb0\\x00\\x8d\\xa7\\xdd\\x95\\xa2\\x9aq\\r\\xc4\\x12\\xb5\\xb2*Y^\\xceI\\xb8\\x11\\xc6\\x1e&\\xd7\\xca\\xaaD\\xc3\\xad\\xf0J\\xb4Q\\xc3V\\xb2\\xbcv\\x86\\x88O\\x185n%\\xcb\\xcbL\\xe0O\\x9bD\\x8d[\\xc9\\xf2\\x15Q\\xc1\\xa8q+Y^m\\xca\\x8c\\xce\\x8d\\x1a\\xb8\\x92\\xe5e\\x1fF\\x14\\x93M\\x07\\r\\\\\\x89\\xf6\\xe9\\x1f^\\x8f\"j\\xacJ\\x96W\\xfb\\x04\\x89\\tF\\rU\\xc9\\xf22\\x1d@P\\x12\\xa7;j\\xacJ\\x96\\x97\\xe9\\x80\\x8b|d{\\x8f\\x1a\\xab\\x92\\xe5%&\\x880\\x15mW\\xa23\\xa3)\\xa6\\x82\\xed\\x9d\\x95\\xfb\\x88\\x1a\\xac\\x92\\xe5e\\xfa\\xa2B\\x1d\\xfd\\xde\\xa4\\x18J4|\\nb\\xa3\\xecR@\\xd4\\xf8\\x94,\\xaf\\xf65\\x92\\xe3\\x8f\\x1a\\x9f\\x92\\xe5\\xd5\\xe2\\xcb\\x0e\\xf9Q\\xab\\xa6\\x92\\xe5\\xa5\\xbb\\xbc~\\\\\\xd4\\x00\\x95,\\xaf\\xfa\\xebI.\"j\\x84J\\x96\\x97\\xe9\\xcb\\xebFD\\rQ\\xc9\\xf2j\\xf5e7\\xc9\\xa3\\xc6\\xa8d\\xf9\\xc5o\\xe8\\xd9\\xea\\xa0!*1\\xc9\\xcb\\xe2\\x8b\\xaf\\xed\\xfd\\x83a\\xd4\\x08\\x95,_z\\x8bZ\\x00\\xb4Y\\xcd\\xd6\\x0c\\xa0\\xc2\\x8b\\xa1G\\x8dQ\\xc9\\xf22w\\x1dM\\x17D\\x8dQ\\xc9\\xf22\\x17x\\xb9\\xbb\\xa81*Y^\\xe6.v6\\xf2\\xa5i\\x88J\\xb4\\xa5T\\xe8\\xf5\\xca\\xa8UR\\xc9\\xf22\\xba\\xb8^\\xc9,X#T\\xa2\\xa9\\xa4r\\xc5\\xe3\\xd5\\x00\\x95h\\x00\\x15\\xbc\\xa0M{\\xabmk\\x86O\\xe1\\x99\\xc4\\xa8\\x11)Y\\xbe\\x0c.\\xbfE\\x18\\xb5\\xca)Y^\\x96\\x05\\x1c~\\xcc\\x0f\\x1b\\x12\\rO\\x89\\x06O\\xb9R6-jxJ\\x96_\\x0e\\xc6\\x8cz\\x89\\x1a\\x90\\x92\\xe5\\x95\\xff\\xcbb\\xc3Q\\xe3Q\\xb2|\\xe9\\xee\\x95\\x99\\xa1=\\xee\\x13\\r\\x8f\\xc2\\xb9\\xb0\\xa8\\xe1(Y^f\\x06\\x88Q\\xe6Fi\\x04J4\\x95R\\xe6G\\x1dHx5j\\x0cJ\\x96\\x975\\x18w6i\\xbbR\\xc9\\x86h \\x14\\xd4\\xf7\\xed\\x87\\x15\\x0f\\xe6F\\x8dH\\xc9\\xf2\\xa5\\xf3\\x00\\xad\\xa65\\xf9\\xe6\\xa8\\xe1)Y^\\x96\\x10\\xbcZ\\xc5FH\\xa3S\\xa2y\\xdb\\x07wUh\\xb3\\x12\\x08\\x1d\\r\\x9c2\\x17VF\\x04\\xe8\\xf2\\xc3f\\xa3VU%\\x1aR\\x051\\\\0<k>D\\x8b\\xad\\xd8\\x12+\\x98B,\\xb0\\xa0a+\\xd1`+\\xf3\\xe3G\\xcc?\\xd0\\xaa\\xaaDC\\xad\\x00A\\xa7\\xdd\\xd5vF\\x03\\xad\\\\\\xc9\\x92F\\rZ\\xc9\\xf2\\xea@M]f\\rZ\\x01\\xf3QU\\x15\\xc3\\xf02\\xf28j\\xd4J\\x96\\x17\\xaf\\xee\\xcaAR\\x83V\\xa2-\\xaeBi\\xb9\\xa8a*Y^m\\x87ta\\xd50\\x95h0\\x156s5D%\\x9a\\x97}\\x18\\xe4\\x105>%\\xcb\\xcbN\\xc8\\x0b\\\\F\\x8dO\\xc9\\xf2\\xca\\xbbg\\xeb\\xa6F\\xa7D[=\\x05\\xef\\xa3\\x9a\\xbd\\x84\\x1d\\xd04R%\\x1aR\\x05\\xb7\\x96Y(O\\xe3T\\xa2\\xe1T\\xe6\\x94\\x1a\\xeb\\xae\\xc6\\xa9D\\xfb\\xc8\\xcf\\x9c\\xe0%\\')\\rT\\x89\\xf6\\x91\\x1f8\\x1d\\xf5\\xee\\xc1 \\xc2\\xa8Q+Y^\\x0c\\x10t7\\x1bl\\x8dZ\\x89\\xb6\\x92\\nN@lL\\xb4\\xdd\\xeem!\\x15\\x1ax\\xd2\\xa8\\x95h\\xa8\\x15x/\\xf6g\\xf8\\xcf\\x1f\\xd8\\x1a\\xa2Q,\\xd1P,\\x03\\xf2L\\xa6\\xfe7\\x0b\\xc7hXK4X\\x0b\\nv0\\xdb\\xd7\\xb0\\x96h\\xb0\\x96\\x1e\\xa9\\xcd55\\x14\\xa3\\xc6\\xb8dy\\xf1\\xb4i\\x81\\x98\\xa8!.Y^\\xb9\\xa7\\x0c\\xaa\\x8b\\x1a\\xe3\\x92\\xe5\\xa5\\xbb\\x81\\x16\\xd7\\x8e\\x1a\\xe4\\x92\\xe5K\\x7f#\\x8d\\xcdh\\x88K4o\\x03a\\xa1b\\x0b\\xa0F\\xb8D\\xf34\\x10.\\xcd\\xb0\\x07\\xeb\\xa3\\x06\\xb5dy9\\xd3\\xa2\\n\\x11[D4\\xaa%\\x9a\\xaa+\\xa8\\x10\\xc3\\xc7A\\xf3A-\\xc8\\x82\\x94$Y\\xf34\\x8e%\\xbe}\\x1d\\x88]\\xd4\\x83=*\\xdcf\\x96\\x97\\xe1m\\x9bUE\\x14\\xa3\\xc6\\xb1dy\\xf1C\\x00;\\xb3\\xb3\\x95\\xc6\\xb1D\\xc3\\xb1\\x8c(\\xceH\\xdb\\xd5\\x0e\\x86\\xa6\\x12\\x0b\\xf8f\\xeb\\xdf\\xb0\\t\\xa8A-\\xd1@-\\xa1\\xa1g\\n\\x8di\\x89\\xf6\\xb5 l.\\xcc\\xbe5\\xa6%\\xbe\\xa9\\xca\\xc2^V\\x8b\\x1a\\xd2\\x92\\xe5\\xe5h5\\xbf\\xbe\\xb3\\xa2\\xb0r\\xd4\\xf8\\x96,_&9\\xd6g:M\\xb4\\xc0\\x8d\\xe5[p\\x8c\\xa5\\xedj6iJ\\xb5\\xccK\\x139\\x1e#\\x92\\xa0\\xd8z\\x96/c\\r\\xb7\\x9dxy\\x9d\\x86\\xb7dy\\xd9\\xfe\\xf8=\\xccN\\xe3[\\xb2|\\xe9\\xee\\x1fS\\x13\\xb7\\xa7\\xc7\\xdd\\xee\\xfcy{\\xde~\\xfa\\xf8\\xba\\xfd\\xb6\\xfb\\xef\\xed\\xf1\\xdb\\xfe\\xe5t\\xf3\\xb4\\xfbz\\xbe\\xdb\\xe0\\xc0\\xb0\\xb99\\xee\\xbf=.\\x7f>\\x1f^\\xd3?E\\x95\\x94/\\x87\\xf3\\xf9\\xf0\\xbc\\xfc\\xedq\\xb7}\\xd8\\x1d\\xe7\\xbf\\xe1\\n\\xd2\\xd7\\xc3\\xe1\\xbc\\xfc\\xe5\\xf6\\xd3\\xc7\\xdb\\xef\\x87\\xe3\\xcf\\xe9s>\\xfd\\xbf\\x00\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00]{\\xd5\\xfeJ\\x01\\x00\\x00o\\x02\\x00\\x00\\x11\\x00\\x08\\x01docProps/core.xml \\xa2\\x04\\x01(\\xa0\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x92QK\\xc30\\x14\\x85\\xdf\\x05\\xffC\\xc9\\xb3m\\x9a\\r\\xd4\\x85\\xb6c*\\x8317\\x06\\x9b(\\xbe\\x85\\xe4n+6iH\\xa2\\xdd\\xfe\\xbdi\\xbb\\xd5\\xca|\\xf019\\xe7~9\\xe7\\x92d|\\x90E\\xf0\\x05\\xc6\\xe6\\xa5J\\x11\\x89b\\x14\\x80\\xe2\\xa5\\xc8\\xd5.E/\\x9bix\\x8f\\x02\\xeb\\x98\\x12\\xac(\\x15\\xa4\\xe8\\x08\\x16\\x8d\\xb3\\xeb\\xab\\x84k\\xcaK\\x03+Sj0.\\x07\\x1bx\\x92\\xb2\\x94\\xeb\\x14\\xed\\x9d\\xd3\\x14c\\xcb\\xf7 \\x99\\x8d\\xbcCyq[\\x1a\\xc9\\x9c?\\x9a\\x1d\\xd6\\x8c\\x7f\\xb0\\x1d\\xe0A\\x1c\\xdfb\\t\\x8e\\t\\xe6\\x18\\xae\\x81\\xa1\\xee\\x88\\xe8\\x84\\x14\\xbcC\\xeaOS4\\x00\\xc11\\x14 A9\\x8bID\\xf0\\x8f\\xd7\\x81\\x91\\xf6\\xcf\\x81F\\xe99e\\xee\\x8e\\xdaw:\\xc5\\xed\\xb3\\x05o\\xc5\\xce}\\xb0yg\\xac\\xaa*\\xaa\\x86M\\x0c\\x9f\\x9f\\xe0\\xb7\\xc5\\xf3\\xba\\xa9\\x1a\\xe6\\xaa\\xde\\x15\\x07\\x94%\\x82Sn\\x80\\xb9\\xd2d\\xd1M0\\x9f,g\\xf3I\\xb0\\x9e\\xcc\\x96\\xb3\\x04\\xf7\\xb4z\\x8f\\x05\\xb3n\\xe1W\\xbe\\xcdA<\\x1c/\\xed\\x97\\x16Oo\\xca\\xb4O\\x80\\x08|<\\xda\\x969+\\xaf\\xc3\\xc7\\xa7\\xcd\\x14e\\x83\\x98\\x8c\\xc2x\\x18\\x92\\xbb\\r\\x19\\xd18\\xa6d\\xf4^\\'\\xf85_\\xc7m/\\xe4)\\xc7\\xff\\x89\\x03JH\\x8fx\\x06d\\t\\xbe\\xf8\"\\xd97\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x03\\x04\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xea\\xf0\\xeb(\\x96\\x01\\x00\\x00$\\x03\\x00\\x00\\x10\\x00\\x08\\x01docProps/app.xml \\xa2\\x04\\x01(\\xa0\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x9c\\x92\\xc1n\\xdb0\\x0c\\x86\\xef\\x05\\xf6\\x0e\\x86\\xee\\x8d\\x9c\\xae(\\x8a@V1\\xa4\\x1bz\\xe8\\xd0\\x00q\\xda\\xb3&\\xd3\\xb1PY\\x12D\\xd6\\x88\\xf7\\xf4\\x95m$q\\xb6\\x9dv#\\xf9\\x13\\xbf>\\x91\\x14\\x0f\\x87\\xd6f\\x1dD4\\xde\\x15l\\xb9\\xc8Y\\x06N\\xfb\\xca\\xb8}\\xc1v\\xe5\\x8f\\xeb{\\x96!)W)\\xeb\\x1d\\x14\\xac\\x07d\\x0f\\xf2\\xcb\\x95\\xd8D\\x1f \\x92\\x01\\xcc\\x92\\x85\\xc3\\x825Da\\xc59\\xea\\x06Z\\x85\\x8b$\\xbb\\xa4\\xd4>\\xb6\\x8aR\\x1a\\xf7\\xdc\\xd7\\xb5\\xd1\\xf0\\xe8\\xf5G\\x0b\\x8e\\xf8M\\x9e\\xdfq8\\x10\\xb8\\n\\xaa\\xebp2d\\x93\\xe3\\xaa\\xa3\\xff5\\xad\\xbc\\x1e\\xf8\\xf0\\xb5\\xecC\\x02\\x96\\xe2[\\x08\\xd6hE\\xe9\\x97\\xf2\\xa7\\xd1\\xd1\\xa3\\xaf)\\xfb~\\xd0`\\x05\\x9f\\x8b\"\\xd1mA\\x7fDC\\xbd\\xcc\\x05\\x9f\\xa7b\\xab\\x95\\x85u2\\x96\\xb5\\xb2\\x08\\x82\\x9f\\x0b\\xe2\\t\\xd40\\xb4\\x8d2\\x11\\xa5\\xe8h\\xd5\\x81&\\x1f34\\xbf\\xd3\\xd8nX\\xf6K!\\x0c8\\x05\\xebT4\\xcaQ\\xc2\\x1a\\xda\\xa6d\\x8cm@\\x8a\\xf2\\xcd\\xc7wl\\x00\\x08\\x05O\\rSq\\x0c\\xe7\\xbd\\xf3\\xd8\\xdc\\xca\\xe5\\xd8\\x90\\x82\\xcb\\xc6\\xc1`\\x02I\\xc2%bi\\xc8\\x02\\xbe\\xd4\\x1b\\x15\\xe9\\x1f\\xc4\\xcb9\\xf1\\xc80\\xf1N8\\xdb\\x81ozs\\xce7~9\\xbd\\xf4\\x87\\xf7\\xda\\xb7A\\xb9^\\x96\\x00\\x88\\xa6\\x82l\\xe7\\xccxs\\xd4\\x0b~\\x14\\xc5\\xb3q\\xef\\xb8\\x0b\\xa5\\x7fT\\x04\\xc7\\t_\\x16\\xc5\\xb6Q\\x11\\xaa\\xb4\\x94\\xd3\\x06N\\x05\\xf1\\x94\\x86\\x1b\\xed`\\xb2n\\x94\\xdbCu\\xec\\xf9[\\x18\\xee\\xe1u:z\\xb9\\xbc[\\xe4_\\xf3\\xb4\\xeaYM\\xf0\\xf3y\\xcbO\\x00\\x00\\x00\\xff\\xff\\x03\\x00PK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00b\\xee\\x9dh^\\x01\\x00\\x00\\x90\\x04\\x00\\x00\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00[Content_Types].xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xb5U0#\\xf4\\x00\\x00\\x00L\\x02\\x00\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x97\\x03\\x00\\x00_rels/.relsPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\x81>\\x94\\x97\\xf3\\x00\\x00\\x00\\xba\\x02\\x00\\x00\\x1a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xbc\\x06\\x00\\x00xl/_rels/workbook.xml.relsPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00s\\xdf\\x1f\\xde-\\x02\\x00\\x00\\x82\\x04\\x00\\x00\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xef\\x08\\x00\\x00xl/workbook.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xeb\\x1b\\xc7=\\xa5\\x00\\x00\\x00\\xc0\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00I\\x0b\\x00\\x00xl/sharedStrings.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00u>\\x99i\\x93\\x06\\x00\\x00\\x8c\\x1a\\x00\\x00\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00 \\x0c\\x00\\x00xl/theme/theme1.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xf1L/\\xb4@\\x03\\x00\\x00Y\\t\\x00\\x00\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xe4\\x12\\x00\\x00xl/styles.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xa9\\xa5\\x93\\xeb%\\x17\\x00\\x00\\x9f\\xb0\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00O\\x16\\x00\\x00xl/worksheets/sheet1.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00]{\\xd5\\xfeJ\\x01\\x00\\x00o\\x02\\x00\\x00\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xaa-\\x00\\x00docProps/core.xmlPK\\x01\\x02-\\x00\\x14\\x00\\x06\\x00\\x08\\x00\\x00\\x00!\\x00\\xea\\xf0\\xeb(\\x96\\x01\\x00\\x00$\\x03\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00+0\\x00\\x00docProps/app.xmlPK\\x05\\x06\\x00\\x00\\x00\\x00\\n\\x00\\n\\x00\\x80\\x02\\x00\\x00\\xf72\\x00\\x00\\x00\\x00'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "gglbrFTMV5OF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tar=pd.read_excel('survival_LUSC.xlsx')\n",
        "dt=pd.read_excel('LUSC_200.xlsx',index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PdIPO8szV5R0",
        "colab_type": "code",
        "outputId": "817d8e0a-b69b-442d-9b18-8d28afbd9fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(dt,tar,test_size=0.3)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(350, 200) (350, 1)\n",
            "(151, 200) (151, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v9fYZ73CV5Wd",
        "colab_type": "code",
        "outputId": "356cdfca-be26-482f-b583-cd7cc514262b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               25728     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 190,593\n",
            "Trainable params: 190,593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAqlR-fZV5ZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xby6goSfV5bt",
        "colab_type": "code",
        "outputId": "8ac3e944-864a-4493-b090-29765e508a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6902
        }
      },
      "cell_type": "code",
      "source": [
        "NN_model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split = 0.2,callbacks=callbacks_list) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 280 samples, validate on 70 samples\n",
            "Epoch 1/100\n",
            "280/280 [==============================] - 1s 3ms/step - loss: 25.2121 - mean_absolute_error: 25.2121 - val_loss: 20.7679 - val_mean_absolute_error: 20.7679\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 20.76786, saving model to Weights-001--20.76786.hdf5\n",
            "Epoch 2/100\n",
            "280/280 [==============================] - 0s 269us/step - loss: 20.8401 - mean_absolute_error: 20.8401 - val_loss: 20.9712 - val_mean_absolute_error: 20.9712\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 20.76786\n",
            "Epoch 3/100\n",
            "280/280 [==============================] - 0s 259us/step - loss: 20.3617 - mean_absolute_error: 20.3617 - val_loss: 20.4559 - val_mean_absolute_error: 20.4559\n",
            "\n",
            "Epoch 00003: val_loss improved from 20.76786 to 20.45592, saving model to Weights-003--20.45592.hdf5\n",
            "Epoch 4/100\n",
            "280/280 [==============================] - 0s 230us/step - loss: 19.6015 - mean_absolute_error: 19.6015 - val_loss: 20.7605 - val_mean_absolute_error: 20.7605\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.45592\n",
            "Epoch 5/100\n",
            "280/280 [==============================] - 0s 224us/step - loss: 19.2634 - mean_absolute_error: 19.2634 - val_loss: 20.6000 - val_mean_absolute_error: 20.6000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.45592\n",
            "Epoch 6/100\n",
            "280/280 [==============================] - 0s 224us/step - loss: 18.5004 - mean_absolute_error: 18.5004 - val_loss: 22.3677 - val_mean_absolute_error: 22.3677\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.45592\n",
            "Epoch 7/100\n",
            "280/280 [==============================] - 0s 223us/step - loss: 18.5338 - mean_absolute_error: 18.5338 - val_loss: 20.9682 - val_mean_absolute_error: 20.9682\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 20.45592\n",
            "Epoch 8/100\n",
            "280/280 [==============================] - 0s 256us/step - loss: 17.9773 - mean_absolute_error: 17.9773 - val_loss: 22.0222 - val_mean_absolute_error: 22.0222\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 20.45592\n",
            "Epoch 9/100\n",
            "280/280 [==============================] - 0s 239us/step - loss: 16.7255 - mean_absolute_error: 16.7255 - val_loss: 21.8224 - val_mean_absolute_error: 21.8224\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 20.45592\n",
            "Epoch 10/100\n",
            "280/280 [==============================] - 0s 229us/step - loss: 15.7957 - mean_absolute_error: 15.7957 - val_loss: 21.3764 - val_mean_absolute_error: 21.3764\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.45592\n",
            "Epoch 11/100\n",
            "280/280 [==============================] - 0s 243us/step - loss: 14.5090 - mean_absolute_error: 14.5090 - val_loss: 22.4353 - val_mean_absolute_error: 22.4353\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 20.45592\n",
            "Epoch 12/100\n",
            "280/280 [==============================] - 0s 250us/step - loss: 13.3131 - mean_absolute_error: 13.3131 - val_loss: 23.0809 - val_mean_absolute_error: 23.0809\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 20.45592\n",
            "Epoch 13/100\n",
            "280/280 [==============================] - 0s 238us/step - loss: 11.8675 - mean_absolute_error: 11.8675 - val_loss: 23.9132 - val_mean_absolute_error: 23.9132\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 20.45592\n",
            "Epoch 14/100\n",
            "280/280 [==============================] - 0s 244us/step - loss: 11.5291 - mean_absolute_error: 11.5291 - val_loss: 24.6346 - val_mean_absolute_error: 24.6346\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 20.45592\n",
            "Epoch 15/100\n",
            "280/280 [==============================] - 0s 242us/step - loss: 11.3133 - mean_absolute_error: 11.3133 - val_loss: 25.2254 - val_mean_absolute_error: 25.2254\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 20.45592\n",
            "Epoch 16/100\n",
            "280/280 [==============================] - 0s 223us/step - loss: 10.5159 - mean_absolute_error: 10.5159 - val_loss: 24.0274 - val_mean_absolute_error: 24.0274\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 20.45592\n",
            "Epoch 17/100\n",
            "280/280 [==============================] - 0s 237us/step - loss: 10.1326 - mean_absolute_error: 10.1326 - val_loss: 24.4121 - val_mean_absolute_error: 24.4121\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 20.45592\n",
            "Epoch 18/100\n",
            "280/280 [==============================] - 0s 255us/step - loss: 9.3165 - mean_absolute_error: 9.3165 - val_loss: 24.9559 - val_mean_absolute_error: 24.9559\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 20.45592\n",
            "Epoch 19/100\n",
            "280/280 [==============================] - 0s 221us/step - loss: 7.8093 - mean_absolute_error: 7.8093 - val_loss: 24.9749 - val_mean_absolute_error: 24.9749\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 20.45592\n",
            "Epoch 20/100\n",
            "280/280 [==============================] - 0s 221us/step - loss: 7.3225 - mean_absolute_error: 7.3225 - val_loss: 25.7387 - val_mean_absolute_error: 25.7387\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 20.45592\n",
            "Epoch 21/100\n",
            "280/280 [==============================] - 0s 242us/step - loss: 7.1156 - mean_absolute_error: 7.1156 - val_loss: 24.4811 - val_mean_absolute_error: 24.4811\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 20.45592\n",
            "Epoch 22/100\n",
            "280/280 [==============================] - 0s 235us/step - loss: 6.4869 - mean_absolute_error: 6.4869 - val_loss: 26.0387 - val_mean_absolute_error: 26.0387\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 20.45592\n",
            "Epoch 23/100\n",
            "280/280 [==============================] - 0s 235us/step - loss: 5.5640 - mean_absolute_error: 5.5640 - val_loss: 25.5249 - val_mean_absolute_error: 25.5249\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 20.45592\n",
            "Epoch 24/100\n",
            "280/280 [==============================] - 0s 247us/step - loss: 5.7564 - mean_absolute_error: 5.7564 - val_loss: 26.1417 - val_mean_absolute_error: 26.1417\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 20.45592\n",
            "Epoch 25/100\n",
            "280/280 [==============================] - 0s 248us/step - loss: 4.8051 - mean_absolute_error: 4.8051 - val_loss: 26.2089 - val_mean_absolute_error: 26.2089\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 20.45592\n",
            "Epoch 26/100\n",
            "280/280 [==============================] - 0s 239us/step - loss: 4.7008 - mean_absolute_error: 4.7008 - val_loss: 26.7689 - val_mean_absolute_error: 26.7689\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 20.45592\n",
            "Epoch 27/100\n",
            "280/280 [==============================] - 0s 251us/step - loss: 4.7244 - mean_absolute_error: 4.7244 - val_loss: 26.6869 - val_mean_absolute_error: 26.6869\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 20.45592\n",
            "Epoch 28/100\n",
            "280/280 [==============================] - 0s 233us/step - loss: 4.5193 - mean_absolute_error: 4.5193 - val_loss: 26.8532 - val_mean_absolute_error: 26.8532\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 20.45592\n",
            "Epoch 29/100\n",
            "280/280 [==============================] - 0s 233us/step - loss: 4.0271 - mean_absolute_error: 4.0271 - val_loss: 24.9666 - val_mean_absolute_error: 24.9666\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 20.45592\n",
            "Epoch 30/100\n",
            "280/280 [==============================] - 0s 227us/step - loss: 3.4925 - mean_absolute_error: 3.4925 - val_loss: 26.3439 - val_mean_absolute_error: 26.3439\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 20.45592\n",
            "Epoch 31/100\n",
            "280/280 [==============================] - 0s 237us/step - loss: 3.2597 - mean_absolute_error: 3.2597 - val_loss: 25.3291 - val_mean_absolute_error: 25.3291\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 20.45592\n",
            "Epoch 32/100\n",
            "280/280 [==============================] - 0s 289us/step - loss: 3.6568 - mean_absolute_error: 3.6568 - val_loss: 26.4108 - val_mean_absolute_error: 26.4108\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 20.45592\n",
            "Epoch 33/100\n",
            "280/280 [==============================] - 0s 251us/step - loss: 3.3395 - mean_absolute_error: 3.3395 - val_loss: 26.4569 - val_mean_absolute_error: 26.4569\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 20.45592\n",
            "Epoch 34/100\n",
            "280/280 [==============================] - 0s 238us/step - loss: 2.9095 - mean_absolute_error: 2.9095 - val_loss: 25.4847 - val_mean_absolute_error: 25.4847\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 20.45592\n",
            "Epoch 35/100\n",
            "280/280 [==============================] - 0s 235us/step - loss: 3.4127 - mean_absolute_error: 3.4127 - val_loss: 27.6787 - val_mean_absolute_error: 27.6787\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 20.45592\n",
            "Epoch 36/100\n",
            "280/280 [==============================] - 0s 233us/step - loss: 2.8158 - mean_absolute_error: 2.8158 - val_loss: 26.5085 - val_mean_absolute_error: 26.5085\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 20.45592\n",
            "Epoch 37/100\n",
            "280/280 [==============================] - 0s 245us/step - loss: 2.7826 - mean_absolute_error: 2.7826 - val_loss: 26.3213 - val_mean_absolute_error: 26.3213\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 20.45592\n",
            "Epoch 38/100\n",
            "280/280 [==============================] - 0s 244us/step - loss: 2.6070 - mean_absolute_error: 2.6070 - val_loss: 25.9951 - val_mean_absolute_error: 25.9951\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 20.45592\n",
            "Epoch 39/100\n",
            "280/280 [==============================] - 0s 241us/step - loss: 2.8056 - mean_absolute_error: 2.8056 - val_loss: 26.2577 - val_mean_absolute_error: 26.2577\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 20.45592\n",
            "Epoch 40/100\n",
            "280/280 [==============================] - 0s 225us/step - loss: 2.7252 - mean_absolute_error: 2.7252 - val_loss: 26.2114 - val_mean_absolute_error: 26.2114\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 20.45592\n",
            "Epoch 41/100\n",
            "280/280 [==============================] - 0s 251us/step - loss: 2.6600 - mean_absolute_error: 2.6600 - val_loss: 25.6970 - val_mean_absolute_error: 25.6970\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 20.45592\n",
            "Epoch 42/100\n",
            "280/280 [==============================] - 0s 221us/step - loss: 2.2723 - mean_absolute_error: 2.2723 - val_loss: 27.2576 - val_mean_absolute_error: 27.2576\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 20.45592\n",
            "Epoch 43/100\n",
            "280/280 [==============================] - 0s 244us/step - loss: 2.5069 - mean_absolute_error: 2.5069 - val_loss: 25.8815 - val_mean_absolute_error: 25.8815\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 20.45592\n",
            "Epoch 44/100\n",
            "280/280 [==============================] - 0s 223us/step - loss: 2.1245 - mean_absolute_error: 2.1245 - val_loss: 26.5093 - val_mean_absolute_error: 26.5093\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 20.45592\n",
            "Epoch 45/100\n",
            "280/280 [==============================] - 0s 241us/step - loss: 2.1614 - mean_absolute_error: 2.1614 - val_loss: 25.4343 - val_mean_absolute_error: 25.4343\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 20.45592\n",
            "Epoch 46/100\n",
            "280/280 [==============================] - 0s 246us/step - loss: 2.0685 - mean_absolute_error: 2.0685 - val_loss: 26.3938 - val_mean_absolute_error: 26.3938\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 20.45592\n",
            "Epoch 47/100\n",
            "280/280 [==============================] - 0s 229us/step - loss: 2.1193 - mean_absolute_error: 2.1193 - val_loss: 27.2343 - val_mean_absolute_error: 27.2343\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 20.45592\n",
            "Epoch 48/100\n",
            "280/280 [==============================] - 0s 236us/step - loss: 2.5846 - mean_absolute_error: 2.5846 - val_loss: 26.8185 - val_mean_absolute_error: 26.8185\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 20.45592\n",
            "Epoch 49/100\n",
            "280/280 [==============================] - 0s 232us/step - loss: 2.3816 - mean_absolute_error: 2.3816 - val_loss: 26.1675 - val_mean_absolute_error: 26.1675\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 20.45592\n",
            "Epoch 50/100\n",
            "280/280 [==============================] - 0s 226us/step - loss: 2.2104 - mean_absolute_error: 2.2104 - val_loss: 25.9420 - val_mean_absolute_error: 25.9420\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 20.45592\n",
            "Epoch 51/100\n",
            "280/280 [==============================] - 0s 242us/step - loss: 2.1985 - mean_absolute_error: 2.1985 - val_loss: 26.5191 - val_mean_absolute_error: 26.5191\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 20.45592\n",
            "Epoch 52/100\n",
            "280/280 [==============================] - 0s 236us/step - loss: 2.2706 - mean_absolute_error: 2.2706 - val_loss: 25.5436 - val_mean_absolute_error: 25.5436\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 20.45592\n",
            "Epoch 53/100\n",
            "280/280 [==============================] - 0s 216us/step - loss: 2.3076 - mean_absolute_error: 2.3076 - val_loss: 24.6435 - val_mean_absolute_error: 24.6435\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 20.45592\n",
            "Epoch 54/100\n",
            "280/280 [==============================] - 0s 262us/step - loss: 2.1073 - mean_absolute_error: 2.1073 - val_loss: 26.1837 - val_mean_absolute_error: 26.1837\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 20.45592\n",
            "Epoch 55/100\n",
            "280/280 [==============================] - 0s 237us/step - loss: 1.7931 - mean_absolute_error: 1.7931 - val_loss: 26.2240 - val_mean_absolute_error: 26.2240\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 20.45592\n",
            "Epoch 56/100\n",
            "280/280 [==============================] - 0s 224us/step - loss: 1.9355 - mean_absolute_error: 1.9355 - val_loss: 25.2016 - val_mean_absolute_error: 25.2016\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 20.45592\n",
            "Epoch 57/100\n",
            "280/280 [==============================] - 0s 232us/step - loss: 1.8152 - mean_absolute_error: 1.8152 - val_loss: 25.0613 - val_mean_absolute_error: 25.0613\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 20.45592\n",
            "Epoch 58/100\n",
            "280/280 [==============================] - 0s 245us/step - loss: 1.7772 - mean_absolute_error: 1.7772 - val_loss: 25.9742 - val_mean_absolute_error: 25.9742\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 20.45592\n",
            "Epoch 59/100\n",
            "280/280 [==============================] - 0s 227us/step - loss: 1.5486 - mean_absolute_error: 1.5486 - val_loss: 26.1187 - val_mean_absolute_error: 26.1187\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 20.45592\n",
            "Epoch 60/100\n",
            "280/280 [==============================] - 0s 236us/step - loss: 1.4677 - mean_absolute_error: 1.4677 - val_loss: 26.2438 - val_mean_absolute_error: 26.2438\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 20.45592\n",
            "Epoch 61/100\n",
            "280/280 [==============================] - 0s 228us/step - loss: 1.3416 - mean_absolute_error: 1.3416 - val_loss: 26.1173 - val_mean_absolute_error: 26.1173\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 20.45592\n",
            "Epoch 62/100\n",
            "280/280 [==============================] - 0s 254us/step - loss: 1.5666 - mean_absolute_error: 1.5666 - val_loss: 25.5149 - val_mean_absolute_error: 25.5149\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 20.45592\n",
            "Epoch 63/100\n",
            "280/280 [==============================] - 0s 245us/step - loss: 1.4809 - mean_absolute_error: 1.4809 - val_loss: 26.0758 - val_mean_absolute_error: 26.0758\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 20.45592\n",
            "Epoch 64/100\n",
            "280/280 [==============================] - 0s 233us/step - loss: 1.8653 - mean_absolute_error: 1.8653 - val_loss: 25.4594 - val_mean_absolute_error: 25.4594\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 20.45592\n",
            "Epoch 65/100\n",
            "280/280 [==============================] - 0s 231us/step - loss: 2.2307 - mean_absolute_error: 2.2307 - val_loss: 25.2132 - val_mean_absolute_error: 25.2132\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 20.45592\n",
            "Epoch 66/100\n",
            "280/280 [==============================] - 0s 233us/step - loss: 1.9801 - mean_absolute_error: 1.9801 - val_loss: 26.0206 - val_mean_absolute_error: 26.0206\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 20.45592\n",
            "Epoch 67/100\n",
            "280/280 [==============================] - 0s 222us/step - loss: 2.1568 - mean_absolute_error: 2.1568 - val_loss: 27.2173 - val_mean_absolute_error: 27.2173\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 20.45592\n",
            "Epoch 68/100\n",
            "280/280 [==============================] - 0s 265us/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 26.9884 - val_mean_absolute_error: 26.9884\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 20.45592\n",
            "Epoch 69/100\n",
            "280/280 [==============================] - 0s 259us/step - loss: 1.7531 - mean_absolute_error: 1.7531 - val_loss: 25.7246 - val_mean_absolute_error: 25.7246\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 20.45592\n",
            "Epoch 70/100\n",
            "280/280 [==============================] - 0s 237us/step - loss: 1.8202 - mean_absolute_error: 1.8202 - val_loss: 25.1182 - val_mean_absolute_error: 25.1182\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 20.45592\n",
            "Epoch 71/100\n",
            "280/280 [==============================] - 0s 224us/step - loss: 2.3377 - mean_absolute_error: 2.3377 - val_loss: 26.4316 - val_mean_absolute_error: 26.4316\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 20.45592\n",
            "Epoch 72/100\n",
            "280/280 [==============================] - 0s 237us/step - loss: 2.1868 - mean_absolute_error: 2.1868 - val_loss: 26.7959 - val_mean_absolute_error: 26.7959\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 20.45592\n",
            "Epoch 73/100\n",
            "280/280 [==============================] - 0s 246us/step - loss: 2.3532 - mean_absolute_error: 2.3532 - val_loss: 26.3599 - val_mean_absolute_error: 26.3599\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 20.45592\n",
            "Epoch 74/100\n",
            "280/280 [==============================] - 0s 224us/step - loss: 1.9269 - mean_absolute_error: 1.9269 - val_loss: 25.8699 - val_mean_absolute_error: 25.8699\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 20.45592\n",
            "Epoch 75/100\n",
            "280/280 [==============================] - 0s 226us/step - loss: 1.6381 - mean_absolute_error: 1.6381 - val_loss: 26.0042 - val_mean_absolute_error: 26.0042\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 20.45592\n",
            "Epoch 76/100\n",
            "280/280 [==============================] - 0s 260us/step - loss: 1.7374 - mean_absolute_error: 1.7374 - val_loss: 25.5380 - val_mean_absolute_error: 25.5380\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 20.45592\n",
            "Epoch 77/100\n",
            "280/280 [==============================] - 0s 220us/step - loss: 1.2830 - mean_absolute_error: 1.2830 - val_loss: 26.1267 - val_mean_absolute_error: 26.1267\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 20.45592\n",
            "Epoch 78/100\n",
            "280/280 [==============================] - 0s 221us/step - loss: 1.4811 - mean_absolute_error: 1.4811 - val_loss: 25.3958 - val_mean_absolute_error: 25.3958\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 20.45592\n",
            "Epoch 79/100\n",
            "280/280 [==============================] - 0s 226us/step - loss: 1.6469 - mean_absolute_error: 1.6469 - val_loss: 26.7608 - val_mean_absolute_error: 26.7608\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 20.45592\n",
            "Epoch 80/100\n",
            "280/280 [==============================] - 0s 232us/step - loss: 1.5764 - mean_absolute_error: 1.5764 - val_loss: 25.1915 - val_mean_absolute_error: 25.1915\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 20.45592\n",
            "Epoch 81/100\n",
            "280/280 [==============================] - 0s 226us/step - loss: 1.9399 - mean_absolute_error: 1.9399 - val_loss: 25.0446 - val_mean_absolute_error: 25.0446\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 20.45592\n",
            "Epoch 82/100\n",
            "280/280 [==============================] - 0s 244us/step - loss: 1.7060 - mean_absolute_error: 1.7060 - val_loss: 25.9765 - val_mean_absolute_error: 25.9765\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 20.45592\n",
            "Epoch 83/100\n",
            "280/280 [==============================] - 0s 230us/step - loss: 1.5374 - mean_absolute_error: 1.5374 - val_loss: 26.3176 - val_mean_absolute_error: 26.3176\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 20.45592\n",
            "Epoch 84/100\n",
            "280/280 [==============================] - 0s 221us/step - loss: 1.5298 - mean_absolute_error: 1.5298 - val_loss: 25.4061 - val_mean_absolute_error: 25.4061\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 20.45592\n",
            "Epoch 85/100\n",
            "280/280 [==============================] - 0s 224us/step - loss: 1.5321 - mean_absolute_error: 1.5321 - val_loss: 25.9379 - val_mean_absolute_error: 25.9379\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 20.45592\n",
            "Epoch 86/100\n",
            "280/280 [==============================] - 0s 248us/step - loss: 1.2319 - mean_absolute_error: 1.2319 - val_loss: 26.3409 - val_mean_absolute_error: 26.3409\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 20.45592\n",
            "Epoch 87/100\n",
            "280/280 [==============================] - 0s 233us/step - loss: 1.4254 - mean_absolute_error: 1.4254 - val_loss: 25.6784 - val_mean_absolute_error: 25.6784\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 20.45592\n",
            "Epoch 88/100\n",
            "280/280 [==============================] - 0s 239us/step - loss: 1.4722 - mean_absolute_error: 1.4722 - val_loss: 26.3443 - val_mean_absolute_error: 26.3443\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 20.45592\n",
            "Epoch 89/100\n",
            "280/280 [==============================] - 0s 248us/step - loss: 1.5331 - mean_absolute_error: 1.5331 - val_loss: 25.6535 - val_mean_absolute_error: 25.6535\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 20.45592\n",
            "Epoch 90/100\n",
            "280/280 [==============================] - 0s 238us/step - loss: 1.5627 - mean_absolute_error: 1.5627 - val_loss: 24.7095 - val_mean_absolute_error: 24.7095\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 20.45592\n",
            "Epoch 91/100\n",
            "280/280 [==============================] - 0s 258us/step - loss: 2.0014 - mean_absolute_error: 2.0014 - val_loss: 25.2775 - val_mean_absolute_error: 25.2775\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 20.45592\n",
            "Epoch 92/100\n",
            "280/280 [==============================] - 0s 240us/step - loss: 2.0382 - mean_absolute_error: 2.0382 - val_loss: 26.3949 - val_mean_absolute_error: 26.3949\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 20.45592\n",
            "Epoch 93/100\n",
            "280/280 [==============================] - 0s 218us/step - loss: 1.6281 - mean_absolute_error: 1.6281 - val_loss: 26.2264 - val_mean_absolute_error: 26.2264\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 20.45592\n",
            "Epoch 94/100\n",
            "280/280 [==============================] - 0s 238us/step - loss: 2.0410 - mean_absolute_error: 2.0410 - val_loss: 24.7508 - val_mean_absolute_error: 24.7508\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 20.45592\n",
            "Epoch 95/100\n",
            "280/280 [==============================] - 0s 237us/step - loss: 3.0880 - mean_absolute_error: 3.0880 - val_loss: 25.2356 - val_mean_absolute_error: 25.2356\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 20.45592\n",
            "Epoch 96/100\n",
            "280/280 [==============================] - 0s 229us/step - loss: 2.2501 - mean_absolute_error: 2.2501 - val_loss: 25.7165 - val_mean_absolute_error: 25.7165\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 20.45592\n",
            "Epoch 97/100\n",
            "280/280 [==============================] - 0s 224us/step - loss: 1.6105 - mean_absolute_error: 1.6105 - val_loss: 25.4275 - val_mean_absolute_error: 25.4275\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 20.45592\n",
            "Epoch 98/100\n",
            "280/280 [==============================] - 0s 239us/step - loss: 1.6436 - mean_absolute_error: 1.6436 - val_loss: 25.7516 - val_mean_absolute_error: 25.7516\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 20.45592\n",
            "Epoch 99/100\n",
            "280/280 [==============================] - 0s 250us/step - loss: 1.3866 - mean_absolute_error: 1.3866 - val_loss: 26.1050 - val_mean_absolute_error: 26.1050\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 20.45592\n",
            "Epoch 100/100\n",
            "280/280 [==============================] - 0s 237us/step - loss: 1.2295 - mean_absolute_error: 1.2295 - val_loss: 25.8365 - val_mean_absolute_error: 25.8365\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 20.45592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4d4431d7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "qThnHL8aV5eN",
        "colab_type": "code",
        "outputId": "ea87a239-7ebe-47bb-ab4d-3d84bc444b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = NN_model.predict(X_train)\n",
        "from sklearn.metrics import r2_score\n",
        "accuracy=r2_score(Y_train,predictions)\n",
        "print('train')\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "0.7278526290112686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DaJ58U7bV5f-",
        "colab_type": "code",
        "outputId": "6aecc910-07d1-4f3d-8725-bd910a4c46b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6851
        }
      },
      "cell_type": "code",
      "source": [
        "NN_model.fit(X_test, Y_test, epochs=100, batch_size=32, validation_split = 0.3,callbacks=callbacks_list) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 46 samples\n",
            "Epoch 1/100\n",
            "105/105 [==============================] - 0s 363us/step - loss: 1.6001 - mean_absolute_error: 1.6001 - val_loss: 29.4068 - val_mean_absolute_error: 29.4068\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 20.45592\n",
            "Epoch 2/100\n",
            "105/105 [==============================] - 0s 308us/step - loss: 1.4234 - mean_absolute_error: 1.4234 - val_loss: 28.7539 - val_mean_absolute_error: 28.7539\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 20.45592\n",
            "Epoch 3/100\n",
            "105/105 [==============================] - 0s 304us/step - loss: 1.8306 - mean_absolute_error: 1.8306 - val_loss: 28.7996 - val_mean_absolute_error: 28.7996\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 20.45592\n",
            "Epoch 4/100\n",
            "105/105 [==============================] - 0s 270us/step - loss: 1.6489 - mean_absolute_error: 1.6489 - val_loss: 28.5760 - val_mean_absolute_error: 28.5760\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 20.45592\n",
            "Epoch 5/100\n",
            "105/105 [==============================] - 0s 274us/step - loss: 2.2946 - mean_absolute_error: 2.2946 - val_loss: 28.7037 - val_mean_absolute_error: 28.7037\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 20.45592\n",
            "Epoch 6/100\n",
            "105/105 [==============================] - 0s 263us/step - loss: 1.8965 - mean_absolute_error: 1.8965 - val_loss: 28.5667 - val_mean_absolute_error: 28.5667\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 20.45592\n",
            "Epoch 7/100\n",
            "105/105 [==============================] - 0s 279us/step - loss: 2.9045 - mean_absolute_error: 2.9045 - val_loss: 29.2664 - val_mean_absolute_error: 29.2664\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 20.45592\n",
            "Epoch 8/100\n",
            "105/105 [==============================] - 0s 309us/step - loss: 3.5461 - mean_absolute_error: 3.5461 - val_loss: 29.0364 - val_mean_absolute_error: 29.0364\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 20.45592\n",
            "Epoch 9/100\n",
            "105/105 [==============================] - 0s 346us/step - loss: 2.1756 - mean_absolute_error: 2.1756 - val_loss: 29.0831 - val_mean_absolute_error: 29.0831\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 20.45592\n",
            "Epoch 10/100\n",
            "105/105 [==============================] - 0s 313us/step - loss: 1.7909 - mean_absolute_error: 1.7909 - val_loss: 28.8284 - val_mean_absolute_error: 28.8284\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 20.45592\n",
            "Epoch 11/100\n",
            "105/105 [==============================] - 0s 340us/step - loss: 2.3951 - mean_absolute_error: 2.3951 - val_loss: 28.7096 - val_mean_absolute_error: 28.7096\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 20.45592\n",
            "Epoch 12/100\n",
            "105/105 [==============================] - 0s 306us/step - loss: 2.1319 - mean_absolute_error: 2.1319 - val_loss: 28.5846 - val_mean_absolute_error: 28.5846\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 20.45592\n",
            "Epoch 13/100\n",
            "105/105 [==============================] - 0s 339us/step - loss: 1.6592 - mean_absolute_error: 1.6592 - val_loss: 29.2020 - val_mean_absolute_error: 29.2020\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 20.45592\n",
            "Epoch 14/100\n",
            "105/105 [==============================] - 0s 288us/step - loss: 1.8835 - mean_absolute_error: 1.8835 - val_loss: 28.9305 - val_mean_absolute_error: 28.9305\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 20.45592\n",
            "Epoch 15/100\n",
            "105/105 [==============================] - 0s 333us/step - loss: 1.8433 - mean_absolute_error: 1.8433 - val_loss: 29.3409 - val_mean_absolute_error: 29.3409\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 20.45592\n",
            "Epoch 16/100\n",
            "105/105 [==============================] - 0s 268us/step - loss: 2.3944 - mean_absolute_error: 2.3944 - val_loss: 28.8201 - val_mean_absolute_error: 28.8201\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 20.45592\n",
            "Epoch 17/100\n",
            "105/105 [==============================] - 0s 309us/step - loss: 3.4276 - mean_absolute_error: 3.4276 - val_loss: 29.3702 - val_mean_absolute_error: 29.3702\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 20.45592\n",
            "Epoch 18/100\n",
            "105/105 [==============================] - 0s 278us/step - loss: 4.0787 - mean_absolute_error: 4.0787 - val_loss: 29.0909 - val_mean_absolute_error: 29.0909\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 20.45592\n",
            "Epoch 19/100\n",
            "105/105 [==============================] - 0s 331us/step - loss: 3.0134 - mean_absolute_error: 3.0134 - val_loss: 28.8997 - val_mean_absolute_error: 28.8997\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 20.45592\n",
            "Epoch 20/100\n",
            "105/105 [==============================] - 0s 310us/step - loss: 2.4957 - mean_absolute_error: 2.4957 - val_loss: 29.4228 - val_mean_absolute_error: 29.4228\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 20.45592\n",
            "Epoch 21/100\n",
            "105/105 [==============================] - 0s 312us/step - loss: 2.6741 - mean_absolute_error: 2.6741 - val_loss: 29.0111 - val_mean_absolute_error: 29.0111\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 20.45592\n",
            "Epoch 22/100\n",
            "105/105 [==============================] - 0s 380us/step - loss: 2.6837 - mean_absolute_error: 2.6837 - val_loss: 29.0763 - val_mean_absolute_error: 29.0763\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 20.45592\n",
            "Epoch 23/100\n",
            "105/105 [==============================] - 0s 301us/step - loss: 2.8360 - mean_absolute_error: 2.8360 - val_loss: 28.9942 - val_mean_absolute_error: 28.9942\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 20.45592\n",
            "Epoch 24/100\n",
            "105/105 [==============================] - 0s 300us/step - loss: 2.0303 - mean_absolute_error: 2.0303 - val_loss: 29.3016 - val_mean_absolute_error: 29.3016\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 20.45592\n",
            "Epoch 25/100\n",
            "105/105 [==============================] - 0s 322us/step - loss: 1.7636 - mean_absolute_error: 1.7636 - val_loss: 29.0399 - val_mean_absolute_error: 29.0399\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 20.45592\n",
            "Epoch 26/100\n",
            "105/105 [==============================] - 0s 288us/step - loss: 1.6170 - mean_absolute_error: 1.6170 - val_loss: 28.9894 - val_mean_absolute_error: 28.9894\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 20.45592\n",
            "Epoch 27/100\n",
            "105/105 [==============================] - 0s 305us/step - loss: 1.8629 - mean_absolute_error: 1.8629 - val_loss: 29.2020 - val_mean_absolute_error: 29.2020\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 20.45592\n",
            "Epoch 28/100\n",
            "105/105 [==============================] - 0s 332us/step - loss: 1.6918 - mean_absolute_error: 1.6918 - val_loss: 29.1837 - val_mean_absolute_error: 29.1837\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 20.45592\n",
            "Epoch 29/100\n",
            "105/105 [==============================] - 0s 307us/step - loss: 1.4079 - mean_absolute_error: 1.4079 - val_loss: 29.0071 - val_mean_absolute_error: 29.0071\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 20.45592\n",
            "Epoch 30/100\n",
            "105/105 [==============================] - 0s 298us/step - loss: 1.5333 - mean_absolute_error: 1.5333 - val_loss: 29.0188 - val_mean_absolute_error: 29.0188\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 20.45592\n",
            "Epoch 31/100\n",
            "105/105 [==============================] - 0s 342us/step - loss: 1.0927 - mean_absolute_error: 1.0927 - val_loss: 28.9426 - val_mean_absolute_error: 28.9426\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 20.45592\n",
            "Epoch 32/100\n",
            "105/105 [==============================] - 0s 283us/step - loss: 1.3087 - mean_absolute_error: 1.3087 - val_loss: 28.9036 - val_mean_absolute_error: 28.9036\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 20.45592\n",
            "Epoch 33/100\n",
            "105/105 [==============================] - 0s 293us/step - loss: 1.1040 - mean_absolute_error: 1.1040 - val_loss: 28.8482 - val_mean_absolute_error: 28.8482\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 20.45592\n",
            "Epoch 34/100\n",
            "105/105 [==============================] - 0s 358us/step - loss: 1.4320 - mean_absolute_error: 1.4320 - val_loss: 29.0714 - val_mean_absolute_error: 29.0714\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 20.45592\n",
            "Epoch 35/100\n",
            "105/105 [==============================] - 0s 291us/step - loss: 1.5240 - mean_absolute_error: 1.5240 - val_loss: 29.5796 - val_mean_absolute_error: 29.5796\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 20.45592\n",
            "Epoch 36/100\n",
            "105/105 [==============================] - 0s 283us/step - loss: 2.2625 - mean_absolute_error: 2.2625 - val_loss: 29.2086 - val_mean_absolute_error: 29.2086\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 20.45592\n",
            "Epoch 37/100\n",
            "105/105 [==============================] - 0s 303us/step - loss: 1.9490 - mean_absolute_error: 1.9490 - val_loss: 29.3888 - val_mean_absolute_error: 29.3888\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 20.45592\n",
            "Epoch 38/100\n",
            "105/105 [==============================] - 0s 312us/step - loss: 1.8684 - mean_absolute_error: 1.8684 - val_loss: 28.8560 - val_mean_absolute_error: 28.8560\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 20.45592\n",
            "Epoch 39/100\n",
            "105/105 [==============================] - 0s 282us/step - loss: 2.2601 - mean_absolute_error: 2.2601 - val_loss: 29.4391 - val_mean_absolute_error: 29.4391\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 20.45592\n",
            "Epoch 40/100\n",
            "105/105 [==============================] - 0s 299us/step - loss: 3.2842 - mean_absolute_error: 3.2842 - val_loss: 28.9121 - val_mean_absolute_error: 28.9121\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 20.45592\n",
            "Epoch 41/100\n",
            "105/105 [==============================] - 0s 312us/step - loss: 2.6612 - mean_absolute_error: 2.6612 - val_loss: 28.7005 - val_mean_absolute_error: 28.7005\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 20.45592\n",
            "Epoch 42/100\n",
            "105/105 [==============================] - 0s 327us/step - loss: 1.9115 - mean_absolute_error: 1.9115 - val_loss: 28.7903 - val_mean_absolute_error: 28.7903\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 20.45592\n",
            "Epoch 43/100\n",
            "105/105 [==============================] - 0s 292us/step - loss: 1.4954 - mean_absolute_error: 1.4954 - val_loss: 28.5693 - val_mean_absolute_error: 28.5693\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 20.45592\n",
            "Epoch 44/100\n",
            "105/105 [==============================] - 0s 307us/step - loss: 1.5158 - mean_absolute_error: 1.5158 - val_loss: 28.6282 - val_mean_absolute_error: 28.6282\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 20.45592\n",
            "Epoch 45/100\n",
            "105/105 [==============================] - 0s 287us/step - loss: 1.4358 - mean_absolute_error: 1.4358 - val_loss: 28.7313 - val_mean_absolute_error: 28.7313\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 20.45592\n",
            "Epoch 46/100\n",
            "105/105 [==============================] - 0s 315us/step - loss: 1.4160 - mean_absolute_error: 1.4160 - val_loss: 28.9085 - val_mean_absolute_error: 28.9085\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 20.45592\n",
            "Epoch 47/100\n",
            "105/105 [==============================] - 0s 327us/step - loss: 1.0896 - mean_absolute_error: 1.0896 - val_loss: 28.8139 - val_mean_absolute_error: 28.8139\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 20.45592\n",
            "Epoch 48/100\n",
            "105/105 [==============================] - 0s 312us/step - loss: 1.1950 - mean_absolute_error: 1.1950 - val_loss: 28.9002 - val_mean_absolute_error: 28.9002\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 20.45592\n",
            "Epoch 49/100\n",
            "105/105 [==============================] - 0s 308us/step - loss: 1.0595 - mean_absolute_error: 1.0595 - val_loss: 28.8559 - val_mean_absolute_error: 28.8559\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 20.45592\n",
            "Epoch 50/100\n",
            "105/105 [==============================] - 0s 311us/step - loss: 1.1185 - mean_absolute_error: 1.1185 - val_loss: 29.1018 - val_mean_absolute_error: 29.1018\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 20.45592\n",
            "Epoch 51/100\n",
            "105/105 [==============================] - 0s 266us/step - loss: 1.3832 - mean_absolute_error: 1.3832 - val_loss: 29.1017 - val_mean_absolute_error: 29.1017\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 20.45592\n",
            "Epoch 52/100\n",
            "105/105 [==============================] - 0s 314us/step - loss: 1.4280 - mean_absolute_error: 1.4280 - val_loss: 29.0086 - val_mean_absolute_error: 29.0086\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 20.45592\n",
            "Epoch 53/100\n",
            "105/105 [==============================] - 0s 285us/step - loss: 1.4235 - mean_absolute_error: 1.4235 - val_loss: 28.9321 - val_mean_absolute_error: 28.9321\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 20.45592\n",
            "Epoch 54/100\n",
            "105/105 [==============================] - 0s 325us/step - loss: 1.9252 - mean_absolute_error: 1.9252 - val_loss: 28.4643 - val_mean_absolute_error: 28.4643\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 20.45592\n",
            "Epoch 55/100\n",
            "105/105 [==============================] - 0s 314us/step - loss: 1.5273 - mean_absolute_error: 1.5273 - val_loss: 28.4840 - val_mean_absolute_error: 28.4840\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 20.45592\n",
            "Epoch 56/100\n",
            "105/105 [==============================] - 0s 317us/step - loss: 1.3042 - mean_absolute_error: 1.3042 - val_loss: 28.3968 - val_mean_absolute_error: 28.3968\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 20.45592\n",
            "Epoch 57/100\n",
            "105/105 [==============================] - 0s 269us/step - loss: 1.2581 - mean_absolute_error: 1.2581 - val_loss: 29.1339 - val_mean_absolute_error: 29.1339\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 20.45592\n",
            "Epoch 58/100\n",
            "105/105 [==============================] - 0s 322us/step - loss: 1.7293 - mean_absolute_error: 1.7293 - val_loss: 28.7225 - val_mean_absolute_error: 28.7225\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 20.45592\n",
            "Epoch 59/100\n",
            "105/105 [==============================] - 0s 309us/step - loss: 1.7172 - mean_absolute_error: 1.7172 - val_loss: 29.0172 - val_mean_absolute_error: 29.0172\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 20.45592\n",
            "Epoch 60/100\n",
            "105/105 [==============================] - 0s 352us/step - loss: 1.2896 - mean_absolute_error: 1.2896 - val_loss: 28.7336 - val_mean_absolute_error: 28.7336\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 20.45592\n",
            "Epoch 61/100\n",
            "105/105 [==============================] - 0s 317us/step - loss: 1.3921 - mean_absolute_error: 1.3921 - val_loss: 28.9508 - val_mean_absolute_error: 28.9508\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 20.45592\n",
            "Epoch 62/100\n",
            "105/105 [==============================] - 0s 311us/step - loss: 1.6519 - mean_absolute_error: 1.6519 - val_loss: 28.7363 - val_mean_absolute_error: 28.7363\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 20.45592\n",
            "Epoch 63/100\n",
            "105/105 [==============================] - 0s 292us/step - loss: 1.6017 - mean_absolute_error: 1.6017 - val_loss: 29.0137 - val_mean_absolute_error: 29.0137\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 20.45592\n",
            "Epoch 64/100\n",
            "105/105 [==============================] - 0s 324us/step - loss: 1.3509 - mean_absolute_error: 1.3509 - val_loss: 29.0131 - val_mean_absolute_error: 29.0131\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 20.45592\n",
            "Epoch 65/100\n",
            "105/105 [==============================] - 0s 305us/step - loss: 1.3431 - mean_absolute_error: 1.3431 - val_loss: 28.9626 - val_mean_absolute_error: 28.9626\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 20.45592\n",
            "Epoch 66/100\n",
            "105/105 [==============================] - 0s 296us/step - loss: 1.5013 - mean_absolute_error: 1.5013 - val_loss: 29.0439 - val_mean_absolute_error: 29.0439\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 20.45592\n",
            "Epoch 67/100\n",
            "105/105 [==============================] - 0s 320us/step - loss: 1.3721 - mean_absolute_error: 1.3721 - val_loss: 29.1122 - val_mean_absolute_error: 29.1122\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 20.45592\n",
            "Epoch 68/100\n",
            "105/105 [==============================] - 0s 303us/step - loss: 1.0810 - mean_absolute_error: 1.0810 - val_loss: 29.6378 - val_mean_absolute_error: 29.6378\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 20.45592\n",
            "Epoch 69/100\n",
            "105/105 [==============================] - 0s 346us/step - loss: 1.4576 - mean_absolute_error: 1.4576 - val_loss: 28.9563 - val_mean_absolute_error: 28.9563\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 20.45592\n",
            "Epoch 70/100\n",
            "105/105 [==============================] - 0s 325us/step - loss: 2.3062 - mean_absolute_error: 2.3062 - val_loss: 29.1708 - val_mean_absolute_error: 29.1708\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 20.45592\n",
            "Epoch 71/100\n",
            "105/105 [==============================] - 0s 334us/step - loss: 1.8423 - mean_absolute_error: 1.8423 - val_loss: 28.7964 - val_mean_absolute_error: 28.7964\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 20.45592\n",
            "Epoch 72/100\n",
            "105/105 [==============================] - 0s 290us/step - loss: 1.6166 - mean_absolute_error: 1.6166 - val_loss: 29.5060 - val_mean_absolute_error: 29.5060\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 20.45592\n",
            "Epoch 73/100\n",
            "105/105 [==============================] - 0s 276us/step - loss: 1.7918 - mean_absolute_error: 1.7918 - val_loss: 28.9582 - val_mean_absolute_error: 28.9582\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 20.45592\n",
            "Epoch 74/100\n",
            "105/105 [==============================] - 0s 301us/step - loss: 1.7268 - mean_absolute_error: 1.7268 - val_loss: 29.4085 - val_mean_absolute_error: 29.4085\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 20.45592\n",
            "Epoch 75/100\n",
            "105/105 [==============================] - 0s 302us/step - loss: 2.1912 - mean_absolute_error: 2.1912 - val_loss: 28.7102 - val_mean_absolute_error: 28.7102\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 20.45592\n",
            "Epoch 76/100\n",
            "105/105 [==============================] - 0s 272us/step - loss: 2.5738 - mean_absolute_error: 2.5738 - val_loss: 28.9971 - val_mean_absolute_error: 28.9971\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 20.45592\n",
            "Epoch 77/100\n",
            "105/105 [==============================] - 0s 308us/step - loss: 1.9113 - mean_absolute_error: 1.9113 - val_loss: 28.6633 - val_mean_absolute_error: 28.6633\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 20.45592\n",
            "Epoch 78/100\n",
            "105/105 [==============================] - 0s 297us/step - loss: 2.0527 - mean_absolute_error: 2.0527 - val_loss: 29.3200 - val_mean_absolute_error: 29.3200\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 20.45592\n",
            "Epoch 79/100\n",
            "105/105 [==============================] - 0s 318us/step - loss: 1.5874 - mean_absolute_error: 1.5874 - val_loss: 29.0754 - val_mean_absolute_error: 29.0754\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 20.45592\n",
            "Epoch 80/100\n",
            "105/105 [==============================] - 0s 302us/step - loss: 1.9117 - mean_absolute_error: 1.9117 - val_loss: 29.5022 - val_mean_absolute_error: 29.5022\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 20.45592\n",
            "Epoch 81/100\n",
            "105/105 [==============================] - 0s 352us/step - loss: 1.6248 - mean_absolute_error: 1.6248 - val_loss: 28.9827 - val_mean_absolute_error: 28.9827\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 20.45592\n",
            "Epoch 82/100\n",
            "105/105 [==============================] - 0s 282us/step - loss: 1.2472 - mean_absolute_error: 1.2472 - val_loss: 29.0400 - val_mean_absolute_error: 29.0400\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 20.45592\n",
            "Epoch 83/100\n",
            "105/105 [==============================] - 0s 341us/step - loss: 1.1921 - mean_absolute_error: 1.1921 - val_loss: 29.1000 - val_mean_absolute_error: 29.1000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 20.45592\n",
            "Epoch 84/100\n",
            "105/105 [==============================] - 0s 329us/step - loss: 1.1154 - mean_absolute_error: 1.1154 - val_loss: 28.8642 - val_mean_absolute_error: 28.8642\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 20.45592\n",
            "Epoch 85/100\n",
            "105/105 [==============================] - 0s 318us/step - loss: 1.1594 - mean_absolute_error: 1.1594 - val_loss: 29.1687 - val_mean_absolute_error: 29.1687\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 20.45592\n",
            "Epoch 86/100\n",
            "105/105 [==============================] - 0s 326us/step - loss: 1.2931 - mean_absolute_error: 1.2931 - val_loss: 29.2405 - val_mean_absolute_error: 29.2405\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 20.45592\n",
            "Epoch 87/100\n",
            "105/105 [==============================] - 0s 312us/step - loss: 1.1115 - mean_absolute_error: 1.1115 - val_loss: 28.9904 - val_mean_absolute_error: 28.9904\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 20.45592\n",
            "Epoch 88/100\n",
            "105/105 [==============================] - 0s 277us/step - loss: 1.0403 - mean_absolute_error: 1.0403 - val_loss: 28.8503 - val_mean_absolute_error: 28.8503\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 20.45592\n",
            "Epoch 89/100\n",
            "105/105 [==============================] - 0s 362us/step - loss: 1.0392 - mean_absolute_error: 1.0392 - val_loss: 28.7667 - val_mean_absolute_error: 28.7667\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 20.45592\n",
            "Epoch 90/100\n",
            "105/105 [==============================] - 0s 281us/step - loss: 1.0514 - mean_absolute_error: 1.0514 - val_loss: 28.7591 - val_mean_absolute_error: 28.7591\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 20.45592\n",
            "Epoch 91/100\n",
            "105/105 [==============================] - 0s 327us/step - loss: 1.2720 - mean_absolute_error: 1.2720 - val_loss: 29.0945 - val_mean_absolute_error: 29.0945\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 20.45592\n",
            "Epoch 92/100\n",
            "105/105 [==============================] - 0s 314us/step - loss: 1.0516 - mean_absolute_error: 1.0516 - val_loss: 29.0514 - val_mean_absolute_error: 29.0514\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 20.45592\n",
            "Epoch 93/100\n",
            "105/105 [==============================] - 0s 346us/step - loss: 0.8893 - mean_absolute_error: 0.8893 - val_loss: 28.8846 - val_mean_absolute_error: 28.8846\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 20.45592\n",
            "Epoch 94/100\n",
            "105/105 [==============================] - 0s 298us/step - loss: 1.0312 - mean_absolute_error: 1.0312 - val_loss: 29.1270 - val_mean_absolute_error: 29.1270\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 20.45592\n",
            "Epoch 95/100\n",
            "105/105 [==============================] - 0s 304us/step - loss: 1.2993 - mean_absolute_error: 1.2993 - val_loss: 28.8695 - val_mean_absolute_error: 28.8695\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 20.45592\n",
            "Epoch 96/100\n",
            "105/105 [==============================] - 0s 290us/step - loss: 1.0685 - mean_absolute_error: 1.0685 - val_loss: 29.1321 - val_mean_absolute_error: 29.1321\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 20.45592\n",
            "Epoch 97/100\n",
            "105/105 [==============================] - 0s 316us/step - loss: 1.5386 - mean_absolute_error: 1.5386 - val_loss: 29.4797 - val_mean_absolute_error: 29.4797\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 20.45592\n",
            "Epoch 98/100\n",
            "105/105 [==============================] - 0s 282us/step - loss: 1.6824 - mean_absolute_error: 1.6824 - val_loss: 28.8387 - val_mean_absolute_error: 28.8387\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 20.45592\n",
            "Epoch 99/100\n",
            "105/105 [==============================] - 0s 303us/step - loss: 1.6891 - mean_absolute_error: 1.6891 - val_loss: 29.1748 - val_mean_absolute_error: 29.1748\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 20.45592\n",
            "Epoch 100/100\n",
            "105/105 [==============================] - 0s 320us/step - loss: 1.4733 - mean_absolute_error: 1.4733 - val_loss: 29.0229 - val_mean_absolute_error: 29.0229\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 20.45592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4d457b5ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "SqJIRAD4V5ht",
        "colab_type": "code",
        "outputId": "4179b550-3f75-4f9d-977e-7d3efd19e8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = NN_model.predict(X_test)\n",
        "from sklearn.metrics import r2_score\n",
        "accuracy=r2_score(Y_test,predictions)\n",
        "print('test')\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n",
            "0.5828658377646302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wb9YxC3ceDi6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hotUMbCHeDnC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcSkPNzseDqJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}